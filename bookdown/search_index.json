[
["index.html", "Panel Data and Optimization with R Preface", " Panel Data and Optimization with R Fan Wang 2020-04-22 Preface This is a work-in-progress website consisting of R panel data and optimization examples for Statistics/Econometrics/Economic Analysis. Materials gathered from various projects in which R code is used. Files are from Fan’s R4Econ repository. This is not a R package, but a list of examples in PDF/HTML/Rmd formats. REconTools is a package that can be installed with tools used in projects involving R. Bullet points show which base R, tidyverse or other functions/commands are used to achieve various objectives. An effort is made to use only base R (R Core Team 2019) and tidyverse (Wickham 2019) packages whenever possible to reduce dependencies. The goal of this repository is to make it easier to find/re-use codes produced for various projects. Some functions also rely on or correspond to functions from REconTools (Wang 2020). From Fan’s other repositories: For dynamic borrowing and savings problems, see Dynamic Asset Repository; For code examples, see also Matlab Example Code and Stata Example Code; For intro econ with Matlab, see Intro Mathematics for Economists, and for intro stat with R, see Intro Statistics for Undergraduates. See here for all of Fan’s public repositories. The site is built using Bookdown (Xie 2020). Please contact FanWangEcon for issues or problems. References "],
["array-matrix-dataframe.html", "Chapter 1 Array, Matrix, Dataframe 1.1 List 1.2 Array 1.3 Matrix 1.4 Variables in Dataframes", " Chapter 1 Array, Matrix, Dataframe 1.1 List 1.1.1 Multiple Dimensional List Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r list tutorial r vector vs list r initialize empty multiple element list r name rows and columns of 2 dimensional list r row and colum names of list list dimnames 1.1.1.1 One Dimensional Named List define list slice list # Define Lists ls_num &lt;- list(1,2,3) ls_str &lt;- list(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;) ls_num_str &lt;- list(1,2,&#39;3&#39;) # Named Lists ar_st_names &lt;- c(&#39;e1&#39;,&#39;e2&#39;,&#39;e3&#39;) ls_num_str_named &lt;- ls_num_str names(ls_num_str_named) &lt;- ar_st_names # Add Element to Named List ls_num_str_named$e4 &lt;- &#39;this is added&#39; # display print(paste0(&#39;ls_num:&#39;, str(ls_num))) ## List of 3 ## $ : num 1 ## $ : num 2 ## $ : num 3 ## [1] &quot;ls_num:&quot; print(paste0(&#39;ls_num[2:3]:&#39;, str(ls_num[2:3]))) ## List of 2 ## $ : num 2 ## $ : num 3 ## [1] &quot;ls_num[2:3]:&quot; print(paste0(&#39;ls_str:&#39;, str(ls_str))) ## List of 3 ## $ : chr &quot;1&quot; ## $ : chr &quot;2&quot; ## $ : chr &quot;3&quot; ## [1] &quot;ls_str:&quot; print(paste0(&#39;ls_str[2:3]:&#39;, str(ls_str[2:3]))) ## List of 2 ## $ : chr &quot;2&quot; ## $ : chr &quot;3&quot; ## [1] &quot;ls_str[2:3]:&quot; print(paste0(&#39;ls_num_str:&#39;, str(ls_num_str))) ## List of 3 ## $ : num 1 ## $ : num 2 ## $ : chr &quot;3&quot; ## [1] &quot;ls_num_str:&quot; print(paste0(&#39;ls_num_str[2:4]:&#39;, str(ls_num_str[2:4]))) ## List of 3 ## $ : num 2 ## $ : chr &quot;3&quot; ## $ : NULL ## [1] &quot;ls_num_str[2:4]:&quot; print(paste0(&#39;ls_num_str_named:&#39;, str(ls_num_str_named))) ## List of 4 ## $ e1: num 1 ## $ e2: num 2 ## $ e3: chr &quot;3&quot; ## $ e4: chr &quot;this is added&quot; ## [1] &quot;ls_num_str_named:&quot; print(paste0(&#39;ls_num_str_named[c(\\&#39;e2\\&#39;,\\&#39;e3\\&#39;,\\&#39;e4\\&#39;)]&#39;, str(ls_num_str_named[c(&#39;e2&#39;,&#39;e3&#39;,&#39;e4&#39;)]))) ## List of 3 ## $ e2: num 2 ## $ e3: chr &quot;3&quot; ## $ e4: chr &quot;this is added&quot; ## [1] &quot;ls_num_str_named[c(&#39;e2&#39;,&#39;e3&#39;,&#39;e4&#39;)]&quot; 1.1.1.2 Two Dimensional Unnamed List Generate a multiple dimensional list: Initiate with an N element empty list Reshape list to M by Q Fill list elements Get list element by row and column number List allows for different data types to be stored together. Note that element specific names in named list are not preserved when the list is reshaped to be two dimensional. Two dimensional list, however, could have row and column names. # Dimensions it_M &lt;- 2 it_Q &lt;- 3 it_N &lt;- it_M*it_Q # Initiate an Empty MxQ=N element list ls_2d_flat &lt;- vector(mode = &quot;list&quot;, length = it_N) ls_2d &lt;- ls_2d_flat # Named flat ls_2d_flat_named &lt;- ls_2d_flat names(ls_2d_flat_named) &lt;- paste0(&#39;e&#39;,seq(1,it_N)) ls_2d_named &lt;- ls_2d_flat_named # Reshape dim(ls_2d) &lt;- c(it_M, it_Q) # named 2d list can not carry 1d name after reshape dim(ls_2d_named) &lt;- c(it_M, it_Q) Print Various objects generated above: # display print(&#39;ls_2d_flat&#39;) ## [1] &quot;ls_2d_flat&quot; print(ls_2d_flat) ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL print(&#39;ls_2d_flat_named&#39;) ## [1] &quot;ls_2d_flat_named&quot; print(ls_2d_flat_named) ## $e1 ## NULL ## ## $e2 ## NULL ## ## $e3 ## NULL ## ## $e4 ## NULL ## ## $e5 ## NULL ## ## $e6 ## NULL print(&#39;ls_2d&#39;) ## [1] &quot;ls_2d&quot; print(ls_2d) ## [,1] [,2] [,3] ## [1,] NULL NULL NULL ## [2,] NULL NULL NULL print(&#39;ls_2d_named&#39;) ## [1] &quot;ls_2d_named&quot; print(ls_2d_named) ## [,1] [,2] [,3] ## [1,] NULL NULL NULL ## [2,] NULL NULL NULL Select element from list: # Select Values, double bracket to select from 2dim list print(&#39;ls_2d[[1,2]]&#39;) ## [1] &quot;ls_2d[[1,2]]&quot; print(ls_2d[[1,2]]) ## NULL 1.1.1.3 Define Two Dimensional Named LIst For naming two dimensional lists, rowname and colname does not work. Rather, we need to use dimnames. Note that in addition to dimnames, we can continue to have element specific names. Both can co-exist. But note that the element specific names are not preserved after dimension transform, so need to be redefined afterwards. How to select an element of a two dimensional list: row and column names: dimnames, ls_2d_flat_named[[‘row2’,‘col2’]] named elements: names, ls_2d_flat_named[[‘e5’]] select by index: index, ls_2d_flat_named[[5]] converted two dimensional named list to tibble/matrix Neither dimnames nor names are required, but both can be used to select elements. # Dimensions it_M &lt;- 3 it_Q &lt;- 4 it_N &lt;- it_M*it_Q # Initiate an Empty MxQ=N element list ls_2d_flat_named &lt;- vector(mode = &quot;list&quot;, length = it_N) dim(ls_2d_flat_named) &lt;- c(it_M, it_Q) # Fill with values for (it_Q_ctr in seq(1,it_Q)) { for (it_M_ctr in seq(1,it_M)) { # linear index ls_2d_flat_named[[it_M_ctr, it_Q_ctr]] &lt;- (it_Q_ctr-1)*it_M+it_M_ctr } } # Replace row names, note rownames does not work dimnames(ls_2d_flat_named)[[1]] &lt;- paste0(&#39;row&#39;,seq(1,it_M)) dimnames(ls_2d_flat_named)[[2]] &lt;- paste0(&#39;col&#39;,seq(1,it_Q)) # Element Specific Names names(ls_2d_flat_named) &lt;- paste0(&#39;e&#39;,seq(1,it_N)) # Convert to Matrix tb_2d_flat_named &lt;- as_tibble(ls_2d_flat_named) %&gt;% unnest() mt_2d_flat_named &lt;- as.matrix(tb_2d_flat_named) Print various objects generated above: # These are not element names, can still name each element # display print(&#39;ls_2d_flat_named&#39;) ## [1] &quot;ls_2d_flat_named&quot; print(ls_2d_flat_named) ## col1 col2 col3 col4 ## row1 1 4 7 10 ## row2 2 5 8 11 ## row3 3 6 9 12 ## attr(,&quot;names&quot;) ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; &quot;e6&quot; &quot;e7&quot; &quot;e8&quot; &quot;e9&quot; &quot;e10&quot; &quot;e11&quot; &quot;e12&quot; print(&#39;str(ls_2d_flat_named)&#39;) ## [1] &quot;str(ls_2d_flat_named)&quot; print(str(ls_2d_flat_named)) ## List of 12 ## $ e1 : num 1 ## $ e2 : num 2 ## $ e3 : num 3 ## $ e4 : num 4 ## $ e5 : num 5 ## $ e6 : num 6 ## $ e7 : num 7 ## $ e8 : num 8 ## $ e9 : num 9 ## $ e10: num 10 ## $ e11: num 11 ## $ e12: num 12 ## - attr(*, &quot;dim&quot;)= int [1:2] 3 4 ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:3] &quot;row1&quot; &quot;row2&quot; &quot;row3&quot; ## ..$ : chr [1:4] &quot;col1&quot; &quot;col2&quot; &quot;col3&quot; &quot;col4&quot; ## NULL print(&#39;tb_2d_flat_named&#39;) ## [1] &quot;tb_2d_flat_named&quot; print(tb_2d_flat_named) print(&#39;mt_2d_flat_named&#39;) ## [1] &quot;mt_2d_flat_named&quot; print(mt_2d_flat_named) ## col1 col2 col3 col4 ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Select elements from list: # Select elements with with dimnames print(&#39;ls_2d_flat_named[[\\&#39;row2\\&#39;,\\&#39;col2\\&#39;]]&#39;) ## [1] &quot;ls_2d_flat_named[[&#39;row2&#39;,&#39;col2&#39;]]&quot; print(ls_2d_flat_named[[&#39;row2&#39;,&#39;col2&#39;]]) ## [1] 5 # Select elements with element names print(&#39;ls_2d_flat_named[[\\&#39;e5\\&#39;]]&#39;) ## [1] &quot;ls_2d_flat_named[[&#39;e5&#39;]]&quot; print(ls_2d_flat_named[[&#39;e5&#39;]]) ## [1] 5 # Select elements with index print(&#39;ls_2d_flat_named[[5]]&#39;) ## [1] &quot;ls_2d_flat_named[[5]]&quot; print(ls_2d_flat_named[[5]]) ## [1] 5 1.2 Array 1.2.1 Array Basics Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.2.1.1 Multidimesional Arrays 1.2.1.1.1 Generate 2 Dimensional Array # Multidimensional Array # 1 is r1c1t1, 1.5 in r2c1t1, 0 in r1c2t1, etc. # Three dimensions, row first, column second, and tensor third x &lt;- array(c(1, 1.5, 0, 2, 0, 4, 0, 3), dim=c(2, 2, 2)) dim(x) ## [1] 2 2 2 print(x) ## , , 1 ## ## [,1] [,2] ## [1,] 1.0 0 ## [2,] 1.5 2 ## ## , , 2 ## ## [,1] [,2] ## [1,] 0 0 ## [2,] 4 3 1.2.1.2 Array Slicing 1.2.1.2.1 Remove Elements of Array Select elements with direct indexing, or with head and tail functions. Get the first two elements of three elements array. # Remove last element of array vars.group.bydf &lt;- c(&#39;23&#39;,&#39;dfa&#39;, &#39;wer&#39;) vars.group.bydf[-length(vars.group.bydf)] ## [1] &quot;23&quot; &quot;dfa&quot; # Use the head function to remove last element head(vars.group.bydf, -1) ## [1] &quot;23&quot; &quot;dfa&quot; head(vars.group.bydf, 2) ## [1] &quot;23&quot; &quot;dfa&quot; Get last two elements of array. # Remove first element of array vars.group.bydf &lt;- c(&#39;23&#39;,&#39;dfa&#39;, &#39;wer&#39;) vars.group.bydf[2:length(vars.group.bydf)] ## [1] &quot;dfa&quot; &quot;wer&quot; # Use Tail function tail(vars.group.bydf, -1) ## [1] &quot;dfa&quot; &quot;wer&quot; tail(vars.group.bydf, 2) ## [1] &quot;dfa&quot; &quot;wer&quot; 1.2.1.3 NA in Array 1.2.1.3.1 Check if NA is in Array # Convert Inf and -Inf to NA x &lt;- c(1, -1, Inf, 10, -Inf) na_if(na_if(x, -Inf), Inf) ## [1] 1 -1 NA 10 NA 1.2.2 Generate Arrays Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.2.2.1 Generate Special Arrays 1.2.2.1.1 Log Space Arrays Often need to generate arrays on log rather than linear scale, below is log 10 scaled grid. # Parameters it.lower.bd.inc.cnt &lt;- 3 fl.log.lower &lt;- -10 fl.log.higher &lt;- -9 fl.min.rescale &lt;- 0.01 it.log.count &lt;- 4 # Generate ar.fl.log.rescaled &lt;- exp(log(10)*seq(log10(fl.min.rescale), log10(fl.min.rescale + (fl.log.higher-fl.log.lower)), length.out=it.log.count)) ar.fl.log &lt;- ar.fl.log.rescaled + fl.log.lower - fl.min.rescale # Print ar.fl.log ## [1] -10.000000 -9.963430 -9.793123 -9.000000 1.2.3 String Arrays Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.2.3.1 String Replace # String replacement gsub(x = paste0(unique(df.slds.stats.perc$it.inner.counter), &#39;:&#39;, unique(df.slds.stats.perc$z_n_a_n), collapse = &#39;;&#39;), pattern = &quot;\\n&quot;, replacement = &quot;&quot;) gsub(x = var, pattern = &quot;\\n&quot;, replacement = &quot;&quot;) gsub(x = var.input, pattern = &quot;\\\\.&quot;, replacement = &quot;_&quot;) 1.2.3.1.1 Search If and Which String Contains r if string contains r if string contains either or grepl Use grepl to search either of multiple substrings in a text Search for a single substring in a single string: st_example_a &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; st_example_b &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/_main.html&#39; grepl(&#39;_main&#39;, st_example_a) ## [1] FALSE grepl(&#39;_main&#39;, st_example_b) ## [1] TRUE Search for if one of a set of substring exists in a set of strings. In particular which one of the elements of ls_spn contains at least one of the elements of ls_str_if_contains. In the example below, only the first path does not contain either the word aggregate or index in the path. This can be used after all paths have been found recursively in some folder to select only desired paths from the full set of possibilities: ls_spn &lt;- c(&quot;C:/Users/fan/R4Econ//panel/basic/fs_genpanel.Rmd&quot;, &quot;C:/Users/fan/R4Econ//summarize/aggregate/_main.Rmd&quot;, &quot;C:/Users/fan/R4Econ//summarize/index/fs_index_populate.Rmd&quot;) ls_str_if_contains &lt;- c(&quot;aggregate&quot;, &quot;index&quot;) str_if_contains &lt;- paste(ls_str_if_contains, collapse = &quot;|&quot;) grepl(str_if_contains, ls_spn) ## [1] FALSE TRUE TRUE 1.2.3.2 String Concatenate # Simple Collapse vars.group.by &lt;- c(&#39;abc&#39;, &#39;efg&#39;) paste0(vars.group.by, collapse=&#39;|&#39;) ## [1] &quot;abc|efg&quot; 1.2.3.3 String Add Leading Zero # Add Leading zero for integer values to allow for sorting when # integers are combined into strings it_z_n &lt;- 1 it_a_n &lt;- 192 print(sprintf(&quot;%02d&quot;, it_z_n)) ## [1] &quot;01&quot; print(sprintf(&quot;%04d&quot;, it_a_n)) ## [1] &quot;0192&quot; 1.2.3.4 Substring and File Name From path, get file name without suffix. r string split r list last element r get file name from path r get file path no name st_example &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; st_file_wth_suffix &lt;- tail(strsplit(st_example, &quot;/&quot;)[[1]],n=1) st_file_wno_suffix &lt;- sub(&#39;\\\\.Rmd$&#39;, &#39;&#39;, basename(st_example)) st_fullpath_nosufx &lt;- sub(&#39;\\\\.Rmd$&#39;, &#39;&#39;, st_example) st_lastpath_noname &lt;- (dirname(st_example)) st_fullpath_noname &lt;- dirname(st_example) print(strsplit(st_example, &quot;/&quot;)) ## [[1]] ## [1] &quot;C:&quot; &quot;Users&quot; &quot;fan&quot; &quot;R4Econ&quot; &quot;amto&quot; &quot;tibble&quot; &quot;fs_tib_basics.Rmd&quot; print(st_file_wth_suffix) ## [1] &quot;fs_tib_basics.Rmd&quot; print(st_file_wno_suffix) ## [1] &quot;fs_tib_basics&quot; print(st_fullpath_nosufx) ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics&quot; print(st_lastpath_noname) ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble&quot; print(st_fullpath_noname) ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble&quot; 1.2.4 Mesh Matrices, Arrays and Scalars Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r expand.grid meshed array to matrix r meshgrid r array to matrix r reshape array to matrix dplyr permuations rows of matrix and element of array tidyr expand_grid mesh matrix and vector 1.2.4.1 Mesh Two or More Vectors with expand_grid In the example below, we have a matrix that is 2 by 2 (endogenous states), a vector that is 3 by 1 (choices), and another matrix that is 4 by 3 (exogenous states shocks). We want to generate a tibble dataset that meshes the matrix and the vector, so that all combinations show up. Additionally, we want to add some additional values that are common across all rows to the meshed dataframe. Note expand_grid is a from tidyr 1.0.0. # A. Generate the 5 by 2 Matrix (ENDO STATES) # it_child_count = N, the number of children it_N_child_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) fl_rho = 0.1 fl_lambda = 1.1 mt_nP_A_alpha = cbind(ar_nN_A, ar_nN_alpha, fl_rho, fl_lambda) ar_st_varnames &lt;- c(&#39;s_A&#39;, &#39;s_alpha&#39;, &#39;p_rho&#39;, &#39;p_lambda&#39;) tb_states_endo &lt;- as_tibble(mt_nP_A_alpha) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% rowid_to_column(var = &quot;state_id&quot;) # B. Choice Grid it_N_choice_cnt = 3 fl_max = 10 fl_min = 0 ar_nN_d = seq(fl_min, fl_max, length.out = it_N_choice_cnt) ar_st_varnames &lt;- c(&#39;c_food&#39;) tb_choices &lt;- as_tibble(ar_nN_d) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% rowid_to_column(var = &quot;choice_id&quot;) # C. Shock Grid set.seed(123) it_N_shock_cnt = 4 ar_nQ_shocks = exp(rnorm(it_N_shock_cnt, mean=0, sd=1)) ar_st_varnames &lt;- c(&#39;s_eps&#39;) tb_states_exo &lt;- as_tibble(ar_nQ_shocks) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% rowid_to_column(var = &quot;shock_id&quot;) # dataframe expand with other non expanded variables ar_st_varnames &lt;- tb_states_shk_choices &lt;- tb_states_endo %&gt;% expand_grid(tb_choices) %&gt;% expand_grid(tb_states_exo) %&gt;% select(state_id, choice_id, shock_id, s_A, s_alpha, s_eps, c_food, p_rho, p_lambda) # display kable(tb_states_shk_choices) %&gt;% kable_styling_fc() state_id choice_id shock_id s_A s_alpha s_eps c_food p_rho p_lambda 1 1 1 -2 0.1 0.5709374 0 0.1 1.1 1 1 2 -2 0.1 0.7943926 0 0.1 1.1 1 1 3 -2 0.1 4.7526783 0 0.1 1.1 1 1 4 -2 0.1 1.0730536 0 0.1 1.1 1 2 1 -2 0.1 0.5709374 5 0.1 1.1 1 2 2 -2 0.1 0.7943926 5 0.1 1.1 1 2 3 -2 0.1 4.7526783 5 0.1 1.1 1 2 4 -2 0.1 1.0730536 5 0.1 1.1 1 3 1 -2 0.1 0.5709374 10 0.1 1.1 1 3 2 -2 0.1 0.7943926 10 0.1 1.1 1 3 3 -2 0.1 4.7526783 10 0.1 1.1 1 3 4 -2 0.1 1.0730536 10 0.1 1.1 2 1 1 2 0.9 0.5709374 0 0.1 1.1 2 1 2 2 0.9 0.7943926 0 0.1 1.1 2 1 3 2 0.9 4.7526783 0 0.1 1.1 2 1 4 2 0.9 1.0730536 0 0.1 1.1 2 2 1 2 0.9 0.5709374 5 0.1 1.1 2 2 2 2 0.9 0.7943926 5 0.1 1.1 2 2 3 2 0.9 4.7526783 5 0.1 1.1 2 2 4 2 0.9 1.0730536 5 0.1 1.1 2 3 1 2 0.9 0.5709374 10 0.1 1.1 2 3 2 2 0.9 0.7943926 10 0.1 1.1 2 3 3 2 0.9 4.7526783 10 0.1 1.1 2 3 4 2 0.9 1.0730536 10 0.1 1.1 Using expand_grid directly over arrays # expand grid with dplyr expand_grid(x = 1:3, y = 1:2, z = -3:-1) 1.2.4.2 Mesh Arrays with expand.grid Given two arrays, mesh the two arrays together. # use expand.grid to generate all combinations of two arrays it_ar_A = 5 it_ar_alpha = 10 ar_A = seq(-2, 2, length.out=it_ar_A) ar_alpha = seq(0.1, 0.9, length.out=it_ar_alpha) mt_A_alpha = expand.grid(A = ar_A, alpha = ar_alpha) mt_A_meshed = mt_A_alpha[,1] dim(mt_A_meshed) = c(it_ar_A, it_ar_alpha) mt_alpha_meshed = mt_A_alpha[,2] dim(mt_alpha_meshed) = c(it_ar_A, it_ar_alpha) # display kable(mt_A_meshed) %&gt;% kable_styling_fc() -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 kable(mt_alpha_meshed) %&gt;% kable_styling_fc_wide() 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 Two Identical Arrays, individual attributes, each column is an individual for a matrix, and each row is also an individual. # use expand.grid to generate all combinations of two arrays it_ar_A = 5 ar_A = seq(-2, 2, length.out=it_ar_A) mt_A_A = expand.grid(Arow = ar_A, Arow = ar_A) mt_Arow = mt_A_A[,1] dim(mt_Arow) = c(it_ar_A, it_ar_A) mt_Acol = mt_A_A[,2] dim(mt_Acol) = c(it_ar_A, it_ar_A) # display kable(mt_Arow) %&gt;% kable_styling_fc() -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 kable(mt_Acol) %&gt;% kable_styling_fc() -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 1.3 Matrix 1.3.1 Generate Matrixes Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.3.1.1 Create a N by 2 Matrix from 3 arrays Names of each array become row names automatically. ar_row_one &lt;- c(-1,+1) ar_row_two &lt;- c(-3,-2) ar_row_three &lt;- c(0.35,0.75) mt_n_by_2 &lt;- rbind(ar_row_one, ar_row_two, ar_row_three) kable(mt_n_by_2) %&gt;% kable_styling_fc() ar_row_one -1.00 1.00 ar_row_two -3.00 -2.00 ar_row_three 0.35 0.75 1.3.1.2 Generate Random Matrixes Random draw from the normal distribution, random draw from the uniform distribution, and combine resulting matrixes. # Generate 15 random normal, put in 5 rows, and 3 columns mt_rnorm &lt;- matrix(rnorm(15,mean=0,sd=1), nrow=5, ncol=3) # Generate 15 random normal, put in 5 rows, and 3 columns mt_runif &lt;- matrix(runif(15,min=0,max=1), nrow=5, ncol=5) # Combine mt_rnorm_runif &lt;- cbind(mt_rnorm, mt_runif) # Display kable(mt_rnorm_runif) %&gt;% kable_styling_fc_wide() 0.1292877 -0.4456620 -0.5558411 0.3181810 0.3688455 0.2659726 0.3181810 0.3688455 1.7150650 1.2240818 1.7869131 0.2316258 0.1524447 0.8578277 0.2316258 0.1524447 0.4609162 0.3598138 0.4978505 0.1428000 0.1388061 0.0458312 0.1428000 0.1388061 -1.2650612 0.4007715 -1.9666172 0.4145463 0.2330341 0.4422001 0.4145463 0.2330341 -0.6868529 0.1106827 0.7013559 0.4137243 0.4659625 0.7989248 0.4137243 0.4659625 1.3.2 Linear Algebra Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.3.2.1 Matrix Multiplication Multiply Together a 3 by 2 matrix and a 2 by 1 vector ar_row_one &lt;- c(-1,+1) ar_row_two &lt;- c(-3,-2) ar_row_three &lt;- c(0.35,0.75) mt_n_by_2 &lt;- rbind(ar_row_one, ar_row_two, ar_row_three) ar_row_four &lt;- c(3,4) # Matrix Multiplication mt_out &lt;- mt_n_by_2 %*% ar_row_four print(mt_n_by_2) ## [,1] [,2] ## ar_row_one -1.00 1.00 ## ar_row_two -3.00 -2.00 ## ar_row_three 0.35 0.75 print(ar_row_four) ## [1] 3 4 print(mt_out) ## [,1] ## ar_row_one 1.00 ## ar_row_two -17.00 ## ar_row_three 4.05 1.4 Variables in Dataframes 1.4.1 Generate Dataframe Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.1.1 Generate Tibble given Matrixes and Arrays Given Arrays and Matrixes, Generate Tibble and Name Variables/Columns naming tibble columns tibble variable names dplyr rename tibble dplyr rename tibble all variables dplyr rename all columns by index dplyr tibble add index column see also: SO-51205520 # Base Inputs ar_col &lt;- c(-1,+1) mt_rnorm_a &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=2, ncol=2) mt_rnorm_b &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=2, ncol=4) # Combine Matrix mt_combine &lt;- cbind(ar_col, mt_rnorm_a, mt_rnorm_b) colnames(mt_combine) &lt;- c(&#39;ar_col&#39;, paste0(&#39;matcolvar_grpa_&#39;, seq(1,dim(mt_rnorm_a)[2])), paste0(&#39;matcolvar_grpb_&#39;, seq(1,dim(mt_rnorm_b)[2]))) # Variable Names ar_st_varnames &lt;- c(&#39;var_one&#39;, paste0(&#39;tibcolvar_ga_&#39;, c(1,2)), paste0(&#39;tibcolvar_gb_&#39;, c(1,2,3,4))) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_combine) %&gt;% rename_all(~c(ar_st_varnames)) # Add an index column to the dataframe, ID column tb_combine &lt;- tb_combine %&gt;% rowid_to_column(var = &quot;ID&quot;) # Change all gb variable names tb_combine &lt;- tb_combine %&gt;% rename_at(vars(starts_with(&quot;tibcolvar_gb_&quot;)), funs(str_replace(., &quot;_gb_&quot;, &quot;_gbrenamed_&quot;))) # Tibble back to matrix mt_tb_combine_back &lt;- data.matrix(tb_combine) # Display kable(mt_combine) %&gt;% kable_styling_fc_wide() ar_col matcolvar_grpa_1 matcolvar_grpa_2 matcolvar_grpb_1 matcolvar_grpb_2 matcolvar_grpb_3 matcolvar_grpb_4 -1 -1.1655448 0.6849361 -1.3115224 -0.1294107 -1.3115224 -0.1294107 1 -0.8185157 -0.3200564 -0.5996083 0.8867361 -0.5996083 0.8867361 kable(tb_combine) %&gt;% kable_styling_fc_wide() ID var_one tibcolvar_ga_1 tibcolvar_ga_2 tibcolvar_gbrenamed_1 tibcolvar_gbrenamed_2 tibcolvar_gbrenamed_3 tibcolvar_gbrenamed_4 1 -1 -1.1655448 0.6849361 -1.3115224 -0.1294107 -1.3115224 -0.1294107 2 1 -0.8185157 -0.3200564 -0.5996083 0.8867361 -0.5996083 0.8867361 kable(mt_tb_combine_back) %&gt;% kable_styling_fc_wide() ID var_one tibcolvar_ga_1 tibcolvar_ga_2 tibcolvar_gbrenamed_1 tibcolvar_gbrenamed_2 tibcolvar_gbrenamed_3 tibcolvar_gbrenamed_4 1 -1 -1.1655448 0.6849361 -1.3115224 -0.1294107 -1.3115224 -0.1294107 2 1 -0.8185157 -0.3200564 -0.5996083 0.8867361 -0.5996083 0.8867361 1.4.1.2 Rename Tibble with Numeric Column Names After reshaping, often could end up with variable names that are all numeric, intgers for example, how to rename these variables to add a common prefix for example. # Base Inputs ar_col &lt;- c(-1,+1) mt_rnorm_c &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=5, ncol=10) mt_combine &lt;- cbind(ar_col, mt_rnorm_c) # Variable Names ar_it_cols_ctr &lt;- seq(1, dim(mt_rnorm_c)[2]) ar_st_varnames &lt;- c(&#39;var_one&#39;, ar_it_cols_ctr) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_combine) %&gt;% rename_all(~c(ar_st_varnames)) # Add an index column to the dataframe, ID column tb_combine_ori &lt;- tb_combine %&gt;% rowid_to_column(var = &quot;ID&quot;) # Change all gb variable names tb_combine &lt;- tb_combine_ori %&gt;% rename_at( vars(num_range(&#39;&#39;,ar_it_cols_ctr)), funs(paste0(&quot;rho&quot;, . , &#39;var&#39;)) ) # Display kable(tb_combine_ori) %&gt;% kable_styling_fc_wide() ID var_one 1 2 3 4 5 6 7 8 9 10 1 -1 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 2 1 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 3 -1 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 4 1 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 5 -1 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 kable(tb_combine) %&gt;% kable_styling_fc_wide() ID var_one rho1var rho2var rho3var rho4var rho5var rho6var rho7var rho8var rho9var rho10var 1 -1 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 2 1 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 3 -1 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 4 1 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 5 -1 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 -3.2273228 -0.7717918 -0.1513960 0.3297912 1.4.1.3 Tibble Row and Column and Summarize Show what is in the table: 1, column and row names; 2, contents inside table. tb_iris &lt;- as_tibble(iris) print(rownames(tb_iris)) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; ## [28] &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; ## [55] &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; ## [82] &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; &quot;100&quot; &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; ## [109] &quot;109&quot; &quot;110&quot; &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;114&quot; &quot;115&quot; &quot;116&quot; &quot;117&quot; &quot;118&quot; &quot;119&quot; &quot;120&quot; &quot;121&quot; &quot;122&quot; &quot;123&quot; &quot;124&quot; &quot;125&quot; &quot;126&quot; &quot;127&quot; &quot;128&quot; &quot;129&quot; &quot;130&quot; &quot;131&quot; &quot;132&quot; &quot;133&quot; &quot;134&quot; &quot;135&quot; ## [136] &quot;136&quot; &quot;137&quot; &quot;138&quot; &quot;139&quot; &quot;140&quot; &quot;141&quot; &quot;142&quot; &quot;143&quot; &quot;144&quot; &quot;145&quot; &quot;146&quot; &quot;147&quot; &quot;148&quot; &quot;149&quot; &quot;150&quot; colnames(tb_iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; colnames(tb_iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; summary(tb_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 1.4.1.4 Tibble Sorting dplyr arrange desc reverse dplyr sort # Sort in Ascending Order tb_iris %&gt;% select(Species, Sepal.Length, everything()) %&gt;% arrange(Species, Sepal.Length) %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() Species Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 4.3 3.0 1.1 0.1 setosa 4.4 2.9 1.4 0.2 setosa 4.4 3.0 1.3 0.2 setosa 4.4 3.2 1.3 0.2 setosa 4.5 2.3 1.3 0.3 setosa 4.6 3.1 1.5 0.2 setosa 4.6 3.4 1.4 0.3 setosa 4.6 3.6 1.0 0.2 setosa 4.6 3.2 1.4 0.2 setosa 4.7 3.2 1.3 0.2 # Sort in Descending Order tb_iris %&gt;% select(Species, Sepal.Length, everything()) %&gt;% arrange(desc(Species), desc(Sepal.Length)) %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() Species Sepal.Length Sepal.Width Petal.Length Petal.Width virginica 7.9 3.8 6.4 2.0 virginica 7.7 3.8 6.7 2.2 virginica 7.7 2.6 6.9 2.3 virginica 7.7 2.8 6.7 2.0 virginica 7.7 3.0 6.1 2.3 virginica 7.6 3.0 6.6 2.1 virginica 7.4 2.8 6.1 1.9 virginica 7.3 2.9 6.3 1.8 virginica 7.2 3.6 6.1 2.5 virginica 7.2 3.2 6.0 1.8 1.4.1.5 REconTools Summarize over Tible Use R4Econ’s summary tool. df_summ_stats &lt;- ff_summ_percentiles(tb_iris) kable(t(df_summ_stats)) %&gt;% kable_styling_fc_wide() stats n NAobs ZEROobs mean sd cv min p01 p05 p10 p25 p50 p75 p90 p95 p99 max Petal.Length 150 0 0 3.758000 1.7652982 0.4697441 1.0 1.149 1.300 1.4 1.6 4.35 5.1 5.80 6.100 6.700 6.9 Petal.Width 150 0 0 1.199333 0.7622377 0.6355511 0.1 0.100 0.200 0.2 0.3 1.30 1.8 2.20 2.300 2.500 2.5 Sepal.Length 150 0 0 5.843333 0.8280661 0.1417113 4.3 4.400 4.600 4.8 5.1 5.80 6.4 6.90 7.255 7.700 7.9 Sepal.Width 150 0 0 3.057333 0.4358663 0.1425642 2.0 2.200 2.345 2.5 2.8 3.00 3.3 3.61 3.800 4.151 4.4 1.4.2 Factor Label and Combine Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.2.1 Factor, Label, Cross and Graph Generate a Scatter plot with different colors representing different categories. There are multiple underlying factor/categorical variables, for example two binary variables. Generate scatter plot with colors for the combinations of these two binary variables. We combine here the vs and am variables from the mtcars dataset. vs is engine shape, am is auto or manual shift. We will generate a scatter plot of mpg and qsec over four categories with different colors. am: Transmission (0 = automatic, 1 = manual) vs: Engine (0 = V-shaped, 1 = straight) mpg: miles per galon qsec: 1/4 mile time # First make sure these are factors tb_mtcars &lt;- as_tibble(mtcars) %&gt;% mutate(vs = as_factor(vs), am = as_factor(am)) # Second Label the Factors am_levels &lt;- c(auto_shift = &quot;0&quot;, manual_shift = &quot;1&quot;) vs_levels &lt;- c(vshaped_engine = &quot;0&quot;, straight_engine = &quot;1&quot;) tb_mtcars &lt;- tb_mtcars %&gt;% mutate(vs = fct_recode(vs, !!!vs_levels), am = fct_recode(am, !!!am_levels)) # Third Combine Factors tb_mtcars_selected &lt;- tb_mtcars %&gt;% mutate(vs_am = fct_cross(vs, am, sep=&#39;_&#39;, keep_empty = FALSE)) %&gt;% select(mpg, qsec, vs_am) print(tb_mtcars_selected) Now we generate scatter plot based on the combined factors # Labeling st_title &lt;- paste0(&#39;Distribution of MPG and QSEC from mtcars&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/amto/tibble/htmlpdfr/fs_tib_factors.html&#39;) st_caption &lt;- paste0(&#39;mtcars dataset, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;MPG = Miles per Gallon&#39; st_y_label &lt;- &#39;QSEC = time for 1/4 Miles&#39; # Graphing plt_mtcars_scatter &lt;- ggplot(tb_mtcars_selected, aes(x=mpg, y=qsec, colour=vs_am, shape=vs_am)) + geom_jitter(size=3, width = 0.15) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_mtcars_scatter) 1.4.3 Drawly Random Rows Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.3.1 Draw Random Subset of Sample r random discrete We have a sample of N individuals in some dataframe. Draw without replacement a subset \\(M&lt;N\\) of rows. # parameters, it_M &lt; it_N it_N &lt;- 10 it_M &lt;- 5 # Draw it_m from indexed list of it_N set.seed(123) ar_it_rand_idx &lt;- sample(it_N, it_M, replace=FALSE) # dataframe df_full &lt;- as_tibble(matrix(rnorm(4,mean=0,sd=1), nrow=it_N, ncol=4)) %&gt;% rowid_to_column(var = &quot;ID&quot;) # random Subset df_rand_sub_a &lt;- df_full[ar_it_rand_idx,] # Random subset also df_rand_sub_b &lt;- df_full[sample(dim(df_full)[1], it_M, replace=FALSE),] # Print # Display kable(df_full) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 1 0.1292877 0.4609162 0.1292877 0.4609162 2 1.7150650 -1.2650612 1.7150650 -1.2650612 3 0.4609162 0.1292877 0.4609162 0.1292877 4 -1.2650612 1.7150650 -1.2650612 1.7150650 5 0.1292877 0.4609162 0.1292877 0.4609162 6 1.7150650 -1.2650612 1.7150650 -1.2650612 7 0.4609162 0.1292877 0.4609162 0.1292877 8 -1.2650612 1.7150650 -1.2650612 1.7150650 9 0.1292877 0.4609162 0.1292877 0.4609162 10 1.7150650 -1.2650612 1.7150650 -1.2650612 kable(df_rand_sub_a) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 3 0.4609162 0.1292877 0.4609162 0.1292877 10 1.7150650 -1.2650612 1.7150650 -1.2650612 2 1.7150650 -1.2650612 1.7150650 -1.2650612 8 -1.2650612 1.7150650 -1.2650612 1.7150650 6 1.7150650 -1.2650612 1.7150650 -1.2650612 kable(df_rand_sub_b) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 5 0.1292877 0.4609162 0.1292877 0.4609162 3 0.4609162 0.1292877 0.4609162 0.1292877 9 0.1292877 0.4609162 0.1292877 0.4609162 1 0.1292877 0.4609162 0.1292877 0.4609162 4 -1.2650612 1.7150650 -1.2650612 1.7150650 1.4.3.2 Random Subset of Panel There are \\(N\\) individuals, each could be observed \\(M\\) times, but then select a subset of rows only, so each person is randomly observed only a subset of times. Specifically, there there are 3 unique students with student ids, and the second variable shows the random dates in which the student showed up in class, out of the 10 classes available. # Define it_N &lt;- 3 it_M &lt;- 10 svr_id &lt;- &#39;student_id&#39; # dataframe set.seed(123) df_panel_rand &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(date = row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), date) %&gt;% rename(date_in_class = date) # Print kable(df_panel_rand) %&gt;% kable_styling_fc() student_id date_in_class 1 1 1 2 1 8 1 9 1 10 2 5 2 8 2 10 3 1 3 2 3 3 3 4 3 5 3 6 3 9 1.4.4 Generate Variables Conditional On Others Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.4.1 case_when Basic Example Given several other variables, and generate a new variable when these varaibles satisfy conditions. Note that case_when are ifelse type statements. So below group one is below 16 MPG when do qsec &gt;= 20 second line that is elseif, only those that are &gt;=16 are considered here then think about two dimensional mpg and qsec grid, the lower-right area, give another category to manual cars in that group # Get mtcars df_mtcars &lt;- mtcars # case_when with mtcars df_mtcars &lt;- df_mtcars %&gt;% mutate(mpg_qsec_am_grp = case_when(mpg &lt; 16 ~ &quot;&lt; 16 MPG&quot;, qsec &gt;= 20 ~ &quot;&gt; 16 MPG &amp; qsec &gt;= 20&quot;, am == 1 ~ &quot;&gt; 16 MPG &amp; asec &lt; 20 &amp; manual&quot;, TRUE ~ &quot;Others&quot;)) # # For dataframe # df.reg &lt;-df.reg %&gt;% na_if(-Inf) %&gt;% na_if(Inf) # # For a specific variable in dataframe # df.reg.use %&gt;% mutate(!!(var.input) := na_if(!!sym(var.input), 0)) # # # Setting to NA # df.reg.use &lt;- df.reg.guat %&gt;% filter(!!sym(var.mth) != 0) # df.reg.use.log &lt;- df.reg.use # df.reg.use.log[which(is.nan(df.reg.use$prot.imputed.log)),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==Inf),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==-Inf),] = NA # df.reg.use.log &lt;- df.reg.use.log %&gt;% drop_na(prot.imputed.log) # # df.reg.use.log$prot.imputed.log Now we generate scatter plot based on the combined factors # Labeling st_title &lt;- paste0(&#39;Use case_when To Generate ifelse Groupings&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/amto/tibble/htmlpdfr/fs_tib_na.html&#39;) st_caption &lt;- paste0(&#39;mtcars dataset, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;MPG = Miles per Gallon&#39; st_y_label &lt;- &#39;QSEC = time for 1/4 Miles&#39; # Graphing plt_mtcars_casewhen_scatter &lt;- ggplot(df_mtcars, aes(x=mpg, y=qsec, colour=mpg_qsec_am_grp, shape=mpg_qsec_am_grp)) + geom_jitter(size=3, width = 0.15) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_mtcars_casewhen_scatter) 1.4.4.2 Generate NA values if Variables have Certain Value In the example below, in one line: generate a random standard normal vector two set na methods: if the value of the standard normal is negative, set value to -999, otherwise MPG, replace the value -999 with NA case_when only with type specific NA values Assigning NA yields error in case_when note we need to conform NA to type generate new categorical variable based on NA condition using is.na with both string and numeric NAs jointly considered. fake NA string to be printed on chart # Get mtcars df_mtcars &lt;- mtcars # Make some values of mpg randomly NA # the NA has to conform to the type of the remaining values for the new variable # NA_real_, NA_character_, NA_integer_, NA_complex_ set.seed(2341) df_mtcars &lt;- df_mtcars %&gt;% mutate(mpg_wth_NA1 = na_if( case_when( rnorm(n(),mean=0,sd=1) &lt; 0 ~ -999, TRUE ~ mpg), -999)) %&gt;% mutate(mpg_wth_NA2 = case_when( rnorm(n(),mean=0,sd=1) &lt; 0 ~ NA_real_, TRUE ~ mpg)) %&gt;% mutate(mpg_wth_NA3 = case_when( rnorm(n(),mean=0,sd=1) &lt; 0 ~ NA_character_, TRUE ~ &quot;shock &gt; 0 string&quot;)) # Generate New Variables based on if mpg_wth_NA is NA or not # same variable as above, but now first a category based on if NA # And we generate a fake string &quot;NA&quot; variable, this is not NA # the String NA allows for it to be printed on figure df_mtcars &lt;- df_mtcars %&gt;% mutate(group_with_na = case_when(is.na(mpg_wth_NA2) &amp; is.na(mpg_wth_NA3) ~ &quot;Rand String and Rand Numeric both NA&quot;, mpg &lt; 16 ~ &quot;&lt; 16 MPG&quot;, qsec &gt;= 20 ~ &quot;&gt; 16 MPG &amp; qsec &gt;= 20&quot;, am == 1 ~ &quot;&gt; 16 MPG &amp; asec &lt; 20 &amp; manual&quot;, TRUE ~ &quot;Fake String NA&quot;)) # show kable(head(df_mtcars %&gt;% select(starts_with(&#39;mpg&#39;)),13)) %&gt;% kable_styling_fc() mpg mpg_wth_NA1 mpg_wth_NA2 mpg_wth_NA3 21.0 NA NA shock &gt; 0 string 21.0 21.0 21.0 NA 22.8 NA NA NA 21.4 NA 21.4 NA 18.7 NA 18.7 NA 18.1 18.1 NA shock &gt; 0 string 14.3 14.3 NA shock &gt; 0 string 24.4 NA 24.4 NA 22.8 22.8 22.8 NA 19.2 19.2 NA NA 17.8 NA NA NA 16.4 16.4 16.4 NA 17.3 NA NA shock &gt; 0 string # # Setting to NA # df.reg.use &lt;- df.reg.guat %&gt;% filter(!!sym(var.mth) != 0) # df.reg.use.log &lt;- df.reg.use # df.reg.use.log[which(is.nan(df.reg.use$prot.imputed.log)),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==Inf),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==-Inf),] = NA # df.reg.use.log &lt;- df.reg.use.log %&gt;% drop_na(prot.imputed.log) # # df.reg.use.log$prot.imputed.log Now we generate scatter plot based on the combined factors, but now with the NA category # Labeling st_title &lt;- paste0(&#39;Use na_if and is.na to Generate and Distinguish NA Values\\n&#39;, &#39;NA_real_, NA_character_, NA_integer_, NA_complex_&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/amto/tibble/htmlpdfr/fs_tib_na.html&#39;) st_caption &lt;- paste0(&#39;mtcars dataset, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;MPG = Miles per Gallon&#39; st_y_label &lt;- &#39;QSEC = time for 1/4 Miles&#39; # Graphing plt_mtcars_ifisna_scatter &lt;- ggplot(df_mtcars, aes(x=mpg, y=qsec, colour=group_with_na, shape=group_with_na)) + geom_jitter(size=3, width = 0.15) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_mtcars_ifisna_scatter) 1.4.4.3 Approximate Values Comparison r values almost the same all.equal From numeric approximation, often values are very close, and should be set to equal. Use isTRUE(all.equal). In the example below, we randomly generates four arrays. Two of the arrays have slightly higher variance, two arrays have slightly lower variance. They sd are to be 10 times below or 10 times above the tolerance comparison level. The values are not the same in any of the columns, but by allowing for almost true given some tolerance level, in the low standard deviation case, the values differences are within tolerance, so they are equal. This is an essential issue when dealing with optimization results. # Set tolerance tol_lvl = 1.5e-3 sd_lower_than_tol = tol_lvl/10 sd_higher_than_tol = tol_lvl*10 # larger SD set.seed(123) mt_runif_standard &lt;- matrix(rnorm(10,mean=0,sd=sd_higher_than_tol), nrow=5, ncol=2) # small SD set.seed(123) mt_rnorm_small_sd &lt;- matrix(rnorm(10,mean=0,sd=sd_lower_than_tol), nrow=5, ncol=2) # Generates Random Matirx tb_rnorm_runif &lt;- as_tibble(cbind(mt_rnorm_small_sd, mt_runif_standard)) # Are Variables the same, not for strict comparison tb_rnorm_runif_approxi_same &lt;- tb_rnorm_runif %&gt;% mutate(V1_V2_ALMOST_SAME = case_when(isTRUE(all.equal(V1, V2, tolerance=tol_lvl)) ~ paste0(&#39;TOL=&#39;,sd_lower_than_tol,&#39;, SAME ALMOST&#39;), TRUE ~ paste0(&#39;TOL=&#39;,sd_lower_than_tol,&#39;, NOT SAME ALMOST&#39;))) %&gt;% mutate(V3_V4_ALMOST_SAME = case_when(isTRUE(all.equal(V3, V4, tolerance=tol_lvl)) ~ paste0(&#39;TOL=&#39;,sd_higher_than_tol,&#39;, SAME ALMOST&#39;), TRUE ~ paste0(&#39;TOL=&#39;,sd_higher_than_tol,&#39;, NOT SAME ALMOST&#39;))) # Pring kable(tb_rnorm_runif_approxi_same) %&gt;% kable_styling_fc_wide() V1 V2 V3 V4 V1_V2_ALMOST_SAME V3_V4_ALMOST_SAME -0.0000841 0.0002573 -0.0084071 0.0257260 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST -0.0000345 0.0000691 -0.0034527 0.0069137 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 0.0002338 -0.0001898 0.0233806 -0.0189759 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 0.0000106 -0.0001030 0.0010576 -0.0103028 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 0.0000194 -0.0000668 0.0019393 -0.0066849 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 1.4.5 String Values Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.5.1 Find and Replace Find and Replace in Dataframe. # if string value is contained in variable (&quot;bridex.B&quot; %in% (df.reg.out.all$vars_var.y)) # if string value is not contained in variable: # 1. type is variable name # 2. Toyota|Mazda are strings to be excluded filter(mtcars, !grepl(&#39;Toyota|Mazda&#39;, type)) # filter does not contain string rs_hgt_prot_log_tidy %&gt;% filter(!str_detect(term, &#39;prot&#39;)) "],
["summarize-data.html", "Chapter 2 Summarize Data 2.1 Counting Observation 2.2 Sorting, Indexing, Slicing 2.3 Group Statistics 2.4 Distributional Statistics 2.5 Summarize Multiple Variables", " Chapter 2 Summarize Data 2.1 Counting Observation 2.1.1 Uncount Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. In some panel, there are \\(N\\) individuals, each observed for \\(Y_i\\) years. Given a dataset with two variables, the individual index, and the \\(Y_i\\) variable, expand the dataframe so that there is a row for each individual index’s each unique year in the survey. Search: r duplicate row by variable Links: see: Create duplicate rows based on a variable Algorithm: generate testing frame, the individual attribute dataset with invariant information over panel uncount, duplicate rows by years in survey group and generate sorted index add indiviual specific stat year to index # 1. Array of Years in the Survey ar_years_in_survey &lt;- c(2,3,1,10,2,5) ar_start_yaer &lt;- c(1,2,3,1,1,1) ar_end_year &lt;- c(2,4,3,10,2,5) mt_combine &lt;- cbind(ar_years_in_survey, ar_start_yaer, ar_end_year) # This is the individual attribute dataset, attributes that are invariant acrosss years tb_indi_attributes &lt;- as_tibble(mt_combine) %&gt;% rowid_to_column(var = &quot;ID&quot;) # 2. Sort and generate variable equal to sorted index tb_indi_panel &lt;- tb_indi_attributes %&gt;% uncount(ar_years_in_survey) # 3. Panel now construct exactly which year in survey, note that all needed is sort index # Note sorting not needed, all rows identical now tb_indi_panel &lt;- tb_indi_panel %&gt;% group_by(ID) %&gt;% mutate(yr_in_survey = row_number()) tb_indi_panel &lt;- tb_indi_panel %&gt;% mutate(calendar_year = yr_in_survey + ar_start_yaer - 1) # Show results Head 10 tb_indi_panel %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() ID ar_start_yaer ar_end_year yr_in_survey calendar_year 1 1 2 1 1 1 1 2 2 2 2 2 4 1 2 2 2 4 2 3 2 2 4 3 4 3 3 3 1 3 4 1 10 1 1 4 1 10 2 2 4 1 10 3 3 4 1 10 4 4 2.2 Sorting, Indexing, Slicing 2.2.1 Sorting Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 2.2.1.1 Generate Sorted Index within Group with Repeating Values There is a variable, sort by this variable, then generate index from 1 to N representing sorted values of this index. If there are repeating values, still assign index, different index each value. r generate index sort dplyr mutate equals index # Sort and generate variable equal to sorted index df_iris &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% select(Sepal.Length, Sepal.Len.Index, everything()) # Show results Head 10 df_iris %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Width Petal.Length Petal.Width Species 4.3 1 3.0 1.1 0.1 setosa 4.4 2 2.9 1.4 0.2 setosa 4.4 3 3.0 1.3 0.2 setosa 4.4 4 3.2 1.3 0.2 setosa 4.5 5 2.3 1.3 0.3 setosa 4.6 6 3.1 1.5 0.2 setosa 4.6 7 3.4 1.4 0.3 setosa 4.6 8 3.6 1.0 0.2 setosa 4.6 9 3.2 1.4 0.2 setosa 4.7 10 3.2 1.3 0.2 setosa 2.2.1.2 Populate Value from Lowest Index to All other Rows We would like to calculate for example the ratio of each individual’s highest to the the person with the lowest height in a dataset. We first need to generated sorted index from lowest to highest, and then populate the lowest height to all rows, and then divide. Search Terms: r spread value to all rows from one row r other rows equal to the value of one row Conditional assignment of one variable to the value of one of two other variables dplyr mutate conditional dplyr value from one row to all rows dplyr mutate equal to value in another cell Links: see: dplyr rank see: dplyr case_when 2.2.1.2.1 Short Method: mutate and min We just want the lowest value to be in its own column, so that we can compute various statistics using the lowest value variable and the original variable. # 1. Sort df_iris_m1 &lt;- iris %&gt;% mutate(Sepal.Len.Lowest.all = min(Sepal.Length)) %&gt;% select(Sepal.Length, Sepal.Len.Lowest.all, everything()) # Show results Head 10 df_iris_m1 %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Lowest.all Sepal.Width Petal.Length Petal.Width Species 5.1 4.3 3.5 1.4 0.2 setosa 4.9 4.3 3.0 1.4 0.2 setosa 4.7 4.3 3.2 1.3 0.2 setosa 4.6 4.3 3.1 1.5 0.2 setosa 5.0 4.3 3.6 1.4 0.2 setosa 5.4 4.3 3.9 1.7 0.4 setosa 4.6 4.3 3.4 1.4 0.3 setosa 5.0 4.3 3.4 1.5 0.2 setosa 4.4 4.3 2.9 1.4 0.2 setosa 4.9 4.3 3.1 1.5 0.1 setosa 2.2.1.2.2 Long Method: row_number and case_when This is the long method, using row_number, and case_when. The benefit of this method is that it generates several intermediate variables that might be useful. And the key final step is to set a new variable (A=Sepal.Len.Lowest.all) equal to another variable’s (B=Sepal.Length’s) value at the index that satisfies condition based a third variable (C=Sepal.Len.Index). # 1. Sort # 2. generate index # 3. value at lowest index (case_when) # 4. spread value from lowest index to other rows # Note step 4 does not require step 3 df_iris_m2 &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% mutate(Sepal.Len.Lowest.one = case_when(row_number()==1 ~ Sepal.Length)) %&gt;% mutate(Sepal.Len.Lowest.all = Sepal.Length[Sepal.Len.Index==1]) %&gt;% select(Sepal.Length, Sepal.Len.Index, Sepal.Len.Lowest.one, Sepal.Len.Lowest.all) # Show results Head 10 df_iris_m2 %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Len.Lowest.one Sepal.Len.Lowest.all 4.3 1 4.3 4.3 4.4 2 NA 4.3 4.4 3 NA 4.3 4.4 4 NA 4.3 4.5 5 NA 4.3 4.6 6 NA 4.3 4.6 7 NA 4.3 4.6 8 NA 4.3 4.6 9 NA 4.3 4.7 10 NA 4.3 2.2.1.3 Generate Sorted Index based on Deviations Generate Positive and Negative Index based on Ordered Deviation from some Number. There is a variable that is continuous, substract a number from this variable, and generate index based on deviations. Think of the index as generating intervals indicating where the value lies. 0th index indicates the largest value in sequence that is smaller than or equal to number \\(x\\), 1st index indicates the smallest value in sequence that is larger than number \\(x\\). The solution below is a little bit convoluated and long, there is likely a much quicker way. The process below shows various intermediary outputs that help arrive at deviation index Sepal.Len.Devi.Index from initial sorted index Sepal.Len.Index. search: dplyr arrange ignore na dplyr index deviation from order number sequence dplyr index below above dplyr index order below above value # 1. Sort and generate variable equal to sorted index # 2. Plus or minus deviations from some value # 3. Find the zero, which means, the number closests to zero including zero from the negative side # 4. Find the index at the highest zero and below deviation point # 5. Difference of zero index and original sorted index sc_val_x &lt;- 4.65 df_iris_deviate &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% mutate(Sepal.Len.Devi = (Sepal.Length - sc_val_x)) %&gt;% mutate(Sepal.Len.Devi.Neg = case_when(Sepal.Len.Devi &lt;= 0 ~ (-1)*(Sepal.Len.Devi))) %&gt;% arrange((Sepal.Len.Devi.Neg), desc(Sepal.Len.Index)) %&gt;% mutate(Sepal.Len.Index.Zero = case_when(row_number() == 1 ~ Sepal.Len.Index)) %&gt;% mutate(Sepal.Len.Devi.Index = Sepal.Len.Index - Sepal.Len.Index.Zero[row_number() == 1]) %&gt;% arrange(Sepal.Len.Index) %&gt;% select(Sepal.Length, Sepal.Len.Index, Sepal.Len.Devi, Sepal.Len.Devi.Neg, Sepal.Len.Index.Zero, Sepal.Len.Devi.Index) # Show results Head 10 df_iris_deviate %&gt;% head(20) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Len.Devi Sepal.Len.Devi.Neg Sepal.Len.Index.Zero Sepal.Len.Devi.Index 4.3 1 -0.35 0.35 NA -8 4.4 2 -0.25 0.25 NA -7 4.4 3 -0.25 0.25 NA -6 4.4 4 -0.25 0.25 NA -5 4.5 5 -0.15 0.15 NA -4 4.6 6 -0.05 0.05 NA -3 4.6 7 -0.05 0.05 NA -2 4.6 8 -0.05 0.05 NA -1 4.6 9 -0.05 0.05 9 0 4.7 10 0.05 NA NA 1 4.7 11 0.05 NA NA 2 4.8 12 0.15 NA NA 3 4.8 13 0.15 NA NA 4 4.8 14 0.15 NA NA 5 4.8 15 0.15 NA NA 6 4.8 16 0.15 NA NA 7 4.9 17 0.25 NA NA 8 4.9 18 0.25 NA NA 9 4.9 19 0.25 NA NA 10 4.9 20 0.25 NA NA 11 2.3 Group Statistics 2.3.1 Groups Statistics Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 2.3.1.1 Aggrgate Groups only Unique Group and Count There are two variables that are numeric, we want to find all the unique groups of these two variables in a dataset and count how many times each unique group occurs r unique occurrence of numeric groups How to add count of unique values by group to R data.frame # Numeric value combinations unique Groups vars.group &lt;- c(&#39;hgt0&#39;, &#39;wgt0&#39;) # dataset subsetting df_use &lt;- df_hgt_wgt %&gt;% select(!!!syms(c(vars.group))) %&gt;% mutate(hgt0 = round(hgt0/5)*5, wgt0 = round(wgt0/2000)*2000) %&gt;% drop_na() # Group, count and generate means for each numeric variables # mutate_at(vars.group, funs(as.factor(.))) %&gt;% df.group.count &lt;- df_use %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(vars.group)) %&gt;% summarise(n_obs_group=n()) # Show results Head 10 df.group.count %&gt;% kable() %&gt;% kable_styling_fc() hgt0 wgt0 n_obs_group 40 2000 122 45 2000 4586 45 4000 470 50 2000 9691 50 4000 13106 55 2000 126 55 4000 1900 60 6000 18 2.3.1.2 Aggrgate Groups only Unique Group Show up With Means Several variables that are grouping identifiers. Several variables that are values which mean be unique for each group members. For example, a Panel of income for N households over T years with also household education information that is invariant over time. Want to generate a dataset where the unit of observation are households, rather than household years. Take average of all numeric variables that are household and year specific. A complicating factor potentially is that the number of observations differ within group, for example, income might be observed for all years for some households but not for other households. r dplyr aggregate group average Aggregating and analyzing data with dplyr column can’t be modified because it is a grouping variable see also: Aggregating and analyzing data with dplyr # In the df_hgt_wgt from R4Econ, there is a country id, village id, # and individual id, and various other statistics vars.group &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;) vars.values &lt;- c(&#39;hgt&#39;, &#39;momEdu&#39;) # dataset subsetting df_use &lt;- df_hgt_wgt %&gt;% select(!!!syms(c(vars.group, vars.values))) # Group, count and generate means for each numeric variables df.group &lt;- df_use %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(vars.group)) %&gt;% summarise_if(is.numeric, funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE), n = sum(is.na(.)==0))) # Show results Head 10 df.group %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id hgt_mean momEdu_mean hgt_sd momEdu_sd hgt_n momEdu_n Cebu 1 1 61.80000 5.3 9.520504 0 7 18 Cebu 1 2 68.86154 7.1 9.058931 0 13 18 Cebu 1 3 80.45882 9.4 29.894231 0 17 18 Cebu 1 4 88.10000 13.9 35.533166 0 18 18 Cebu 1 5 97.70556 11.3 41.090366 0 18 18 Cebu 1 6 87.49444 7.3 35.586439 0 18 18 Cebu 1 7 90.79412 10.4 38.722385 0 17 18 Cebu 1 8 68.45385 13.5 10.011961 0 13 18 Cebu 1 9 86.21111 10.4 35.126057 0 18 18 Cebu 1 10 87.67222 10.5 36.508127 0 18 18 # Show results Head 10 df.group %&gt;% tail(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id hgt_mean momEdu_mean hgt_sd momEdu_sd hgt_n momEdu_n Guatemala 14 2014 66.97000 NaN 8.967974 NaN 10 0 Guatemala 14 2015 71.71818 NaN 11.399984 NaN 11 0 Guatemala 14 2016 66.33000 NaN 9.490352 NaN 10 0 Guatemala 14 2017 76.40769 NaN 14.827871 NaN 13 0 Guatemala 14 2018 74.55385 NaN 12.707846 NaN 13 0 Guatemala 14 2019 70.47500 NaN 11.797390 NaN 12 0 Guatemala 14 2020 60.28750 NaN 7.060036 NaN 8 0 Guatemala 14 2021 84.96000 NaN 15.446193 NaN 10 0 Guatemala 14 2022 79.38667 NaN 15.824749 NaN 15 0 Guatemala 14 2023 66.50000 NaN 8.613113 NaN 8 0 2.3.2 One Variable Group Summary Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. There is a categorical variable (based on one or the interaction of multiple variables), there is a continuous variable, obtain statistics for the continuous variable conditional on the categorical variable, but also unconditionally. Store results in a matrix, but also flatten results wide to row with appropriate keys/variable-names for all group statistics. Pick which statistics to be included in final wide row 2.3.2.1 Build Program # Single Variable Group Statistics (also generate overall statistics) ff_summ_by_group_summ_one &lt;- function( df, vars.group, var.numeric, str.stats.group = &#39;main&#39;, str.stats.specify = NULL, boo.overall.stats = TRUE){ # List of statistics # https://rdrr.io/cran/dplyr/man/summarise.html strs.center &lt;- c(&#39;mean&#39;, &#39;median&#39;) strs.spread &lt;- c(&#39;sd&#39;, &#39;IQR&#39;, &#39;mad&#39;) strs.range &lt;- c(&#39;min&#39;, &#39;max&#39;) strs.pos &lt;- c(&#39;first&#39;, &#39;last&#39;) strs.count &lt;- c(&#39;n_distinct&#39;) # Grouping of Statistics if (missing(str.stats.specify)) { if (str.stats.group == &#39;main&#39;) { strs.all &lt;- c(&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;, &#39;sd&#39;) } if (str.stats.group == &#39;all&#39;) { strs.all &lt;- c(strs.center, strs.spread, strs.range, strs.pos, strs.count) } } else { strs.all &lt;- str.stats.specify } # Start Transform df &lt;- df %&gt;% drop_na() %&gt;% mutate(!!(var.numeric) := as.numeric(!!sym(var.numeric))) # Overall Statistics if (boo.overall.stats) { df.overall.stats &lt;- df %&gt;% summarize_at(vars(var.numeric), funs(!!!strs.all)) if (length(strs.all) == 1) { # give it a name, otherwise if only one stat, name of stat not saved df.overall.stats &lt;- df.overall.stats %&gt;% rename(!!strs.all := !!sym(var.numeric)) } names(df.overall.stats) &lt;- paste0(var.numeric, &#39;.&#39;, names(df.overall.stats)) } # Group Sort df.select &lt;- df %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(c(vars.group, var.numeric))) # Table of Statistics df.table.grp.stats &lt;- df.select %&gt;% summarize_at(vars(var.numeric), funs(!!!strs.all)) # Add Stat Name if (length(strs.all) == 1) { # give it a name, otherwise if only one stat, name of stat not saved df.table.grp.stats &lt;- df.table.grp.stats %&gt;% rename(!!strs.all := !!sym(var.numeric)) } # Row of Statistics str.vars.group.combine &lt;- paste0(vars.group, collapse=&#39;_&#39;) if (length(vars.group) == 1) { df.row.grp.stats &lt;- df.table.grp.stats %&gt;% mutate(!!(str.vars.group.combine) := paste0(var.numeric, &#39;.&#39;, vars.group, &#39;.g&#39;, (!!!syms(vars.group)))) %&gt;% gather(variable, value, -one_of(vars.group)) %&gt;% unite(str.vars.group.combine, c(str.vars.group.combine, &#39;variable&#39;)) %&gt;% spread(str.vars.group.combine, value) } else { df.row.grp.stats &lt;- df.table.grp.stats %&gt;% mutate(vars.groups.combine := paste0(paste0(vars.group, collapse=&#39;.&#39;)), !!(str.vars.group.combine) := paste0(interaction(!!!(syms(vars.group))))) %&gt;% mutate(!!(str.vars.group.combine) := paste0(var.numeric, &#39;.&#39;, vars.groups.combine, &#39;.&#39;, (!!sym(str.vars.group.combine)))) %&gt;% ungroup() %&gt;% select(-vars.groups.combine, -one_of(vars.group)) %&gt;% gather(variable, value, -one_of(str.vars.group.combine)) %&gt;% unite(str.vars.group.combine, c(str.vars.group.combine, &#39;variable&#39;)) %&gt;% spread(str.vars.group.combine, value) } # Clean up name strings names(df.table.grp.stats) &lt;- gsub(x = names(df.table.grp.stats),pattern = &quot;_&quot;, replacement = &quot;\\\\.&quot;) names(df.row.grp.stats) &lt;- gsub(x = names(df.row.grp.stats),pattern = &quot;_&quot;, replacement = &quot;\\\\.&quot;) # Return list.return &lt;- list(df_table_grp_stats = df.table.grp.stats, df_row_grp_stats = df.row.grp.stats) # Overall Statistics, without grouping if (boo.overall.stats) { df.row.stats.all &lt;- c(df.row.grp.stats, df.overall.stats) list.return &lt;- append(list.return, list(df_overall_stats = df.overall.stats, df_row_stats_all = df.row.stats.all)) } # Return return(list.return) } 2.3.2.2 Test Load data and test # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) 2.3.2.2.1 Function Testing By Gender Groups Need two variables, a group variable that is a factor, and a numeric vars.group &lt;- &#39;sex&#39; var.numeric &lt;- &#39;hgt&#39; df.select &lt;- df %&gt;% select(one_of(vars.group, var.numeric)) %&gt;% drop_na() Main Statistics: # Single Variable Group Statistics ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.group = &#39;main&#39;)$df_table_grp_stats Specify Two Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;, &#39;sd&#39;))$df_table_grp_stats Specify One Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;))$df_table_grp_stats 2.3.2.2.2 Function Testing By Country and Gender Groups Need two variables, a group variable that is a factor, and a numeric. Now joint grouping variables. vars.group &lt;- c(&#39;S.country&#39;, &#39;sex&#39;) var.numeric &lt;- &#39;hgt&#39; df.select &lt;- df %&gt;% select(one_of(vars.group, var.numeric)) %&gt;% drop_na() Main Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.group = &#39;main&#39;)$df_table_grp_stats Specify Two Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;, &#39;sd&#39;))$df_table_grp_stats Specify One Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;))$df_table_grp_stats 2.3.3 Nested within Group Stats Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. By Multiple within Individual Groups Variables, Averages for All Numeric Variables within All Groups of All Group Variables (Long to very Wide). Suppose you have an individual level final outcome. The individual is observed for N periods, where each period the inputs differ. What inputs impacted the final outcome? Suppose we can divide N periods in which the individual is in the data into a number of years, a number of semi-years, a number of quarters, or uneven-staggered lengths. We might want to generate averages across individuals and within each of these different possible groups averages of inputs. Then we want to version of the data where each row is an individual, one of the variables is the final outcome, and the other variables are these different averages: averages for the 1st, 2nd, 3rd year in which indivdiual is in data, averages for 1st, …, final quarter in which indivdiual is in data. 2.3.3.1 Build Function This function takes as inputs: vars.not.groups2avg: a list of variables that are not the within-indivdiual or across-individual grouping variables, but the variables we want to average over. Withnin indivdiual grouping averages will be calculated for these variables using the not-listed variables as within indivdiual groups (excluding vars.indi.grp groups). vars.indi.grp: a list or individual variables, and also perhaps villages, province, etc id variables that are higher than individual ID. Note the groups are are ACROSS individual higher level group variables. the remaining variables are all within individual grouping variables. the function output is a dataframe: each row is an individual initial variables individual ID and across individual groups from vars.indi.grp. other variables are all averages for the variables in vars.not.groups2avg if there are 2 within individual group variables, and the first has 3 groups (years), the second has 6 groups (semi-years), then there would be 9 average variables. each average variables has the original variable name from vars.not.groups2avg plus the name of the within individual grouping variable, and at the end ‘c_x’, where x is a integer representing the category within the group (if 3 years, x=1, 2, 3) # Data Function # https://fanwangecon.github.io/R4Econ/summarize/summ/ByGroupsSummWide.html f.by.groups.summ.wide &lt;- function(df.groups.to.average, vars.not.groups2avg, vars.indi.grp = c(&#39;S.country&#39;,&#39;ID&#39;), display=TRUE) { # 1. generate categoricals for full year (m.12), half year (m.6), quarter year (m.4) # 2. generate categoricals also for uneven years (m12t14) using # stagger (+2 rather than -1) # 3. reshape wide to long, so that all categorical date groups appear in var=value, # and categories in var=variable # 4. calculate mean for all numeric variables for all date groups # 5. combine date categorical variable and value, single var: # m.12.c1= first year average from m.12 averaging ######## ######## ######## ######## ####### # Step 1 ######## ######## ######## ######## ####### # 1. generate categoricals for full year (m.12), half year (m.6), quarter year (m.4) # 2. generate categoricals also for uneven years (m12t14) using stagger # (+2 rather than -1) ######## ######## ######## ######## ####### # S2: reshape wide to long, so that all categorical date groups appear in var=value, # and categories in var=variable; calculate mean for all # numeric variables for all date groups ######## ######## ######## ######## ####### df.avg.long &lt;- df.groups.to.average %&gt;% gather(variable, value, -one_of(c(vars.indi.grp, vars.not.groups2avg))) %&gt;% group_by(!!!syms(vars.indi.grp), variable, value) %&gt;% summarise_if(is.numeric, funs(mean(., na.rm = TRUE))) if (display){ dim(df.avg.long) options(repr.matrix.max.rows=10, repr.matrix.max.cols=20) print(df.avg.long) } ######## ######## ######## ######## ####### # S3 combine date categorical variable and value, single var: # m.12.c1= first year average from m.12 averaging; to do this make # data even longer first ######## ######## ######## ######## ####### # We already have the averages, but we want them to show up as variables, # mean for each group of each variable. df.avg.allvars.wide &lt;- df.avg.long %&gt;% ungroup() %&gt;% mutate(all_m_cate = paste0(variable, &#39;_c&#39;, value)) %&gt;% select(all_m_cate, everything(), -variable, -value) %&gt;% gather(variable, value, -one_of(vars.indi.grp), -all_m_cate) %&gt;% unite(&#39;var_mcate&#39;, variable, all_m_cate) %&gt;% spread(var_mcate, value) if (display){ dim(df.avg.allvars.wide) options(repr.matrix.max.rows=10, repr.matrix.max.cols=10) print(df.avg.allvars.wide) } return(df.avg.allvars.wide) } 2.3.3.2 Test Program In our sample dataset, the number of nutrition/height/income etc information observed within each country and month of age group are different. We have a panel dataset for children observed over different months of age. We have two key grouping variables: 1. country: data are observed for guatemala and cebu 2. month-age (survey month round=svymthRound): different months of age at which each individual child is observed A child could be observed for many months, or just a few months. A child’s height information could be observed for more months-of-age than nutritional intake information. We eventually want to run regressions where the outcome is height/weight and the input is nutrition. The regressions will be at the month-of-age level. We need to know how many times different variables are observed at the month-of-age level. # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) 2.3.3.2.1 Generate Within Individual Groups In the data, children are observed for different number of months since birth. We want to calculate quarterly, semi-year, annual, etc average nutritional intakes. First generate these within-individual grouping variables. We can also generate uneven-staggered calendar groups as shown below. mth.var &lt;- &#39;svymthRound&#39; df.groups.to.average&lt;- df %&gt;% filter(!!sym(mth.var) &gt;= 0 &amp; !!sym(mth.var) &lt;= 24) %&gt;% mutate(m12t24=(floor((!!sym(mth.var) - 12) %/% 14) + 1), m8t24=(floor((!!sym(mth.var) - 8) %/% 18) + 1), m12 = pmax((floor((!!sym(mth.var)-1) %/% 12) + 1), 1), m6 = pmax((floor((!!sym(mth.var)-1) %/% 6) + 1), 1), m3 = pmax((floor((!!sym(mth.var)-1) %/% 3) + 1), 1)) # Show Results options(repr.matrix.max.rows=30, repr.matrix.max.cols=20) vars.arrange &lt;- c(&#39;S.country&#39;,&#39;indi.id&#39;,&#39;svymthRound&#39;) vars.groups.within.indi &lt;- c(&#39;m12t24&#39;, &#39;m8t24&#39;, &#39;m12&#39;, &#39;m6&#39;, &#39;m3&#39;) as.tibble(df.groups.to.average %&gt;% group_by(!!!syms(vars.arrange)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% select(!!!syms(vars.arrange), !!!syms(vars.groups.within.indi))) 2.3.3.2.2 Within Group Averages With the within-group averages created, we can generate averages for all variables within these groups. vars.not.groups2avg &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.indi.grp &lt;- c(&#39;S.country&#39;, &#39;indi.id&#39;) vars.groups.within.indi &lt;- c(&#39;m12t24&#39;, &#39;m8t24&#39;, &#39;m12&#39;, &#39;m6&#39;, &#39;m3&#39;) df.groups.to.average.select &lt;- df.groups.to.average %&gt;% select(one_of(c(vars.indi.grp, vars.not.groups2avg, vars.groups.within.indi))) df.avg.allvars.wide &lt;- f.by.groups.summ.wide(df.groups.to.average.select, vars.not.groups2avg, vars.indi.grp, display=FALSE) This is the tabular version of results dim(df.avg.allvars.wide) ## [1] 2023 38 names(df.avg.allvars.wide) ## [1] &quot;S.country&quot; &quot;indi.id&quot; &quot;cal_m12_c1&quot; &quot;cal_m12_c2&quot; &quot;cal_m12t24_c0&quot; &quot;cal_m12t24_c1&quot; &quot;cal_m3_c1&quot; &quot;cal_m3_c2&quot; &quot;cal_m3_c3&quot; ## [10] &quot;cal_m3_c4&quot; &quot;cal_m3_c5&quot; &quot;cal_m3_c6&quot; &quot;cal_m3_c7&quot; &quot;cal_m3_c8&quot; &quot;cal_m6_c1&quot; &quot;cal_m6_c2&quot; &quot;cal_m6_c3&quot; &quot;cal_m6_c4&quot; ## [19] &quot;cal_m8t24_c0&quot; &quot;cal_m8t24_c1&quot; &quot;prot_m12_c1&quot; &quot;prot_m12_c2&quot; &quot;prot_m12t24_c0&quot; &quot;prot_m12t24_c1&quot; &quot;prot_m3_c1&quot; &quot;prot_m3_c2&quot; &quot;prot_m3_c3&quot; ## [28] &quot;prot_m3_c4&quot; &quot;prot_m3_c5&quot; &quot;prot_m3_c6&quot; &quot;prot_m3_c7&quot; &quot;prot_m3_c8&quot; &quot;prot_m6_c1&quot; &quot;prot_m6_c2&quot; &quot;prot_m6_c3&quot; &quot;prot_m6_c4&quot; ## [37] &quot;prot_m8t24_c0&quot; &quot;prot_m8t24_c1&quot; df.avg.allvars.wide[1:20,] %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country indi.id cal_m12_c1 cal_m12_c2 cal_m12t24_c0 cal_m12t24_c1 cal_m3_c1 cal_m3_c2 cal_m3_c3 cal_m3_c4 cal_m3_c5 cal_m3_c6 cal_m3_c7 cal_m3_c8 cal_m6_c1 cal_m6_c2 cal_m6_c3 cal_m6_c4 cal_m8t24_c0 cal_m8t24_c1 prot_m12_c1 prot_m12_c2 prot_m12t24_c0 prot_m12t24_c1 prot_m3_c1 prot_m3_c2 prot_m3_c3 prot_m3_c4 prot_m3_c5 prot_m3_c6 prot_m3_c7 prot_m3_c8 prot_m6_c1 prot_m6_c2 prot_m6_c3 prot_m6_c4 prot_m8t24_c0 prot_m8t24_c1 Cebu 1 132.15714 NaN 97.08333 342.6000 9.10 95.50 85.3 315.30 NaN NaN NaN NaN 52.300 238.63333 NaN NaN 52.300 238.6333 5.3571429 NaN 4.3666667 11.300000 0.65 3.65 2.6 13.15 NaN NaN NaN NaN 2.150 9.6333333 NaN NaN 2.150 9.633333 Cebu 2 90.72857 255.6500 81.46667 240.0286 83.35 12.30 155.1 144.35 228.0 152.85 305.0 347.60 47.825 147.93333 177.9000 333.4000 47.825 219.7444 3.1857143 8.550000 2.7333333 8.171429 3.20 1.25 5.2 4.10 5.4 5.15 7.7 13.95 2.225 4.4666667 5.233333 11.866667 2.225 7.188889 Cebu 3 96.80000 658.8167 31.56667 634.4429 0.50 28.85 57.0 280.95 459.3 549.95 612.0 890.85 14.675 206.30000 519.7333 797.9000 14.675 507.9778 4.5000000 21.116667 1.6833333 21.157143 1.05 2.15 2.3 11.40 18.5 18.05 18.0 27.05 1.600 8.3666667 18.200000 24.033333 1.600 16.866667 Cebu 4 27.45714 371.7000 24.55000 325.0143 4.50 25.95 39.4 45.95 221.2 271.00 581.3 442.85 15.225 43.76667 254.4000 489.0000 15.225 262.3889 0.8714286 6.850000 0.9000000 5.971429 0.75 1.10 1.2 0.60 1.8 4.85 10.1 9.75 0.925 0.8000000 3.833333 9.866667 0.925 4.833333 Cebu 5 101.34286 1080.8500 79.15000 959.9429 14.10 143.80 71.3 161.15 452.6 1345.20 1178.1 1082.00 78.950 131.20000 1047.6667 1114.0333 78.950 764.3000 2.4000000 19.483333 2.3166667 17.114286 1.35 3.00 3.4 2.35 7.1 23.15 24.5 19.50 2.175 2.7000000 17.800000 21.166667 2.175 13.888889 Cebu 6 185.35714 521.5333 162.23333 493.3286 23.85 184.70 169.1 355.65 653.4 506.50 416.8 523.00 104.275 293.46667 555.4667 487.6000 104.275 445.5111 8.4000000 15.116667 7.3833333 15.028571 0.85 7.40 9.8 16.25 26.8 14.10 11.4 12.15 4.125 14.1000000 18.333333 11.900000 4.125 14.777778 Cebu 7 157.25714 570.9800 145.50000 513.7833 8.30 137.80 407.8 200.40 390.6 637.10 688.1 569.55 73.050 269.53333 513.8500 609.0667 73.050 457.9375 3.3000000 20.440000 2.7833333 18.100000 0.95 1.70 8.6 4.60 16.4 23.00 21.5 20.65 1.325 5.9333333 19.700000 20.933333 1.325 15.000000 Cebu 8 471.92857 844.8333 379.20000 871.0429 158.95 423.00 417.5 861.05 691.3 897.95 637.1 972.35 290.975 713.20000 829.0667 860.6000 290.975 800.9556 13.6857143 32.716667 11.0166667 32.285714 3.90 11.35 10.8 27.25 42.7 26.45 25.8 37.45 7.625 21.7666667 31.866667 33.566667 7.625 29.066667 Cebu 9 32.27143 415.2167 16.58333 373.9571 5.05 10.40 15.1 89.95 142.4 203.60 753.2 594.25 7.725 65.00000 183.2000 647.2333 7.725 298.4778 0.9571429 18.283333 0.9166667 15.842857 0.50 0.50 0.5 2.10 4.2 10.85 39.5 22.15 0.500 1.5666667 8.633333 27.933333 0.500 12.711111 Cebu 10 67.18571 395.2500 68.58333 347.1857 9.55 26.40 164.6 116.90 296.6 303.00 385.1 541.90 17.975 132.80000 300.8667 489.6333 17.975 307.7667 2.0428571 8.466667 1.9333333 7.642857 0.85 0.50 4.9 3.35 7.5 6.05 9.2 11.00 0.675 3.8666667 6.533333 10.400000 0.675 6.933333 Cebu 11 14.90000 245.3833 11.80000 215.1143 0.50 5.20 30.0 31.45 126.2 223.05 239.6 330.20 2.850 30.96667 190.7667 300.0000 2.850 173.9111 1.0285714 6.833333 1.1166667 5.928571 0.80 1.70 1.2 0.50 3.6 6.35 7.3 8.70 1.250 0.7333333 5.433333 8.233333 1.250 4.800000 Cebu 12 453.61429 745.6833 419.51667 733.1857 325.60 483.65 463.0 546.90 766.8 676.85 998.5 677.55 404.625 518.93333 706.8333 784.5333 404.625 670.1000 14.7714286 22.133333 14.1666667 21.600000 7.40 13.60 26.6 17.40 18.6 20.35 25.6 23.95 10.500 20.4666667 19.766667 24.500000 10.500 21.577778 Cebu 13 47.51429 210.2500 36.81667 196.1714 17.45 40.00 28.5 94.60 216.9 195.15 281.3 186.50 28.725 72.56667 202.4000 218.1000 28.725 164.3556 1.9571429 6.800000 1.6666667 6.357143 0.70 1.50 2.1 3.60 8.3 4.75 6.2 8.40 1.100 3.1000000 5.933333 7.666667 1.100 5.566667 Cebu 14 608.85714 924.5833 527.30000 949.3857 259.05 554.30 688.1 973.60 525.5 1039.60 800.0 1071.40 406.675 878.43333 868.2333 980.9333 406.675 909.2000 24.6714286 28.050000 23.0833333 28.928571 15.05 28.85 32.6 26.15 12.7 29.75 29.5 33.30 21.950 28.3000000 24.066667 32.033333 21.950 28.133333 Cebu 15 74.67143 440.0833 64.21667 396.8429 62.40 39.60 80.1 119.30 292.5 237.45 607.8 632.65 51.000 106.23333 255.8000 624.3667 51.000 328.8000 2.2571429 10.633333 1.6333333 9.971429 1.65 0.90 1.5 4.60 9.4 5.80 13.6 14.60 1.275 3.5666667 7.000000 14.266667 1.275 8.277778 Cebu 16 128.45714 519.9333 90.50000 496.5429 4.80 11.00 205.3 331.15 290.8 354.50 563.3 778.25 7.900 289.20000 333.2667 706.6000 7.900 443.0222 4.6857143 18.083333 4.1000000 16.671429 0.65 2.70 8.7 8.70 7.8 12.95 25.9 24.45 1.675 8.7000000 11.233333 24.933333 1.675 14.955556 Cebu 17 130.78571 718.8667 97.48333 663.4000 5.50 7.80 249.9 319.50 774.6 892.90 551.9 600.45 6.650 296.30000 853.4667 584.2667 6.650 578.0111 2.8142857 19.716667 2.2166667 17.814286 0.60 0.50 6.1 5.70 20.1 21.55 24.9 15.10 0.550 5.8333333 21.066667 18.366667 0.550 15.088889 Cebu 18 172.64000 497.9500 172.64000 497.9500 29.60 234.40 NaN 335.20 NaN NaN NaN 497.95 132.000 335.20000 NaN 497.9500 132.000 443.7000 11.7000000 19.100000 11.7000000 19.100000 2.95 17.90 NaN 16.80 NaN NaN NaN 19.10 10.425 16.8000000 NaN 19.100000 10.425 18.333333 Cebu 19 74.45714 314.7333 80.10000 275.5714 3.65 95.70 171.3 75.60 131.3 350.50 304.6 375.75 49.675 107.50000 277.4333 352.0333 49.675 245.6556 2.5428571 10.466667 2.8833333 9.042857 0.50 2.95 6.7 2.10 3.4 11.40 12.3 12.15 1.725 3.6333333 8.733333 12.200000 1.725 8.188889 Cebu 20 110.90000 583.2000 80.51667 541.7714 7.30 120.65 77.8 221.30 391.2 582.10 466.1 738.85 63.975 173.46667 518.4667 647.9333 63.975 446.6222 3.2000000 16.966667 2.1833333 15.871429 0.50 3.85 2.7 5.50 7.9 20.15 11.8 20.90 2.175 4.5666667 16.066667 17.866667 2.175 12.833333 2.4 Distributional Statistics 2.4.1 Histogram 2.4.1.1 Generate Test Score Dataset Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r generate text string as csv r tibble matrix hand input First, we will generate a test score dataset, directly from string. Below we type line by line a dataset with four variables in comma separated (csv) format, where the first row includes the variables names. These texts could be stored in a separate file, or they could be directly included in code and read in as csv 2.4.1.1.1 A Dataset with only Two Continuous Variable ar_test_scores_ec3 &lt;- c(107.72,101.28,105.92,109.31,104.27,110.27,91.92846154,81.8,109.0071429,103.07,98.97923077,101.91,96.49,97.79923077,99.07846154,99.17,103.51,112.2225,101.2964286,94.5,98.92,97.09,93.83989011,97.36304945,80.34,65.74,85.275,82.19708791,86.53758242,86.2025,86.63,82.57392857,83.66,79.76,75.55642857,86.32571429,66.41,76.06,44.225,82.28,47.77392857,70.005,69.13769231,73.52571429,60.51,56.04) ar_test_scores_ec1 &lt;- c(101.72,101.28,99.92,103.31,100.27,104.27,90.23615385,77.8,103.4357143,97.07,93.13307692,95.91,92.49,93.95307692,95.38615385,97.17,99.51,100.3475,95.83214286,92.5,94.92,91.09,90.4332967,93.52101648,80.34,59.74,79.525,77.67236264,81.59252747,82.3275,80.63,76.98464286,81.66,79.76,70.59214286,82.46857143,66.41,74.06,40.475,76.28,44.18464286,66.255,65.59923077,69.66857143,60.51,56.04) mt_test_scores &lt;- cbind(ar_test_scores_ec1, ar_test_scores_ec3) ar_st_varnames &lt;- c(&#39;course_total_ec1p&#39;,&#39;course_total_ec3p&#39;) tb_final_twovar &lt;- as_tibble(mt_test_scores) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_final_twovar) ## course_total_ec1p course_total_ec3p ## Min. : 40.48 Min. : 44.23 ## 1st Qu.: 76.46 1st Qu.: 79.91 ## Median : 86.35 Median : 89.28 ## Mean : 83.88 Mean : 87.90 ## 3rd Qu.: 95.89 3rd Qu.:100.75 ## Max. :104.27 Max. :112.22 ff_summ_percentiles(df = tb_final_twovar, bl_statsasrows = TRUE, col2varname = FALSE) 2.4.1.1.2 A Dataset with one Continuous Variable and Histogram ar_final_scores &lt;- c(94.28442509,95.68817475,97.25219512,77.89268293,95.08795497,93.27380863,92.3,84.25317073,86.08642991,84.69219512,71.43634146,76.21365854,71.68878049,77.46142589,79.29579268,43.7285453,63.80634146,67.92994774,100.8980488,100.0857143,99.93073171,98.4102439,97.93,97.10359756,96.97121951,96.60292683,96.23317073,93.92243902,93.82243902,92.75390244,92.65775261,92.20444653,91.73463415,90.38321161,89.37414634,86.95932458,79.58686411,78.70878049,77.2497561,76.88195122,76.52987805,74.72114313,74.27488676,71.30268293,63.70256098,37.90426829,2.292682927) mt_test_scores &lt;- cbind(seq(1,length(ar_final_scores)), ar_final_scores) ar_st_varnames &lt;- c(&#39;index&#39;, &#39;course_final&#39;) tb_onevar &lt;- as_tibble(mt_test_scores) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_onevar) ## index course_final ## Min. : 1.0 Min. : 2.293 ## 1st Qu.:12.5 1st Qu.: 76.372 ## Median :24.0 Median : 86.959 ## Mean :24.0 Mean : 82.415 ## 3rd Qu.:35.5 3rd Qu.: 94.686 ## Max. :47.0 Max. :100.898 ff_summ_percentiles(df = tb_onevar, bl_statsasrows = TRUE, col2varname = FALSE) 2.4.1.1.3 A Dataset with Multiple Variables #load in data empirically by hand txt_test_data &lt;- &quot;init_prof, later_prof, class_id, exam_score &#39;SW&#39;, &#39;SW&#39;, 1, 102 &#39;SW&#39;, &#39;SW&#39;, 1, 102 &#39;SW&#39;, &#39;SW&#39;, 1, 101 &#39;SW&#39;, &#39;SW&#39;, 1, 100 &#39;SW&#39;, &#39;SW&#39;, 1, 100 &#39;SW&#39;, &#39;SW&#39;, 1, 99 &#39;SW&#39;, &#39;SW&#39;, 1, 98.5 &#39;SW&#39;, &#39;SW&#39;, 1, 98.5 &#39;SW&#39;, &#39;SW&#39;, 1, 97 &#39;SW&#39;, &#39;SW&#39;, 1, 95 &#39;SW&#39;, &#39;SW&#39;, 1, 94 &#39;SW&#39;, &#39;SW&#39;, 1, 91 &#39;SW&#39;, &#39;SW&#39;, 1, 91 &#39;SW&#39;, &#39;SW&#39;, 1, 90 &#39;SW&#39;, &#39;SW&#39;, 1, 89 &#39;SW&#39;, &#39;SW&#39;, 1, 88.5 &#39;SW&#39;, &#39;SW&#39;, 1, 88 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 86 &#39;SW&#39;, &#39;SW&#39;, 1, 86 &#39;SW&#39;, &#39;SW&#39;, 1, 84 &#39;SW&#39;, &#39;SW&#39;, 1, 82 &#39;SW&#39;, &#39;SW&#39;, 1, 78.5 &#39;SW&#39;, &#39;SW&#39;, 1, 76 &#39;SW&#39;, &#39;SW&#39;, 1, 72 &#39;SW&#39;, &#39;SW&#39;, 1, 70.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67 &#39;SW&#39;, &#39;SW&#39;, 1, 63.5 &#39;SW&#39;, &#39;SW&#39;, 1, 60 &#39;SW&#39;, &#39;SW&#39;, 1, 59 &#39;SW&#39;, &#39;SW&#39;, 1, 44.5 &#39;SW&#39;, &#39;SW&#39;, 1, 44 &#39;SW&#39;, &#39;SW&#39;, 1, 42.5 &#39;SW&#39;, &#39;SW&#39;, 1, 40.5 &#39;SW&#39;, &#39;SW&#39;, 1, 40.5 &#39;SW&#39;, &#39;SW&#39;, 1, 36.5 &#39;SW&#39;, &#39;SW&#39;, 1, 35.5 &#39;SW&#39;, &#39;SW&#39;, 1, 21.5 &#39;SW&#39;, &#39;SW&#39;, 1, 4 &#39;MP&#39;, &#39;MP&#39;, 2, 105 &#39;MP&#39;, &#39;MP&#39;, 2, 103 &#39;MP&#39;, &#39;MP&#39;, 2, 102 &#39;MP&#39;, &#39;MP&#39;, 2, 101 &#39;MP&#39;, &#39;MP&#39;, 2, 101 &#39;MP&#39;, &#39;MP&#39;, 2, 100.5 &#39;MP&#39;, &#39;MP&#39;, 2, 100 &#39;MP&#39;, &#39;MP&#39;, 2, 99 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 96 &#39;MP&#39;, &#39;MP&#39;, 2, 95 &#39;MP&#39;, &#39;MP&#39;, 2, 91 &#39;MP&#39;, &#39;MP&#39;, 2, 89 &#39;MP&#39;, &#39;MP&#39;, 2, 85 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 83.5 &#39;MP&#39;, &#39;MP&#39;, 2, 82.5 &#39;MP&#39;, &#39;MP&#39;, 2, 81.5 &#39;MP&#39;, &#39;MP&#39;, 2, 80.5 &#39;MP&#39;, &#39;MP&#39;, 2, 80 &#39;MP&#39;, &#39;MP&#39;, 2, 77 &#39;MP&#39;, &#39;MP&#39;, 2, 77 &#39;MP&#39;, &#39;MP&#39;, 2, 75 &#39;MP&#39;, &#39;MP&#39;, 2, 75 &#39;MP&#39;, &#39;MP&#39;, 2, 71 &#39;MP&#39;, &#39;MP&#39;, 2, 70 &#39;MP&#39;, &#39;MP&#39;, 2, 68 &#39;MP&#39;, &#39;MP&#39;, 2, 63 &#39;MP&#39;, &#39;MP&#39;, 2, 56 &#39;MP&#39;, &#39;MP&#39;, 2, 56 &#39;MP&#39;, &#39;MP&#39;, 2, 55.5 &#39;MP&#39;, &#39;MP&#39;, 2, 49.5 &#39;MP&#39;, &#39;MP&#39;, 2, 48.5 &#39;MP&#39;, &#39;MP&#39;, 2, 47.5 &#39;MP&#39;, &#39;MP&#39;, 2, 44.5 &#39;MP&#39;, &#39;MP&#39;, 2, 34.5 &#39;MP&#39;, &#39;MP&#39;, 2, 29.5 &#39;CA&#39;, &#39;MP&#39;, 3, 103 &#39;CA&#39;, &#39;MP&#39;, 3, 103 &#39;CA&#39;, &#39;MP&#39;, 3, 101 &#39;CA&#39;, &#39;MP&#39;, 3, 96.5 &#39;CA&#39;, &#39;MP&#39;, 3, 93.5 &#39;CA&#39;, &#39;MP&#39;, 3, 93 &#39;CA&#39;, &#39;MP&#39;, 3, 93 &#39;CA&#39;, &#39;MP&#39;, 3, 92 &#39;CA&#39;, &#39;MP&#39;, 3, 90 &#39;CA&#39;, &#39;MP&#39;, 3, 90 &#39;CA&#39;, &#39;MP&#39;, 3, 89 &#39;CA&#39;, &#39;MP&#39;, 3, 86.5 &#39;CA&#39;, &#39;MP&#39;, 3, 84.5 &#39;CA&#39;, &#39;MP&#39;, 3, 83 &#39;CA&#39;, &#39;MP&#39;, 3, 83 &#39;CA&#39;, &#39;MP&#39;, 3, 82 &#39;CA&#39;, &#39;MP&#39;, 3, 78 &#39;CA&#39;, &#39;MP&#39;, 3, 75 &#39;CA&#39;, &#39;MP&#39;, 3, 74.5 &#39;CA&#39;, &#39;MP&#39;, 3, 70 &#39;CA&#39;, &#39;MP&#39;, 3, 54.5 &#39;CA&#39;, &#39;MP&#39;, 3, 52 &#39;CA&#39;, &#39;MP&#39;, 3, 50 &#39;CA&#39;, &#39;MP&#39;, 3, 42 &#39;CA&#39;, &#39;MP&#39;, 3, 36.5 &#39;CA&#39;, &#39;MP&#39;, 3, 28 &#39;CA&#39;, &#39;MP&#39;, 3, 26 &#39;CA&#39;, &#39;MP&#39;, 3, 11 &#39;CA&#39;, &#39;SN&#39;, 4, 103 &#39;CA&#39;, &#39;SN&#39;, 4, 103 &#39;CA&#39;, &#39;SN&#39;, 4, 102 &#39;CA&#39;, &#39;SN&#39;, 4, 102 &#39;CA&#39;, &#39;SN&#39;, 4, 101 &#39;CA&#39;, &#39;SN&#39;, 4, 100 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 95 &#39;CA&#39;, &#39;SN&#39;, 4, 95 &#39;CA&#39;, &#39;SN&#39;, 4, 92.5 &#39;CA&#39;, &#39;SN&#39;, 4, 92 &#39;CA&#39;, &#39;SN&#39;, 4, 91 &#39;CA&#39;, &#39;SN&#39;, 4, 90 &#39;CA&#39;, &#39;SN&#39;, 4, 85.5 &#39;CA&#39;, &#39;SN&#39;, 4, 84 &#39;CA&#39;, &#39;SN&#39;, 4, 82.5 &#39;CA&#39;, &#39;SN&#39;, 4, 81 &#39;CA&#39;, &#39;SN&#39;, 4, 77.5 &#39;CA&#39;, &#39;SN&#39;, 4, 77 &#39;CA&#39;, &#39;SN&#39;, 4, 72 &#39;CA&#39;, &#39;SN&#39;, 4, 71.5 &#39;CA&#39;, &#39;SN&#39;, 4, 69 &#39;CA&#39;, &#39;SN&#39;, 4, 68.5 &#39;CA&#39;, &#39;SN&#39;, 4, 68 &#39;CA&#39;, &#39;SN&#39;, 4, 67 &#39;CA&#39;, &#39;SN&#39;, 4, 65.5 &#39;CA&#39;, &#39;SN&#39;, 4, 62.5 &#39;CA&#39;, &#39;SN&#39;, 4, 62 &#39;CA&#39;, &#39;SN&#39;, 4, 61.5 &#39;CA&#39;, &#39;SN&#39;, 4, 61 &#39;CA&#39;, &#39;SN&#39;, 4, 57.5 &#39;CA&#39;, &#39;SN&#39;, 4, 54 &#39;CA&#39;, &#39;SN&#39;, 4, 52.5 &#39;CA&#39;, &#39;SN&#39;, 4, 51 &#39;CA&#39;, &#39;SN&#39;, 4, 50.5 &#39;CA&#39;, &#39;SN&#39;, 4, 50 &#39;CA&#39;, &#39;SN&#39;, 4, 49 &#39;CA&#39;, &#39;SN&#39;, 4, 43 &#39;CA&#39;, &#39;SN&#39;, 4, 39.5 &#39;CA&#39;, &#39;SN&#39;, 4, 32.5 &#39;CA&#39;, &#39;SN&#39;, 4, 25.5 &#39;CA&#39;, &#39;SN&#39;, 4, 18&quot; csv_test_data = read.csv(text=txt_test_data, header=TRUE) ar_st_varnames &lt;- c(&#39;first_half_professor&#39;, &#39;second_half_professor&#39;, &#39;course_id&#39;, &#39;exam_score&#39;) tb_test_data &lt;- as_tibble(csv_test_data) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_test_data) ## first_half_professor second_half_professor course_id exam_score ## &#39;CA&#39;:72 &#39;MP&#39;:70 Min. :1.000 Min. : 4.00 ## &#39;MP&#39;:42 &#39;SN&#39;:44 1st Qu.:1.000 1st Qu.: 60.00 ## &#39;SW&#39;:43 &#39;SW&#39;:43 Median :2.000 Median : 82.00 ## Mean :2.465 Mean : 75.08 ## 3rd Qu.:4.000 3rd Qu.: 94.00 ## Max. :4.000 Max. :105.00 2.4.1.2 Test Score Distributions 2.4.1.2.1 Histogram ggplot(tb_final_twovar, aes(x=ar_test_scores_ec3)) + geom_histogram(bins=25) + labs(title = paste0(&#39;Sandbox: Final Distribution (Econ 2370, FW)&#39;), caption = paste0(&#39;FW Section, formula:&#39;, &#39;0.3*exam1Perc + 0.3*exam2Perc + &#39;, &#39;0.42*HWtotalPerc + 0.03*AttendancePerc \\n&#39;, &#39;+ perfect attendance + 0.03 per Extra Credit&#39;)) + theme_bw() ggplot(tb_test_data, aes(x=exam_score)) + geom_histogram(bins=16) + labs(title = paste0(&#39;Exam Distribution&#39;), caption = &#39;All Sections&#39;) + theme_bw() 2.5 Summarize Multiple Variables 2.5.1 Generate Replace Variables Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 2.5.1.1 Replace NA for Multiple Variables Replace some variables NA by some values, and other variables’ NAs by other values. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;date&#39; # NA dataframe df_NA &lt;- as_tibble(matrix(NA, nrow=it_N, ncol=it_M)) %&gt;% rowid_to_column(var = svr_id) %&gt;% rename_at(vars(starts_with(&quot;V&quot;)), funs(str_replace(., &quot;V&quot;, &quot;var&quot;))) kable(df_NA) %&gt;% kable_styling_fc() date var1 var2 var3 var4 var5 1 NA NA NA NA NA 2 NA NA NA NA NA 3 NA NA NA NA NA # Replace NA df_NA_replace &lt;- df_NA %&gt;% mutate_at(vars(one_of(c(&#39;var1&#39;, &#39;var2&#39;))), list(~replace_na(., 0))) %&gt;% mutate_at(vars(one_of(c(&#39;var3&#39;, &#39;var5&#39;))), list(~replace_na(., 99))) kable(df_NA_replace) %&gt;% kable_styling_fc() date var1 var2 var3 var4 var5 1 0 0 99 NA 99 2 0 0 99 NA 99 3 0 0 99 NA 99 2.5.1.2 Cumulative Sum Multiple Variables Each row is a different date, each column is the profit a firms earns on a date, we want to compute cumulatively how much a person is earning. Also renames variable names below jointly. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;date&#39; # random dataframe, daily profit of firms # dp_fx: daily profit firm ID something set.seed(123) df_daily_profit &lt;- as_tibble(matrix(rnorm(it_N*it_M), nrow=it_N, ncol=it_M)) %&gt;% rowid_to_column(var = svr_id) %&gt;% rename_at(vars(starts_with(&quot;V&quot;)), funs(str_replace(., &quot;V&quot;, &quot;dp_f&quot;))) kable(df_daily_profit) %&gt;% kable_styling_fc() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 # cumulative sum with suffix df_cumu_profit_suffix &lt;- df_daily_profit %&gt;% mutate_at(vars(contains(&#39;dp_f&#39;)), .funs = list(cumu = ~cumsum(.))) kable(df_cumu_profit_suffix) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 dp_f1_cumu dp_f2_cumu dp_f3_cumu dp_f4_cumu dp_f5_cumu 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 -0.7906531 0.1997961 -0.8041450 0.7784198 0.5114542 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 0.7680552 1.9148611 -1.4909979 1.1382337 -0.0443870 # cumulative sum variables naming to prefix df_cumu_profit &lt;- df_cumu_profit_suffix %&gt;% rename_at(vars(contains( &quot;_cumu&quot;) ), list(~paste(&quot;cp_f&quot;, gsub(&quot;_cumu&quot;, &quot;&quot;, .), sep = &quot;&quot;))) %&gt;% rename_at(vars(contains( &quot;cp_f&quot;) ), list(~gsub(&quot;dp_f&quot;, &quot;&quot;, .))) kable(df_cumu_profit) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 cp_f1 cp_f2 cp_f3 cp_f4 cp_f5 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 -0.7906531 0.1997961 -0.8041450 0.7784198 0.5114542 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 0.7680552 1.9148611 -1.4909979 1.1382337 -0.0443870 "],
["functions.html", "Chapter 3 Functions 3.1 Dataframe Mutate 3.2 Dataframe Do Anything 3.3 Apply and pmap", " Chapter 3 Functions 3.1 Dataframe Mutate 3.1.1 Row Input Functions Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. We want evaluate nonlinear function f(Q_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. We would like to evaluate this nonlinear function concurrently across \\(N\\) individuals. The eventual goal is to find the \\(i\\) specific \\(Q\\) that solves the nonlinear equations. This is a continuation of R use Apply, Sapply and dplyr Mutate to Evaluate one Function Across Rows of a Matrix 3.1.1.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) ar_nN_N_choice = seq(1,it_N_child_cnt)/sum(seq(1,it_N_child_cnt)) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha, ar_nN_N_choice) # Show kable(mt_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha ar_nN_N_choice -2 0.1 0.0666667 -1 0.3 0.1333333 0 0.5 0.2000000 1 0.7 0.2666667 2 0.9 0.3333333 3.1.1.2 Testing Function Test non-linear Equation. # Test Parameters fl_N_agg = 100 fl_rho = -1 fl_N_q = ar_nN_N_choice[4]*fl_N_agg ar_A_alpha = mt_nN_by_nQ_A_alpha[4,] # Apply Function ar_p1_s1 = exp((ar_A_alpha[1] - ar_nN_A)*fl_rho) ar_p1_s2 = (ar_A_alpha[2]/ar_nN_alpha) ar_p1_s3 = (1/(ar_nN_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N_q^((ar_A_alpha[2]*fl_rho-1)/(ar_nN_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) print(fl_overall) ## [1] -598.2559 Implement the non-linear problem’s evaluation using apply over all \\(N\\) individuals. # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha # # Test Parameters # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } # Parameters fl_rho = -1 # Evaluate Function print(ffi_nonlin_dplyrdo(mt_nN_by_nQ_A_alpha[1,1], mt_nN_by_nQ_A_alpha[1,2], mt_nN_by_nQ_A_alpha[1,3]*fl_N_agg, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) ## [1] 81.86645 for (i in seq(1,dim(mt_nN_by_nQ_A_alpha)[1])){ fl_eval = ffi_nonlin_dplyrdo(mt_nN_by_nQ_A_alpha[i,1], mt_nN_by_nQ_A_alpha[i,2], mt_nN_by_nQ_A_alpha[i,3]*fl_N_agg, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho) print(fl_eval) } ## [1] 81.86645 ## [1] 54.48885 ## [1] -65.5619 ## [1] -598.2559 ## [1] -3154.072 3.1.1.3 Evaluate Nonlinear Function using dplyr mutate # Convert Matrix to Tibble ar_st_col_names = c(&#39;fl_A&#39;, &#39;fl_alpha&#39;, &#39;fl_N&#39;) tb_nN_by_nQ_A_alpha &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rename_all(~c(ar_st_col_names)) # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # Test Parameters # ar_A = ar_nN_A # ar_alpha = ar_nN_alpha # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = (fl_N*fl_N_agg)^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_nN_by_nQ_A_alpha = tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha fl_N dplyr_eval -2 0.1 0.0666667 81.86645 -1 0.3 0.1333333 54.48885 0 0.5 0.2000000 -65.56190 1 0.7 0.2666667 -598.25595 2 0.9 0.3333333 -3154.07226 3.1.2 Evaluate Choices Across States Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. See the ff_opti_bisect_pmap_multi function from Fan’s REconTools Package, which provides a resuable function based on the algorithm worked out here. We want evaluate linear function \\(0=f(z_{ij}, x_i, y_i, \\textbf{X}, \\textbf{Y}, c, d)\\). There are \\(i\\) functions that have \\(i\\) specific \\(x\\) and \\(y\\). For each \\(i\\) function, we evaluate along a grid of feasible values for \\(z\\), over \\(j\\in J\\) grid points, potentially looking for the \\(j\\) that is closest to the root. \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are arrays common across the \\(i\\) equations, and \\(c\\) and \\(d\\) are constants. The evaluation strategy is the following, given min and max for \\(z\\) that are specific for each \\(j\\), and given common number of grid points, generate a matrix of \\(z_{ij}\\). Suppose there the number of \\(i\\) is \\(I\\), and the number of grid points for \\(j\\) is \\(J\\). Generate a \\(J \\cdot I\\) by \\(3\\) matrix where the columns are \\(z,x,y\\) as tibble Follow this Mutate to evaluate the \\(f(\\cdot)\\) function. Add two categorical columns for grid levels and wich \\(i\\), \\(i\\) and \\(j\\) index. Plot Mutate output evaluated column categorized by \\(i\\) as color and \\(j\\) as x-axis. 3.1.2.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] Now we need to expand this by the number of choice grid. Each row, representing one equation, is expanded by the number of choice grids. We are graphically searching, or rather brute force searching, which means if we have 100 individuals, we want to plot out the nonlinear equation for each of these lines, and show graphically where each line crosses zero. We achieve this, by evaluating the equation for each of the 100 individuals along a grid of feasible choices. In this problem here, the feasible choices are shared across individuals. # Parameters fl_rho = 0.20 svr_id_var = &#39;INDI_ID&#39; # it_child_count = N, the number of children it_N_child_cnt = 4 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # Choice Grid for nutritional feasible choices for each fl_N_agg = 100 fl_N_min = 0 it_N_choice_cnt_ttest = 3 it_N_choice_cnt_dense = 100 ar_N_choices_ttest = seq(fl_N_min, fl_N_agg, length.out = it_N_choice_cnt_ttest) ar_N_choices_dense = seq(fl_N_min, fl_N_agg, length.out = it_N_choice_cnt_dense) # Mesh Expand tb_states_choices &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rowid_to_column(var=svr_id_var) tb_states_choices_ttest &lt;- tb_states_choices %&gt;% expand_grid(choices = ar_N_choices_ttest) tb_states_choices_dense &lt;- tb_states_choices %&gt;% expand_grid(choices = ar_N_choices_dense) # display summary(tb_states_choices_dense) ## INDI_ID ar_nN_A ar_nN_alpha choices ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 0 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 25 ## Median :2.50 Median : 0 Median :0.5 Median : 50 ## Mean :2.50 Mean : 0 Mean :0.5 Mean : 50 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.: 75 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :100 kable(tb_states_choices_ttest) %&gt;% kable_styling_fc() INDI_ID ar_nN_A ar_nN_alpha choices 1 -2.0000000 0.1000000 0 1 -2.0000000 0.1000000 50 1 -2.0000000 0.1000000 100 2 -0.6666667 0.3666667 0 2 -0.6666667 0.3666667 50 2 -0.6666667 0.3666667 100 3 0.6666667 0.6333333 0 3 0.6666667 0.6333333 50 3 0.6666667 0.6333333 100 4 2.0000000 0.9000000 0 4 2.0000000 0.9000000 50 4 2.0000000 0.9000000 100 3.1.2.2 Apply Same Function all Rows, Some Inputs Row-specific, other Shared There are two types of inputs, row-specific inputs, and inputs that should be applied for each row. The Function just requires all of these inputs, it does not know what is row-specific and what is common for all row. Dplyr recognizes which parameter inputs already existing in the piped dataframe/tibble, given rowwise, those will be row-specific inputs. Additional function parameters that do not exist in dataframe as variable names, but that are pre-defined scalars or arrays will be applied to all rows. (???) string variable name of input where functions are evaluated, these are already contained in the dataframe, existing variable names, row specific, rowwise computation over these, each rowwise calculation using different rows: fl_A, fl_alpha, fl_N (???) scalar and array values that are applied to every rowwise calculation, all rowwise calculations using the same scalars and arrays:ar_A, ar_alpha, fl_N_agg, fl_rho (???) string output variable name The function looks within group, finds min/max etc that are relevant. 3.1.2.2.1 3 Points and Denser Dataframs and Define Function # Convert Matrix to Tibble ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_states_choices &lt;- tb_states_choices %&gt;% rename_all(~c(ar_st_col_names)) ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;, &#39;fl_N&#39;) tb_states_choices_ttest &lt;- tb_states_choices_ttest %&gt;% rename_all(~c(ar_st_col_names)) tb_states_choices_dense &lt;- tb_states_choices_dense %&gt;% rename_all(~c(ar_st_col_names)) # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # scalar value that are row-specific, in dataframe already: *fl_A*, *fl_alpha*, *fl_N* # array and scalars not in dataframe, common all rows: *ar_A*, *ar_alpha*, *fl_N_agg*, *fl_rho* # Test Parameters # ar_A = ar_nN_A # ar_alpha = ar_nN_alpha # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } 3.1.2.2.2 Evaluate at Three Choice Points and Show Table In the example below, just show results evaluating over three choice points and show table. # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_states_choices_ttest_eval = tb_states_choices_ttest %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show kable(tb_states_choices_ttest_eval) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha fl_N dplyr_eval 1 -2.0000000 0.1000000 0 100.00000 1 -2.0000000 0.1000000 50 -5666.95576 1 -2.0000000 0.1000000 100 -12880.28392 2 -0.6666667 0.3666667 0 100.00000 2 -0.6666667 0.3666667 50 -595.73454 2 -0.6666667 0.3666667 100 -1394.70698 3 0.6666667 0.6333333 0 100.00000 3 0.6666667 0.6333333 50 -106.51058 3 0.6666667 0.6333333 100 -323.94216 4 2.0000000 0.9000000 0 100.00000 4 2.0000000 0.9000000 50 22.55577 4 2.0000000 0.9000000 100 -51.97161 3.1.2.2.3 Evaluate at Many Choice Points and Show Graphically Same as above, but now we evaluate the function over the individuals at many choice points so that we can graph things out. # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_states_choices_dense_eval = tb_states_choices_dense %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Labeling st_title &lt;- paste0(&#39;Evaluate Non-Linear Functions to Search for Roots&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/function/mutatef/htmlpdfr/fs_func_choice_states.html&#39;) st_caption &lt;- paste0(&#39;Evaluating the function, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;x values&#39; st_y_label &lt;- &#39;f(x)&#39; # Show dim(tb_states_choices_dense_eval) ## [1] 400 5 summary(tb_states_choices_dense_eval) ## INDI_ID fl_A fl_alpha fl_N dplyr_eval ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 0 Min. :-12880.28 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 25 1st Qu.: -1167.29 ## Median :2.50 Median : 0 Median :0.5 Median : 50 Median : -202.42 ## Mean :2.50 Mean : 0 Mean :0.5 Mean : 50 Mean : -1645.65 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.: 75 3rd Qu.: 0.96 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :100 Max. : 100.00 lineplot &lt;- tb_states_choices_dense_eval %&gt;% ggplot(aes(x=fl_N, y=dplyr_eval)) + geom_line() + facet_wrap( . ~ INDI_ID, scales = &quot;free&quot;) + geom_hline(yintercept=0, linetype=&quot;dashed&quot;, color = &quot;red&quot;, size=1) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) print(lineplot) 3.2 Dataframe Do Anything 3.2.1 MxQ to MxP Rows Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 3.2.1.1 MxQ to Mx1 Rows: Within Group Gini There is a Panel with \\(M\\) individuals and each individual has \\(Q\\) records/rows. A function generate an individual specific outcome given the \\(Q\\) individual specific inputs, along with shared parameters and arrays across the \\(M\\) individuals. For example, suppose we have a dataframe of individual wage information from different countries, each row is an individual from one country. We want to generate country specific gini based on the individual data for each country in the dataframe. But additionally, perhaps the gini formula requires not just individual income but some additional parameters or shared dataframes as inputs. Given the within \\(m\\) income observations, we can compute gini statistics that are individual specific based on the observed distribution of incomes. For this, we will use the ff_dist_gini_vector_pos.html function from REconTools. To make this more interesting, we will generate large dataframe with more \\(M\\) and more \\(Q\\) each \\(m\\). 3.2.1.1.1 Large Dataframe There are up to ten thousand income observation per person. And there are ten people. # Parameter Setups it_M &lt;- 10 it_Q_max &lt;- 10000 fl_rnorm_mu &lt;- 1 ar_rnorm_sd &lt;- seq(0.01, 0.2, length.out=it_M) ar_it_q &lt;- sample.int(it_Q_max, it_M, replace=TRUE) # N by Q varying parameters mt_data = cbind(ar_it_q, ar_rnorm_sd) tb_M &lt;- as_tibble(mt_data) %&gt;% rowid_to_column(var = &quot;ID&quot;) %&gt;% rename(sd = ar_rnorm_sd, Q = ar_it_q) %&gt;% mutate(mean = fl_rnorm_mu) 3.2.1.1.2 Compute Group specific gini, NORMAL There is only one input for the gini function ar_pos. Note that the gini are not very large even with large SD, because these are normal distributions. By Construction, most peple are in the middle. So with almost zero standard deviation, we have perfect equality, as standard deviation increases, inequality increases, but still pretty equal overall, there is no fat upper tail. Note that there are three ways of referring to variable names with dot, which are all shown below: We can explicitly refer to names We can use the dollar dot structure to use string variable names in do anything. We can use dot bracket, this is the only option that works with string variable names # A. Normal Draw Expansion, Explicitly Name set.seed(&#39;123&#39;) tb_income_norm_dot_dollar &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.$Q, mean=.$mean, sd=.$sd)) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # Normal Draw Expansion again, dot dollar differently with string variable name set.seed(&#39;123&#39;) tb_income_norm_dollar_dot &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(`$`(., &#39;Q&#39;), mean = `$`(., &#39;mean&#39;), sd = `$`(., &#39;sd&#39;))) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # Normal Draw Expansion again, dot double bracket set.seed(&#39;123&#39;) svr_mean &lt;- &#39;mean&#39; svr_sd &lt;- &#39;sd&#39; svr_Q &lt;- &#39;Q&#39; tb_income_norm_dot_bracket_db &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.[[svr_Q]], mean = .[[svr_mean]], sd = .[[svr_sd]])) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # display sum(sum(tb_income_norm_dollar_dot - tb_income_norm_dot_dollar - tb_income_norm_dot_bracket_db)) ## [1] -463785175 # display head(tb_income_norm_dot_dollar, 20) # Gini by Group tb_gini_norm &lt;- tb_income_norm_dollar_dot %&gt;% group_by(ID) %&gt;% do(inc_gini_norm = ff_dist_gini_vector_pos(.$income)) %&gt;% unnest(c(inc_gini_norm)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # display kable(tb_gini_norm) %&gt;% kable_styling_fc() ID inc_gini_norm Q sd mean 1 0.0056337 9982 0.0100000 1 2 0.0175280 2980 0.0311111 1 3 0.0293986 1614 0.0522222 1 4 0.0422304 555 0.0733333 1 5 0.0535146 4469 0.0944444 1 6 0.0653938 9359 0.1155556 1 7 0.0769135 7789 0.1366667 1 8 0.0894165 9991 0.1577778 1 9 0.1010982 9097 0.1788889 1 10 0.1124019 1047 0.2000000 1 3.2.2 Mx1 to MxQ Rows Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Case One: There is a dataframe with \\(M\\) rows, based on these \\(m\\) specific information, generate dataframes for each \\(m\\). Stack these indivdiual dataframes together and merge original \\(m\\) specific information in as well. The number of rows for each \\(m\\) is \\(Q_m\\), each \\(m\\) could have different number of expansion rows. Generate a panel with \\(M\\) individuals, each individual is observed for different spans of times (uncount). Before expanding, generate individual specific normal distribution standard deviation. All individuals share the same mean, but have increasing standard deviations. 3.2.2.1 Generate Dataframe with M Rows. This is the first step, generate \\(M\\) rows of data, to be expanded. Each row contains the number of normal draws to make and the mean and the standard deviation for normal daraws that are \\(m\\) specific. # Parameter Setups it_M &lt;- 3 it_Q_max &lt;- 5 fl_rnorm_mu &lt;- 1000 ar_rnorm_sd &lt;- seq(0.01, 200, length.out=it_M) ar_it_q &lt;- sample.int(it_Q_max, it_M, replace=TRUE) # N by Q varying parameters mt_data = cbind(ar_it_q, ar_rnorm_sd) tb_M &lt;- as_tibble(mt_data) %&gt;% rowid_to_column(var = &quot;ID&quot;) %&gt;% rename(sd = ar_rnorm_sd, Q = ar_it_q) %&gt;% mutate(mean = fl_rnorm_mu) # display kable(tb_M) %&gt;% kable_styling_fc() ID Q sd mean 1 3 0.010 1000 2 3 100.005 1000 3 1 200.000 1000 3.2.2.2 Random Normal Draw Expansion The steps are: do anything use “.$” sign to refer to variable names, or [[‘name’]] unnest left_join expanded and original Note these all give the same results Use dot dollar to get variables # Generate $Q_m$ individual specific incomes, expanded different number of times for each m tb_income &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.$Q, mean=.$mean, sd=.$sd)) %&gt;% unnest(c(income)) # Merge back with tb_M tb_income_full_dd &lt;- tb_income %&gt;% left_join(tb_M) # display kable(tb_income) %&gt;% kable_styling_fc() ID income 1 1000.0183 1 999.9943 1 999.9822 2 1033.7465 2 1093.1374 2 862.1896 3 988.7742 kable(tb_income_full_dd) %&gt;% kable_styling_fc() ID income Q sd mean 1 1000.0183 3 0.010 1000 1 999.9943 3 0.010 1000 1 999.9822 3 0.010 1000 2 1033.7465 3 100.005 1000 2 1093.1374 3 100.005 1000 2 862.1896 3 100.005 1000 3 988.7742 1 200.000 1000 3.3 Apply and pmap 3.3.1 Apply, Sapply, Mutate Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r apply matrix to function row by row r evaluate function on grid Apply a function to every row of a matrix or a data frame r apply r sapply sapply over matrix row by row apply dplyr vectorize function as parameters using formulas do We want evaluate linear function f(x_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. More specifically, we have a functions, this function takes inputs that are individual specific. We would like to evaluate this function concurrently across \\(N\\) individuals. The function is such that across the \\(N\\) individuals, some of the function parameter inputs are the same, but others are different. If we are looking at demand for a particular product, the prices of all products enter the demand equation for each product, but the product’s own price enters also in a different way. The objective is either to just evaluate this function across \\(N\\) individuals, or this is a part of a nonlinear solution system. What is the relationship between apply, lapply and vectorization? see Is the “*apply” family really not vectorized?. 3.3.1.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are # heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # display kable(mt_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 3.3.1.2 Using apply 3.3.1.2.1 Apply with Named Function First we use the apply function, we have to hard-code the arrays that are fixed for each of the \\(N\\) individuals. Then apply allows us to loop over the matrix that is \\(N\\) by \\(Q\\), each row one at a time, from \\(1\\) to \\(N\\). # Define Implicit Function ffi_linear_hardcode &lt;- function(ar_A_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha fl_out = sum(ar_A_alpha[1]*ar_nN_A + 1/(ar_A_alpha[2] + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row ar_func_apply = apply(mt_nN_by_nQ_A_alpha, 1, ffi_linear_hardcode) 3.3.1.2.2 Apply using Anonymous Function apply over matrix Apply with anonymous function generating a list of arrays of different lengths. In the example below, we want to drawn \\(N\\) sets of random uniform numbers, but for each set the number of draws we want to have is \\(Q_i\\). Furthermore, we want to rescale the random uniform draws so that they all become proportions that sum u pto one for each \\(i\\), but then we multply each row’s values by the row specific aggregates. The anonymous function has hard coded parameters. Using an anonymous function here allows for parameters to be provided inside the function that are shared across each looped evaluation. This is perhaps more convenient than sapply with additional parameters. set.seed(1039) # Define the number of draws each row and total amount it_N &lt;- 4 fl_unif_min &lt;- 1 fl_unif_max &lt;- 2 mt_draw_define &lt;- cbind(sample(it_N, it_N, replace=TRUE), runif(it_N, min=1, max=10)) tb_draw_define &lt;- as_tibble(mt_draw_define) %&gt;% rowid_to_column(var = &quot;draw_group&quot;) print(tb_draw_define) # apply row by row, anonymous function has hard # coded min and max ls_ar_draws_shares_lvls = apply(tb_draw_define, 1, function(row) { it_draw &lt;- row[2] fl_sum &lt;- row[3] ar_unif &lt;- runif(it_draw, min=fl_unif_min, max=fl_unif_max) ar_share &lt;- ar_unif/sum(ar_unif) ar_levels &lt;- ar_share*fl_sum return(list(ar_share=ar_share, ar_levels=ar_levels)) }) # Show Results print(ls_ar_draws_shares_lvls) ## [[1]] ## [[1]]$ar_share ## [1] 0.2783638 0.2224140 0.2797840 0.2194381 ## ## [[1]]$ar_levels ## [1] 1.492414 1.192446 1.500028 1.176491 ## ## ## [[2]] ## [[2]]$ar_share ## [1] 0.5052919 0.4947081 ## ## [[2]]$ar_levels ## [1] 3.866528 3.785541 ## ## ## [[3]] ## [[3]]$ar_share ## [1] 1 ## ## [[3]]$ar_levels ## V2 ## 9.572211 ## ## ## [[4]] ## [[4]]$ar_share ## [1] 0.4211426 0.2909812 0.2878762 ## ## [[4]]$ar_levels ## [1] 4.051971 2.799640 2.769765 We will try to do the same thing as above, but now the output will be a stacked dataframe. Note that within each element of the apply row by row loop, we are generating two variables ar_share and ar_levels. We will not generate a dataframe with multiple columns, storing ar_share, ar_levels as well as information on min, max, number of draws and rescale total sum. set.seed(1039) # apply row by row, anonymous function has hard coded min and max ls_mt_draws_shares_lvls = apply(tb_draw_define, 1, function(row) { it_draw_group &lt;- row[1] it_draw &lt;- row[2] fl_sum &lt;- row[3] ar_unif &lt;- runif(it_draw, min=fl_unif_min, max=fl_unif_max) ar_share &lt;- ar_unif/sum(ar_unif) ar_levels &lt;- ar_share*fl_sum mt_all_res &lt;- cbind(it_draw_group, it_draw, fl_sum, ar_unif, ar_share, ar_levels) colnames(mt_all_res) &lt;- c(&#39;draw_group&#39;, &#39;draw_count&#39;, &#39;sum&#39;, &#39;unif_draw&#39;, &#39;share&#39;, &#39;rescale&#39;) rownames(mt_all_res) &lt;- NULL return(mt_all_res) }) mt_draws_shares_lvls_all &lt;- do.call(rbind, ls_mt_draws_shares_lvls) # Show Results kable(mt_draws_shares_lvls_all) %&gt;% kable_styling_fc() draw_group draw_count sum unif_draw share rescale 1 4 5.361378 1.125668 0.1988606 1.066167 1 4 5.361378 1.668536 0.2947638 1.580340 1 4 5.361378 1.419382 0.2507483 1.344356 1 4 5.361378 1.447001 0.2556274 1.370515 2 2 7.652069 1.484598 0.4605236 3.523959 2 2 7.652069 1.739119 0.5394764 4.128110 3 1 9.572211 1.952468 1.0000000 9.572211 4 3 9.621375 1.957931 0.3609352 3.472693 4 3 9.621375 1.926995 0.3552324 3.417824 4 3 9.621375 1.539678 0.2838324 2.730858 3.3.1.3 Using sapply 3.3.1.3.1 sapply with named function r convert matrix to list Convert a matrix to a list of vectors in R Sapply allows us to not have tohard code in the A and alpha arrays. But Sapply works over List or Vector, not Matrix. So we have to convert the \\(N\\) by \\(Q\\) matrix to a N element list Now update the function with sapply. ls_ar_nN_by_nQ_A_alpha = as.list(data.frame(t(mt_nN_by_nQ_A_alpha))) # Define Implicit Function ffi_linear_sapply &lt;- function(ar_A_alpha, ar_A, ar_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha fl_out = sum(ar_A_alpha[1]*ar_nN_A + 1/(ar_A_alpha[2] + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row ar_func_sapply = sapply(ls_ar_nN_by_nQ_A_alpha, ffi_linear_sapply, ar_A=ar_nN_A, ar_alpha=ar_nN_alpha) 3.3.1.3.2 sapply using anonymous function sapply anonymous function r anoymous function multiple lines Sapply with anonymous function generating a list of arrays of different lengths. In the example below, we want to drawn \\(N\\) sets of random uniform numbers, but for each set the number of draws we want to have is \\(Q_i\\). Furthermore, we want to rescale the random uniform draws so that they all become proportions that sum u pto one for each \\(i\\). it_N &lt;- 4 fl_unif_min &lt;- 1 fl_unif_max &lt;- 2 # Generate using runif without anonymous function set.seed(1039) ls_ar_draws = sapply(seq(it_N), runif, min=fl_unif_min, max=fl_unif_max) print(ls_ar_draws) ## [[1]] ## [1] 1.125668 ## ## [[2]] ## [1] 1.668536 1.419382 ## ## [[3]] ## [1] 1.447001 1.484598 1.739119 ## ## [[4]] ## [1] 1.952468 1.957931 1.926995 1.539678 # Generate Using Anonymous Function set.seed(1039) ls_ar_draws_shares = sapply(seq(it_N), function(n, min, max) { ar_unif &lt;- runif(n,min,max) ar_share &lt;- ar_unif/sum(ar_unif) return(ar_share) }, min=fl_unif_min, max=fl_unif_max) # Print Share print(ls_ar_draws_shares) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 0.5403432 0.4596568 ## ## [[3]] ## [1] 0.3098027 0.3178522 0.3723451 ## ## [[4]] ## [1] 0.2646671 0.2654076 0.2612141 0.2087113 # Sapply with anonymous function to check sums sapply(seq(it_N), function(x) {sum(ls_ar_draws[[x]])}) ## [1] 1.125668 3.087918 4.670717 7.377071 sapply(seq(it_N), function(x) {sum(ls_ar_draws_shares[[x]])}) ## [1] 1 1 1 1 3.3.1.4 Using dplyr mutate rowwise dplyr mutate own function dplyr all row function dplyr do function apply function each row dplyr applying a function to every row of a table using dplyr dplyr rowwise # Convert Matrix to Tibble ar_st_col_names = c(&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_nN_by_nQ_A_alpha &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rename_all(~c(ar_st_col_names)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 # Define Implicit Function ffi_linear_dplyrdo &lt;- function(fl_A, fl_alpha, ar_nN_A, ar_nN_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha print(paste0(&#39;cur row, fl_A=&#39;, fl_A, &#39;, fl_alpha=&#39;, fl_alpha)) fl_out = sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row of tibble # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_nN_by_nQ_A_alpha_show &lt;- tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_linear_dplyrdo(fl_A, fl_alpha, ar_nN_A, ar_nN_alpha)) ## [1] &quot;cur row, fl_A=-2, fl_alpha=0.1&quot; ## [1] &quot;cur row, fl_A=-1, fl_alpha=0.3&quot; ## [1] &quot;cur row, fl_A=0, fl_alpha=0.5&quot; ## [1] &quot;cur row, fl_A=1, fl_alpha=0.7&quot; ## [1] &quot;cur row, fl_A=2, fl_alpha=0.9&quot; # Show kable(tb_nN_by_nQ_A_alpha_show) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 same as before, still rowwise, but hard code some inputs: # Define function, fixed inputs are not parameters, but # defined earlier as a part of the function # ar_nN_A, ar_nN_alpha are fixed, not parameters ffi_linear_dplyrdo_func &lt;- function(fl_A, fl_alpha){ fl_out &lt;- sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row of tibble tbfunc_A_nN_by_nQ_A_alpha_rowwise = tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_linear_dplyrdo_func(fl_A, fl_alpha)) # Show kable(tbfunc_A_nN_by_nQ_A_alpha_rowwise) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.1.5 Using Dplyr Mutate with Pmap Apparantly rowwise() is not a good idea, and pmap should be used, below is the pmap solution to the problem. Which does seem nicer. Crucially, don’t have to define input parameter names, automatically I think they are matching up to the names in the function dplyr mutate pass function r function quosure string multiple r function multiple parameters as one string dplyr mutate anonymous function quosure style lambda pmap tibble rows dplyr pwalk # Define function, fixed inputs are not parameters, but defined # earlier as a part of the function Rorate fl_alpha and fl_A name # compared to before to make sure pmap tracks by names ffi_linear_dplyrdo_func &lt;- function(fl_alpha, fl_A){ fl_out &lt;- sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate a function row by row of dataframe, generate list, # then to vector tb_nN_by_nQ_A_alpha %&gt;% pmap(ffi_linear_dplyrdo_func) %&gt;% unlist() ## [1] 2.346356 2.094273 1.895316 1.733708 1.599477 # Same as above, but in line line and save output as new column # in dataframe note this ONLY works if the tibble only has variables # that are inputs for the function if tibble contains additional # variables, those should be droppd, or only the ones needed selected, # inside the pmap call below. tbfunc_A_nN_by_nQ_A_alpha_pmap &lt;- tb_nN_by_nQ_A_alpha %&gt;% mutate(dplyr_eval_pmap = unlist( pmap(tb_nN_by_nQ_A_alpha, ffi_linear_dplyrdo_func) ) ) # Show kable(tbfunc_A_nN_by_nQ_A_alpha_pmap) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval_pmap -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.1.6 DPLYR Three Types of Inputs ROWWISE Now, we have three types of parameters, for something like a bisection type calculation. We will supply the program with a function with some hard-coded value inside, and as parameters, we will have one parameter which is a row in the current matrix, and another parameter which is a sclar values. The three types of parameters are dealt with sparately: parameters that are fixed for all bisection iterations, but differ for each row these are hard-coded into the function parameters that are fixed for all bisection iterations, but are shared across rows these are the first parameter of the function, a list parameters that differ for each iteration, but differ acoss iterations second scalar value parameter for the function dplyr mutate function applow to each row dot notation note rowwise might be bad according to Hadley, should use pmap? ffi_linear_dplyrdo_fdot &lt;- function(ls_row, fl_param){ # Type 1 Param = ar_nN_A, ar_nN_alpha # Type 2 Param = ls_row$fl_A, ls_row$fl_alpha # Type 3 Param = fl_param fl_out &lt;- (sum(ls_row$fl_A*ar_nN_A + 1/(ls_row$fl_alpha + 1/ar_nN_alpha))) + fl_param return(fl_out) } cur_func &lt;- ffi_linear_dplyrdo_fdot fl_param &lt;- 0 dplyr_eval_flex &lt;- tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% do(dplyr_eval_flex = cur_func(., fl_param)) %&gt;% unnest(dplyr_eval_flex) tbfunc_B_nN_by_nQ_A_alpha &lt;- tb_nN_by_nQ_A_alpha %&gt;% add_column(dplyr_eval_flex) # Show kable(tbfunc_B_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval_flex -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.1.7 Compare Apply and Mutate Results # Show overall Results mt_results &lt;- cbind(ar_func_apply, ar_func_sapply, tb_nN_by_nQ_A_alpha_show[&#39;dplyr_eval&#39;], tbfunc_A_nN_by_nQ_A_alpha_rowwise[&#39;dplyr_eval&#39;], tbfunc_A_nN_by_nQ_A_alpha_pmap[&#39;dplyr_eval_pmap&#39;], tbfunc_B_nN_by_nQ_A_alpha[&#39;dplyr_eval_flex&#39;], mt_nN_by_nQ_A_alpha) colnames(mt_results) &lt;- c(&#39;eval_lin_apply&#39;, &#39;eval_lin_sapply&#39;, &#39;eval_dplyr_mutate&#39;, &#39;eval_dplyr_mutate_hcode&#39;, &#39;eval_dplyr_mutate_pmap&#39;, &#39;eval_dplyr_mutate_flex&#39;, &#39;A_child&#39;, &#39;alpha_child&#39;) kable(mt_results) %&gt;% kable_styling_fc_wide() eval_lin_apply eval_lin_sapply eval_dplyr_mutate eval_dplyr_mutate_hcode eval_dplyr_mutate_pmap eval_dplyr_mutate_flex A_child alpha_child X1 2.346356 2.346356 2.346356 2.346356 2.346356 2.346356 -2 0.1 X2 2.094273 2.094273 2.094273 2.094273 2.094273 2.094273 -1 0.3 X3 1.895316 1.895316 1.895316 1.895316 1.895316 1.895316 0 0.5 X4 1.733708 1.733708 1.733708 1.733708 1.733708 1.733708 1 0.7 X5 1.599477 1.599477 1.599477 1.599477 1.599477 1.599477 2 0.9 "],
["panel.html", "Chapter 4 Panel 4.1 Generate and Join 4.2 Wide and Long", " Chapter 4 Panel 4.1 Generate and Join 4.1.1 Generate Panel Structure Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 4.1.1.1 Balanced Panel Skeleton There are \\(N\\) individuals, each could be observed \\(M\\) times. In the example below, there are 3 students, each observed over 4 dates. This just uses the uncount function from tidyr. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;student_id&#39; svr_date &lt;- &#39;class_day&#39; # dataframe df_panel_skeleton &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Print kable(df_panel_skeleton) %&gt;% kable_styling_fc() student_id class_day 1 1 1 2 1 3 1 4 1 5 2 1 2 2 2 3 2 4 2 5 3 1 3 2 3 3 3 4 3 5 4.1.2 Join Datasets Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 4.1.2.1 Join Panel with Multiple Keys We have two datasets, one for student enrollment, panel over time, but some students do not show up on some dates. The other is a skeleton panel with all student ID and all dates. Often we need to join dataframes together, and we need to join by the student ID and the panel time Key at the same time. When students show up, there is a quiz score for that day, so the joined panel should have as data column quiz score Student count is \\(N\\), total dates are \\(M\\). First we generate two panels below, then we join by both keys using left_join. First, define dataframes: # Define it_N &lt;- 4 it_M &lt;- 3 svr_id &lt;- &#39;sid&#39; svr_date &lt;- &#39;classday&#39; svr_attend &lt;- &#39;date_in_class&#39; # Panel Skeleton df_panel_balanced_skeleton &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Print kable(df_panel_balanced_skeleton) %&gt;% kable_styling_fc() sid classday 1 1 1 2 1 3 2 1 2 2 2 3 3 1 3 2 3 3 4 1 4 2 4 3 # Smaller Panel of Random Days in School set.seed(456) df_panel_attend &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), !!sym(svr_date)) %&gt;% rename(!!sym(svr_attend) := !!sym(svr_date)) %&gt;% mutate(dayquizscore = rnorm(n(),mean=80,sd=10)) # Print kable(df_panel_attend) %&gt;% kable_styling_fc() sid date_in_class dayquizscore 1 1 89.88726 2 1 96.53929 2 2 65.59195 2 3 99.47356 4 2 97.36936 Second, now join dataframes: # Join with explicit names df_quiz_joined_multikey &lt;- df_panel_balanced_skeleton %&gt;% left_join(df_panel_attend, by=(c(&#39;sid&#39;=&#39;sid&#39;, &#39;classday&#39;=&#39;date_in_class&#39;))) # Join with setname strings df_quiz_joined_multikey_setnames &lt;- df_panel_balanced_skeleton %&gt;% left_join(df_panel_attend, by=setNames(c(&#39;sid&#39;, &#39;date_in_class&#39;), c(&#39;sid&#39;, &#39;classday&#39;))) # Print kable(df_quiz_joined_multikey) %&gt;% kable_styling_fc() sid classday dayquizscore 1 1 89.88726 1 2 NA 1 3 NA 2 1 96.53929 2 2 65.59195 2 3 99.47356 3 1 NA 3 2 NA 3 3 NA 4 1 NA 4 2 97.36936 4 3 NA kable(df_quiz_joined_multikey_setnames) %&gt;% kable_styling_fc() sid classday dayquizscore 1 1 89.88726 1 2 NA 1 3 NA 2 1 96.53929 2 2 65.59195 2 3 99.47356 3 1 NA 3 2 NA 3 3 NA 4 1 NA 4 2 97.36936 4 3 NA 4.1.2.2 Stack Panel Frames Together There are multiple panel dataframe, each for different subsets of dates. All variable names and units of observations are identical. Use DPLYR bind_rows. # Define it_N &lt;- 2 # Number of individuals it_M &lt;- 3 # Number of Months svr_id &lt;- &#39;sid&#39; svr_date &lt;- &#39;date&#39; # Panel First Half of Year df_panel_m1tom3 &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Panel Second Half of Year df_panel_m4tom6 &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number() + 3) %&gt;% ungroup() # Bind Rows df_panel_m1tm6 &lt;- bind_rows(df_panel_m1tom3, df_panel_m4tom6) %&gt;% arrange(!!!syms(c(svr_id, svr_date))) # Print kable(df_panel_m1tom3) %&gt;% kable_styling_fc() sid date 1 1 1 2 1 3 2 1 2 2 2 3 kable(df_panel_m4tom6) %&gt;% kable_styling_fc() sid date 1 4 1 5 1 6 2 4 2 5 2 6 kable(df_panel_m1tm6) %&gt;% kable_styling_fc() sid date 1 1 1 2 1 3 1 4 1 5 1 6 2 1 2 2 2 3 2 4 2 5 2 6 4.2 Wide and Long 4.2.1 Long to Wide Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Using the pivot_wider function in tidyr to reshape panel or other data structures 4.2.1.1 Panel Long Attendance Roster to Wide There are \\(N\\) students in class, but only a subset of them attend class each day. If student \\(id_i\\) is in class on day \\(Q\\), the teacher records on a sheet the date and the student ID. So if the student has been in class 10 times, the teacher has ten rows of recorded data for the student with two columns: column one is the student ID, and column two is the date on which the student was in class. Suppose there were 50 students, who on average attended exactly 10 classes each during the semester, this means we have \\(10 \\cdot 50\\) rows of data, with differing numbers of rows for each student. This is shown as df_panel_attend_date generated below. Now we want to generate a new dataframe, where each row is a date, and each column is a student. The values in the new dataframe shows, at the \\(Q^{th}\\) day, how many classes student \\(i\\) has attended so far. The following results is also in a REconTools Function. This is shown as df_attend_cumu_by_day generated below. First, generate the raw data structure, df_panel_attend_date: # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;student_id&#39; # from : support/rand/fs_rand_draws.Rmd set.seed(222) df_panel_attend_date &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(date = row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), date) %&gt;% rename(date_in_class = date) # Print kable(df_panel_attend_date) %&gt;% kable_styling_fc() student_id date_in_class 1 2 1 4 2 1 2 2 2 5 3 2 3 3 3 5 Second, generate wider data structure, df_attend_cumu_by_day: # Define svr_id &lt;- &#39;student_id&#39; svr_date &lt;- &#39;date_in_class&#39; st_idcol_prefix &lt;- &#39;sid_&#39; # Generate cumulative enrollment counts by date df_panel_attend_date_addone &lt;- df_panel_attend_date %&gt;% mutate(attended = 1) kable(df_panel_attend_date_addone) %&gt;% kable_styling_fc() student_id date_in_class attended 1 2 1 1 4 1 2 1 1 2 2 1 2 5 1 3 2 1 3 3 1 3 5 1 # Pivot Wide df_panel_attend_date_wider &lt;- df_panel_attend_date_addone %&gt;% pivot_wider(names_from = svr_id, values_from = attended) kable(df_panel_attend_date_wider) %&gt;% kable_styling_fc() date_in_class 1 2 3 2 1 1 1 4 1 NA NA 1 NA 1 NA 5 NA 1 1 3 NA NA 1 # Sort and rename # rename see: https://fanwangecon.github.io/R4Econ/amto/tibble/fs_tib_basics.html ar_unique_ids &lt;- sort(unique(df_panel_attend_date %&gt;% pull(!!sym(svr_id)))) df_panel_attend_date_wider_sort &lt;- df_panel_attend_date_wider %&gt;% arrange(!!sym(svr_date)) %&gt;% rename_at(vars(num_range(&#39;&#39;,ar_unique_ids)) , list(~paste0(st_idcol_prefix, . , &#39;&#39;)) ) kable(df_panel_attend_date_wider_sort) %&gt;% kable_styling_fc() date_in_class sid_1 sid_2 sid_3 1 NA 1 NA 2 1 1 1 3 NA NA 1 4 1 NA NA 5 NA 1 1 # replace NA and cumusum again # see: R4Econ/support/function/fs_func_multivar for renaming and replacing df_attend_cumu_by_day &lt;- df_panel_attend_date_wider_sort %&gt;% mutate_at(vars(contains(st_idcol_prefix)), list(~replace_na(., 0))) %&gt;% mutate_at(vars(contains(st_idcol_prefix)), list(~cumsum(.))) kable(df_attend_cumu_by_day) %&gt;% kable_styling_fc() date_in_class sid_1 sid_2 sid_3 1 0 1 0 2 1 2 1 3 1 2 2 4 2 2 2 5 2 3 3 The structure above is also a function in Fan’s REconTools Package, here the function is tested: # Parameters df &lt;- df_panel_attend_date svr_id_i &lt;- &#39;student_id&#39; svr_id_t &lt;- &#39;date_in_class&#39; st_idcol_prefix &lt;- &#39;sid_&#39; # Invoke Function ls_df_rosterwide &lt;- ff_panel_expand_longrosterwide(df, svr_id_t, svr_id_i, st_idcol_prefix) df_roster_wide_func &lt;- ls_df_rosterwide$df_roster_wide df_roster_wide_cumu_func &lt;- ls_df_rosterwide$df_roster_wide_cumu # Print print(df_roster_wide_func) print(df_roster_wide_cumu_func) "],
["linear-regression.html", "Chapter 5 Linear Regression 5.1 OLS and IV 5.2 Decomposition", " Chapter 5 Linear Regression 5.1 OLS and IV Back to Fan’s R4Econ Homepage Table of Content 5.1.1 OLS and IV Regression Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. IV regression using AER package. Option to store all results in dataframe row for combining results from other estimations together. Produce Row Statistics. 5.1.1.1 Construct Program # IV regression function # The code below uses the AER library&#39;s regresison function # All results are stored in a single row as data_frame # This functoin could work with dplyr do # var.y is single outcome, vars.x, vars.c and vars.z are vectors of endogenous variables, controls and instruments. regf.iv &lt;- function(var.y, vars.x, vars.c, vars.z, df, transpose=TRUE) { # A. Set-Up Equation str.vars.x &lt;- paste(vars.x, collapse=&#39;+&#39;) str.vars.c &lt;- paste(vars.c, collapse=&#39;+&#39;) df &lt;- df %&gt;% select(one_of(var.y, vars.x, vars.c, vars.z)) %&gt;% drop_na() %&gt;% filter_all(all_vars(!is.infinite(.))) if (length(vars.z) &gt;= 1) { # library(AER) str.vars.z &lt;- paste(vars.z, collapse=&#39;+&#39;) equa.iv &lt;- paste(var.y, paste(paste(str.vars.x, str.vars.c, sep=&#39;+&#39;), paste(str.vars.z, str.vars.c, sep=&#39;+&#39;), sep=&#39;|&#39;), sep=&#39;~&#39;) # print(equa.iv) # B. IV Regression ivreg.summ &lt;- summary(ivreg(as.formula(equa.iv), data=df), vcov = sandwich, df = Inf, diagnostics = TRUE) # C. Statistics from IV Regression # ivreg.summ$coef # ivreg.summ$diagnostics # D. Combine Regression Results into a Matrix df.results &lt;- suppressWarnings(suppressMessages( as_tibble(ivreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(as_tibble(ivreg.summ$diagnostics, rownames=&#39;rownames&#39;)) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.z=str.vars.z, vars.c=str.vars.c)))) } else { # OLS regression equa.ols &lt;- paste(var.y, paste(paste(vars.x, collapse=&#39;+&#39;), paste(vars.c, collapse=&#39;+&#39;), sep=&#39;+&#39;), sep=&#39;~&#39;) lmreg.summ &lt;- summary(lm(as.formula(equa.ols), data=df)) lm.diagnostics &lt;- as_tibble( list(df1=lmreg.summ$df[[1]], df2=lmreg.summ$df[[2]], df3=lmreg.summ$df[[3]], sigma=lmreg.summ$sigma, r.squared=lmreg.summ$r.squared, adj.r.squared=lmreg.summ$adj.r.squared)) %&gt;% gather(variable, value) %&gt;% rename(rownames = variable) %&gt;% rename(v = value) df.results &lt;- suppressWarnings(suppressMessages( as_tibble(lmreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(lm.diagnostics) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.c=str.vars.c)))) } # E. Flatten Matrix, All IV results as a single tibble # row to be combined with other IV results df.row.results &lt;- df.results %&gt;% gather(variable, value, -rownames) %&gt;% drop_na() %&gt;% unite(esti.val, rownames, variable) %&gt;% mutate(esti.val = gsub(&#39; &#39;, &#39;&#39;, esti.val)) if (transpose) { df.row.results &lt;- df.row.results %&gt;% spread(esti.val, value) } # F. Return return(data.frame(df.row.results)) } 5.1.1.2 Program Testing Load Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) 5.1.1.2.1 Example No Instrument, OLS # One Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- NULL vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 52.1186286658651 prot_Estimate 0.374472386357917 sexMale_Estimate 0.611043720578292 hgt0_Estimate 0.148513781160842 wgt0_Estimate 0.00150560230505631 (Intercept)_Std.Error 1.57770483608693 prot_Std.Error 0.00418121191133815 sexMale_Std.Error 0.118396259120659 hgt0_Std.Error 0.0393807494783186 wgt0_Std.Error 0.000187123663624397 (Intercept)_tvalue 33.0344608660332 prot_tvalue 89.5607288744356 sexMale_tvalue 5.16100529794248 hgt0_tvalue 3.77122790013449 wgt0_tvalue 8.04602836377991 (Intercept)_Pr(&gt;|t|) 9.92126150975783e-233 prot_Pr(&gt;|t|) 0 sexMale_Pr(&gt;|t|) 2.48105505495642e-07 hgt0_Pr(&gt;|t|) 0.000162939618371183 wgt0_Pr(&gt;|t|) 9.05257561534111e-16 df1_v 5 df2_v 18958 df3_v 5 sigma_v 8.06197784622979 r.squared_v 0.319078711001325 adj.r.squared_v 0.318935041565942 vars_var.y hgt vars_vars.x prot vars_vars.c sex+hgt0+wgt0 5.1.1.2.2 Example 1 Insturment # One Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- c(&#39;momEdu&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 43.4301969117558 prot_Estimate 0.130833343849446 sexMale_Estimate 0.868121847262411 hgt0_Estimate 0.412093881817148 wgt0_Estimate 0.000858630042617921 (Intercept)_Std.Error 1.82489550971182 prot_Std.Error 0.0192036220809189 sexMale_Std.Error 0.13373016700542 hgt0_Std.Error 0.0459431912927002 wgt0_Std.Error 0.00022691057702563 (Intercept)_zvalue 23.798730766023 prot_zvalue 6.81295139521853 sexMale_zvalue 6.49159323361366 hgt0_zvalue 8.96963990141069 wgt0_zvalue 3.7840018472164 (Intercept)_Pr(&gt;|z|) 3.4423766196876e-125 prot_Pr(&gt;|z|) 9.56164541643828e-12 sexMale_Pr(&gt;|z|) 8.49333228172763e-11 hgt0_Pr(&gt;|z|) 2.97485394526792e-19 wgt0_Pr(&gt;|z|) 0.000154326676608523 Weakinstruments_df1 1 Wu-Hausman_df1 1 Sargan_df1 0 Weakinstruments_df2 16394 Wu-Hausman_df2 16393 Weakinstruments_statistic 935.817456612075 Wu-Hausman_statistic 123.595856606729 Weakinstruments_p-value 6.39714929178024e-200 Wu-Hausman_p-value 1.30703637796748e-28 vars_var.y hgt vars_vars.x prot vars_vars.z momEdu vars_vars.c sex+hgt0+wgt0 5.1.1.2.3 Example Multiple Instrucments # Multiple Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 42.2437613555242 prot_Estimate 0.26699945194704 sexMale_Estimate 0.695548488812932 hgt0_Estimate 0.424954881263031 wgt0_Estimate 0.000486951420329484 (Intercept)_Std.Error 1.85356686789642 prot_Std.Error 0.0154939347964083 sexMale_Std.Error 0.133157977814374 hgt0_Std.Error 0.0463195803786233 wgt0_Std.Error 0.000224867994873235 (Intercept)_zvalue 22.7905246296649 prot_zvalue 17.2325142357597 sexMale_zvalue 5.22348341593581 hgt0_zvalue 9.17441129192849 wgt0_zvalue 2.16549901022595 (Intercept)_Pr(&gt;|z|) 5.69294074735747e-115 prot_Pr(&gt;|z|) 1.51424021931607e-66 sexMale_Pr(&gt;|z|) 1.75588197502565e-07 hgt0_Pr(&gt;|z|) 4.54048595587756e-20 wgt0_Pr(&gt;|z|) 0.030349491114332 Weakinstruments_df1 4 Wu-Hausman_df1 1 Sargan_df1 3 Weakinstruments_df2 14914 Wu-Hausman_df2 14916 Weakinstruments_statistic 274.147084958343 Wu-Hausman_statistic 17.7562545747101 Sargan_statistic 463.729664547249 Weakinstruments_p-value 8.61731956233366e-228 Wu-Hausman_p-value 2.52567249124181e-05 Sargan_p-value 3.45452874915475e-100 vars_var.y hgt vars_vars.x prot vars_vars.z momEdu+wealthIdx+p.A.prot+p.A.nProt vars_vars.c sex+hgt0+wgt0 5.1.1.2.4 Example Multiple Endogenous Variables # Multiple Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 44.0243196254297 prot_Estimate -1.4025623247106 cal_Estimate 0.065104895750151 sexMale_Estimate 0.120832787571818 hgt0_Estimate 0.286525437984517 wgt0_Estimate 0.000850481389651033 (Intercept)_Std.Error 2.75354847244082 prot_Std.Error 0.198640060273635 cal_Std.Error 0.00758881298880996 sexMale_Std.Error 0.209984580636303 hgt0_Std.Error 0.0707828182888255 wgt0_Std.Error 0.00033711210444429 (Intercept)_zvalue 15.9882130516502 prot_zvalue -7.06082309267581 cal_zvalue 8.57906181719737 sexMale_zvalue 0.575436478267434 hgt0_zvalue 4.04795181812859 wgt0_zvalue 2.52284441418383 (Intercept)_Pr(&gt;|z|) 1.54396598126854e-57 prot_Pr(&gt;|z|) 1.65519210848649e-12 cal_Pr(&gt;|z|) 9.56500648203187e-18 sexMale_Pr(&gt;|z|) 0.564996139463599 hgt0_Pr(&gt;|z|) 5.16677787108928e-05 wgt0_Pr(&gt;|z|) 0.0116409892837831 Weakinstruments(prot)_df1 4 Weakinstruments(cal)_df1 4 Wu-Hausman_df1 2 Sargan_df1 2 Weakinstruments(prot)_df2 14914 Weakinstruments(cal)_df2 14914 Wu-Hausman_df2 14914 Weakinstruments(prot)_statistic 274.147084958343 Weakinstruments(cal)_statistic 315.036848606231 Wu-Hausman_statistic 94.7020085425169 Sargan_statistic 122.081979628898 Weakinstruments(prot)_p-value 8.61731956233366e-228 Weakinstruments(cal)_p-value 1.18918641220866e-260 Wu-Hausman_p-value 1.35024050408262e-41 Sargan_p-value 3.09196773720398e-27 vars_var.y hgt vars_vars.x prot+cal vars_vars.z momEdu+wealthIdx+p.A.prot+p.A.nProt vars_vars.c sex+hgt0+wgt0 5.1.1.2.5 Examples Line by Line The examples are just to test the code with different types of variables. # Selecting Variables var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # A. create Equation str.vars.x &lt;- paste(vars.x, collapse=&#39;+&#39;) str.vars.c &lt;- paste(vars.c, collapse=&#39;+&#39;) str.vars.z &lt;- paste(vars.z, collapse=&#39;+&#39;) print(str.vars.x) ## [1] &quot;prot+cal&quot; print(str.vars.c) ## [1] &quot;sex+hgt0+wgt0&quot; print(str.vars.z) ## [1] &quot;momEdu+wealthIdx+p.A.prot+p.A.nProt&quot; equa.iv &lt;- paste(var.y, paste(paste(str.vars.x, str.vars.c, sep=&#39;+&#39;), paste(str.vars.z, str.vars.c, sep=&#39;+&#39;), sep=&#39;|&#39;), sep=&#39;~&#39;) print(equa.iv) ## [1] &quot;hgt~prot+cal+sex+hgt0+wgt0|momEdu+wealthIdx+p.A.prot+p.A.nProt+sex+hgt0+wgt0&quot; # B. regression res.ivreg &lt;- ivreg(as.formula(equa.iv), data=df) coef(res.ivreg) ## (Intercept) prot cal sexMale hgt0 wgt0 ## 44.0243196254 -1.4025623247 0.0651048958 0.1208327876 0.2865254380 0.0008504814 # C. Regression Summary ivreg.summ &lt;- summary(res.ivreg, vcov = sandwich, df = Inf, diagnostics = TRUE) ivreg.summ$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 44.0243196254 2.7535484724 15.9882131 1.543966e-57 ## prot -1.4025623247 0.1986400603 -7.0608231 1.655192e-12 ## cal 0.0651048958 0.0075888130 8.5790618 9.565006e-18 ## sexMale 0.1208327876 0.2099845806 0.5754365 5.649961e-01 ## hgt0 0.2865254380 0.0707828183 4.0479518 5.166778e-05 ## wgt0 0.0008504814 0.0003371121 2.5228444 1.164099e-02 ## attr(,&quot;df&quot;) ## [1] 0 ivreg.summ$diagnostics ## df1 df2 statistic p-value ## Weak instruments (prot) 4 14914 274.14708 8.617320e-228 ## Weak instruments (cal) 4 14914 315.03685 1.189186e-260 ## Wu-Hausman 2 14914 94.70201 1.350241e-41 ## Sargan 2 NA 122.08198 3.091968e-27 # D. Combine Regression Results into a Matrix df.results &lt;- suppressMessages(as_tibble(ivreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(as_tibble(ivreg.summ$diagnostics, rownames=&#39;rownames&#39;)) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.z=str.vars.z, vars.c=str.vars.c))) # E. Flatten Matrix, All IV results as a single tibble row to be combined with other IV results df.row.results &lt;- df.results %&gt;% gather(variable, value, -rownames) %&gt;% drop_na() %&gt;% unite(esti.val, rownames, variable) %&gt;% mutate(esti.val = gsub(&#39; &#39;, &#39;&#39;, esti.val)) # F. Results as Single Colum # df.row.results # G. Results as Single Row # df.row.results # t(df.row.results %&gt;% spread(esti.val, value)) %&gt;% # kable() %&gt;% # kable_styling_fc_wide() 5.1.2 IV Loop over RHS Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Regression with a Variety of Outcome Variables and Right Hand Side Variables. There are M outcome variables, and there are N alternative right hand side variables. Regress each M outcome variable and each N alternative right hand side variable, with some common sets of controls and perhaps shared instruments. The output file is a M by N matrix of coefficients, with proper variable names and row names. The matrix stores coefficients for this key endogenous variable. Dependency: R4Econ/linreg/ivreg/ivregdfrow.R 5.1.2.1 Construct Program The program relies on double lapply. lapply is used for convenience, not speed. ff_reg_mbyn &lt;- function(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;, time = FALSE) { # regf.iv() function is from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R if (time) { start_time &lt;- Sys.time() } if (return_all) { df.reg.out.all &lt;- bind_rows(lapply(list.vars.x, function(x) ( bind_rows( lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) ))) } else { df.reg.out.all &lt;- (lapply(list.vars.x, function(x) ( bind_rows( lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) %&gt;% select(vars_var.y, starts_with(x)) %&gt;% select(vars_var.y, ends_with(stats_ends)) ))) %&gt;% reduce(full_join) } if (time) { end_time &lt;- Sys.time() print(paste0(&#39;Estimation for all ys and xs took (seconds):&#39;, end_time - start_time)) } return(df.reg.out.all) } 5.1.2.2 Prepare Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) # Source Dependency source(&#39;C:/Users/fan/R4Econ/linreg/ivreg/ivregdfrow.R&#39;) # Setting options(repr.matrix.max.rows=50, repr.matrix.max.cols=50) Parameters. var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) var.y3 &lt;- c(&#39;vil.id&#39;) list.vars.y &lt;- c(var.y1, var.y2, var.y3) var.x1 &lt;- c(&#39;prot&#39;) var.x2 &lt;- c(&#39;cal&#39;) var.x3 &lt;- c(&#39;wealthIdx&#39;) var.x4 &lt;- c(&#39;p.A.prot&#39;) var.x5 &lt;- c(&#39;p.A.nProt&#39;) list.vars.x &lt;- c(var.x1, var.x2, var.x3, var.x4, var.x5) vars.z &lt;- c(&#39;indi.id&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) 5.1.2.3 Program Testing 5.1.2.3.1 Test Program OLS Z-Stat vars.z &lt;- NULL suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_tvalue cal_tvalue wealthIdx_tvalue p.A.prot_tvalue p.A.nProt_tvalue hgt 18.8756010031786 23.4421863484661 13.508899618216 3.83682180045518 32.5448257554855 wgt 16.3591125056062 17.3686031309332 14.1390521528113 1.36958319982295 12.0961557911467 vil.id -14.9385580468907 -19.6150110809452 34.0972558327347 8.45943342783186 17.7801422421419 5.1.2.3.2 Test Program IV T-stat vars.z &lt;- c(&#39;indi.id&#39;) suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_zvalue cal_zvalue wealthIdx_zvalue p.A.prot_zvalue p.A.nProt_zvalue hgt 8.87674929300964 12.0739764947235 4.62589553677969 26.6373587567312 32.1162192385744 wgt 5.60385871756365 6.1225187008946 5.17869536991717 11.9295584469998 12.3509307017263 vil.id -9.22106223347162 -13.0586007975839 -51.5866689219593 -29.9627476577329 -38.3528894620707 5.1.2.3.3 Test Program OLS Coefficient vars.z &lt;- NULL suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;Estimate&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_Estimate cal_Estimate wealthIdx_Estimate p.A.prot_Estimate p.A.nProt_Estimate hgt 0.049431093806755 0.00243408846205622 0.21045655488185 3.86952250259526e-05 0.00542428867316449 wgt 16.5557424523585 0.699072500364623 106.678721085969 0.00521731297924587 0.779514232050632 vil.id -0.0758835879205584 -0.00395676177098486 0.451733304543324 0.000149388430455142 0.00526237555581024 5.1.2.3.4 Test Program IV coefficient vars.z &lt;- c(&#39;indi.id&#39;) suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;Estimate&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_Estimate cal_Estimate wealthIdx_Estimate p.A.prot_Estimate p.A.nProt_Estimate hgt 0.859205733632614 0.0238724384575419 0.144503490136948 0.00148073028434642 0.0141317656200726 wgt 98.9428234201406 2.71948246216953 69.1816142883022 0.221916473012486 2.11856940494335 vil.id -6.02451379136132 -0.168054407187466 -1.91414470908345 -0.00520794333267238 -0.0494468877742109 5.1.2.3.5 Test Program OLS Return All vars.z &lt;- NULL t(suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = TRUE, stats_ends = &#39;Estimate&#39;)))) %&gt;% kable() %&gt;% kable_styling_fc_wide() X.Intercept._Estimate 27.3528514188608 99.873884728925 31.4646660224049 27.9038445914729 219.626705179399 30.5103987898551 35.7840188807906 -2662.74787734003 29.2381039651127 23.9948407749744 -547.959546430028 22.3367814226238 24.4904444950827 -476.703973630552 22.7781908464511 X.Intercept._Pr…t.. 5.68247182214952e-231 0.75529705553815 6.78164655340399e-84 8.24252673989353e-242 0.493216914827181 1.62608789535248e-79 2.26726906489443e-145 7.13318862990131e-05 1.53578035267873e-124 2.11912344053336e-165 0.0941551350855875 3.04337266226599e-49 2.34941965806705e-181 0.143844033032183 9.58029450711211e-52 X.Intercept._Std.Error 0.831272666092284 320.450650378664 1.61328519718754 0.828072565159449 320.522532223672 1.60831193651104 1.38461348429899 670.301542938561 1.22602177264147 0.86658104216672 327.343126852912 1.5098937308759 0.843371070670838 326.132837036936 1.5004526558957 X.Intercept._tvalue 32.9047886867776 0.31166697465244 19.503474077155 33.6973421962119 0.685214557790078 18.9704485163756 25.8440491058106 -3.97246270039407 23.8479483950102 27.6890903532576 -1.6739607509042 14.7936116071335 29.0387533397398 -1.46168652614567 15.1808794212527 adj.r.squared_v 0.814249026159781 0.60716936506893 0.0373247512680971 0.81608922805658 0.607863678511207 0.0453498711076042 0.935014931990565 0.92193683733695 0.059543122812776 0.814690803458616 0.617300597776144 0.0261131074199838 0.824542352656376 0.620250730454724 0.0385437355117917 df1_v 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 df2_v 18957 18962 18999 18957 18962 18999 25092 25102 30013 18587 18591 18845 18587 18591 18845 df3_v 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 hgt0_Estimate 0.60391817340617 56.3852027199184 -0.296844389234445 0.589847843438394 52.9707041800704 -0.273219210757899 0.439374451256039 47.176969664749 -0.35908163982046 0.687269209411865 72.105560623359 -0.108789161111504 0.622395388389206 62.7336220289257 -0.157811627494693 hgt0_Pr…t.. 1.14533314566771e-183 1.52417506966835e-12 1.40290395213743e-13 7.79174951119325e-177 3.05720143843395e-11 8.49149153665126e-12 2.71000479249152e-36 0.00520266507060071 2.41020063623865e-31 1.31914432912869e-220 4.78613024244006e-19 0.0034801146146182 1.11511327164938e-190 8.38546282719268e-15 2.13723119924676e-05 hgt0_Std.Error 0.0206657538633713 7.96735224000553 0.0401060913799595 0.0205836398278421 7.96822145797115 0.0399777363511633 0.0348701896610764 16.8823489375743 0.0307984635553859 0.0213841849324282 8.07744906400683 0.0372288594891345 0.0208846437570215 8.07589192978212 0.0371223237183417 hgt0_tvalue 29.2231378249683 7.0770314931977 -7.40147890309685 28.6561486875877 6.64774497790599 -6.83428417151858 12.6002885423502 2.79445531182864 -11.659076407325 32.1391351404584 8.92677379355593 -2.92217281443323 29.8015803204665 7.76801157994423 -4.25112470577158 prot_Estimate 0.049431093806755 16.5557424523585 -0.0758835879205584 NA NA NA NA NA NA NA NA NA NA NA NA prot_Pr…t.. 9.54769322304645e-79 9.61203373222183e-60 3.56396093562335e-50 NA NA NA NA NA NA NA NA NA NA NA NA prot_Std.Error 0.00261878251179557 1.01201959743751 0.00507971302734622 NA NA NA NA NA NA NA NA NA NA NA NA prot_tvalue 18.8756010031786 16.3591125056062 -14.9385580468907 NA NA NA NA NA NA NA NA NA NA NA NA r.squared_v 0.814298005954592 0.607272921412825 0.0375780335372857 0.816137722617266 0.60796705182314 0.0456010419476623 0.93502787877066 0.921952383432195 0.0596997716363463 0.814740639193486 0.617403496088206 0.0263714328556815 0.824589538985803 0.620352835549783 0.0387987636986586 sexMale_Estimate 0.935177182449406 415.163616765357 -0.254089999175318 0.893484662055608 405.534891838028 -0.181389489610951 1.80682463132073 999.926876716707 -0.33436777751525 0.932686930233136 397.141948675354 -0.445232370681998 0.96466980500711 401.59056368102 -0.423829627017582 sexMale_Pr…t.. 2.36432111724607e-51 2.48252880290814e-67 0.0343768259467621 2.08765935335877e-47 2.51355675686752e-64 0.129768754080748 1.26527362032354e-66 2.64630894140004e-86 0.000311174554787706 7.90489020586094e-47 6.19449742677662e-59 7.93666802281971e-05 1.24556615236597e-52 1.18469030741261e-60 0.00015644693636154 sexMale_Std.Error 0.0618482294097262 23.8518341439675 0.120093045309631 0.0616078355613525 23.8567507583516 0.11972270545355 0.104475287357902 50.5879876531386 0.0927193334338799 0.0647209948973267 24.4473730956481 0.112797805327952 0.0629827627260302 24.3549086073387 0.112083516545945 sexMale_tvalue 15.1205166481668 17.4059409544552 -2.11577613441484 14.5027763743757 16.9987478993157 -1.51508010885476 17.2942776901016 19.7660931597596 -3.60623577771614 14.4108867873979 16.2447698213453 -3.94717228218682 15.316409812052 16.4891016491029 -3.78137339083082 sigma_v 4.21029844914315 1623.77111076428 8.18491760066961 4.18939119979502 1622.33549880859 8.15073036560541 8.18607049768594 3964.45339913597 7.93450742809862 4.35662621773428 1645.77655955938 7.6435668370875 4.23923961592693 1639.42085007515 7.59462918474114 svymthRound_Estimate 0.87166589100565 189.04290688382 -0.0154759587993917 0.851989049736817 185.318286001897 0.0201471237605442 0.432815253441723 189.877994795061 0.00215144302579706 0.91961467696139 205.597385664745 -0.0509574460702806 0.921894094780682 205.945143306004 -0.0557204455206461 svymthRound_Pr…t.. 0 0 0.0397984032097113 0 0 0.0117151185126433 0 0 0.000447277200167272 0 0 1.37139389802397e-18 0 0 7.79141497751766e-23 svymthRound_Std.Error 0.00387681209575621 1.4955473831309 0.00752730297891317 0.00411253488213795 1.59266949679221 0.00799217807522278 0.000728323735328998 0.352701518968252 0.000612792699568233 0.00331108017589107 1.25083486490652 0.00578476859618168 0.00317113547025635 1.22639878616071 0.00565696328562864 svymthRound_tvalue 224.840892330022 126.403823119306 -2.05597660181154 207.168832400006 116.357025971267 2.52085521254888 594.262183761197 538.353209678558 3.51088227277012 277.738571133786 164.368128386085 -8.80889965139067 290.714194782148 167.926734460268 -9.84988636256528 vars_var.y hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id vars_vars.c sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound vars_vars.x prot prot prot cal cal cal wealthIdx wealthIdx wealthIdx p.A.prot p.A.prot p.A.prot p.A.nProt p.A.nProt p.A.nProt wgt0_Estimate -0.000146104685986986 0.637023553461055 -0.000903390591533867 -0.000116898230009949 0.649394003614758 -0.000941137072743919 0.00122231975126219 1.32870822160235 -0.000845938526704796 -0.000489534836079617 0.580023505722658 -0.00156196911156061 3.23596154259101e-05 0.65551206304675 -0.00115432723977403 wgt0_Pr…t.. 0.136011583497549 2.96480083692757e-63 2.05763549729273e-06 0.230228828649018 7.43034302413852e-66 6.66901196231733e-07 1.22269348058816e-13 6.75367630221077e-62 4.32675510884621e-09 7.77000489086602e-07 7.42419220783427e-54 1.40362012201826e-19 0.740027016459552 4.09082062947785e-67 2.75472781728448e-11 wgt0_Std.Error 9.79994437486573e-05 0.0378027371614794 0.000190221503167431 9.74307633896921e-05 0.037739875283113 0.000189270503626621 0.000164767846917989 0.0798131859486402 0.000144040382619518 9.90410500454311e-05 0.0374185042114355 0.000172365145002826 9.75208524392668e-05 0.0377202854835204 0.000173241059789276 wgt0_tvalue -1.49087260496811 16.8512547316329 -4.74915073475531 -1.19980821193398 17.2071051836606 -4.97244448929308 7.41843614592224 16.6477281392748 -5.872926128913 -4.94274682926991 15.5009805428138 -9.0619777654873 0.331822524275644 17.3782370584956 -6.66312732777158 cal_Estimate NA NA NA 0.00243408846205622 0.699072500364623 -0.00395676177098486 NA NA NA NA NA NA NA NA NA cal_Pr…t.. NA NA NA 8.01672708877986e-120 4.71331900885298e-67 7.94646124029527e-85 NA NA NA NA NA NA NA NA NA cal_Std.Error NA NA NA 0.000103833679413418 0.0402492068645167 0.000201721108117477 NA NA NA NA NA NA NA NA NA cal_tvalue NA NA NA 23.4421863484661 17.3686031309332 -19.6150110809452 NA NA NA NA NA NA NA NA NA wealthIdx_Estimate NA NA NA NA NA NA 0.21045655488185 106.678721085969 0.451733304543324 NA NA NA NA NA NA wealthIdx_Pr…t.. NA NA NA NA NA NA 1.93494257274268e-41 3.2548345535026e-45 4.82890644822007e-250 NA NA NA NA NA NA wealthIdx_Std.Error NA NA NA NA NA NA 0.0155791042075745 7.54496977117083 0.0132483771350785 NA NA NA NA NA NA wealthIdx_tvalue NA NA NA NA NA NA 13.508899618216 14.1390521528113 34.0972558327347 NA NA NA NA NA NA p.A.prot_Estimate NA NA NA NA NA NA NA NA NA 3.86952250259526e-05 0.00521731297924587 0.000149388430455142 NA NA NA p.A.prot_Pr…t.. NA NA NA NA NA NA NA NA NA 0.000125048896903791 0.170833589209346 2.88060045451681e-17 NA NA NA p.A.prot_Std.Error NA NA NA NA NA NA NA NA NA 1.00852286184785e-05 0.00380941660201464 1.76593895713687e-05 NA NA NA p.A.prot_tvalue NA NA NA NA NA NA NA NA NA 3.83682180045518 1.36958319982295 8.45943342783186 NA NA NA p.A.nProt_Estimate NA NA NA NA NA NA NA NA NA NA NA NA 0.00542428867316449 0.779514232050632 0.00526237555581024 p.A.nProt_Pr…t.. NA NA NA NA NA NA NA NA NA NA NA NA 5.25341325077391e-226 1.47950939943836e-33 3.7685780281174e-70 p.A.nProt_Std.Error NA NA NA NA NA NA NA NA NA NA NA NA 0.000166671307872964 0.06444313759758 0.000295969260771016 p.A.nProt_tvalue NA NA NA NA NA NA NA NA NA NA NA NA 32.5448257554855 12.0961557911467 17.7801422421419 5.1.2.3.6 Test Program IV Return All vars.z &lt;- c(&#39;indi.id&#39;) t(suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = TRUE, stats_ends = &#39;Estimate&#39;)))) %&gt;% kable() %&gt;% kable_styling_fc_wide() X.Intercept._Estimate 40.2173991882938 1408.1626637032 -64.490636067872 39.6732302990235 1325.54736576331 -59.8304089440729 35.5561817357046 -2791.221534909 21.8005242861645 24.3009261707644 -499.067024090554 21.4632286881661 25.299209739617 -352.278518334717 17.9359211844992 X.Intercept._Pr…z.. 3.69748206920405e-59 0.00217397545504963 0.000109756271656929 1.30030240177373e-103 0.00138952700443324 3.75547414421179e-07 2.01357089467444e-142 1.95034793045284e-05 1.17899313785408e-34 1.97968607369592e-84 0.155922992163314 1.84405333738942e-09 1.29388565624566e-157 0.287184942021997 1.13855583530306e-12 X.Intercept._Std.Error 2.47963650430699 459.377029874119 16.673099250727 1.83545587849039 414.645900526211 11.7754321198995 1.39936229104453 653.605248808641 1.77547715237629 1.2481331128579 351.723712333143 3.57067054655531 0.945826571474308 330.990098562619 2.52170174723203 X.Intercept._zvalue 16.2190704639323 3.06537456626657 -3.86794531107106 21.6149190857443 3.19681772828602 -5.08095230263053 25.4088465605032 -4.27050048939585 12.2786847788984 19.4698193008609 -1.41891776582254 6.01097984491234 26.748254386829 -1.0643173915611 7.11262590993832 hgt0_Estimate 0.403139725681418 35.5765914326678 1.20995060148712 0.357976348180876 31.0172706497394 1.5037447089682 0.460434521499963 59.1545587745268 0.412512139031067 0.515794899569023 46.2591615803265 0.520812513246773 0.510868687340428 45.5654716961559 0.534362107844268 hgt0_Pr…z.. 1.25009876641748e-13 0.000445802636381424 0.00097112649404847 2.82141265004339e-17 0.0013100303315764 3.70002169470828e-08 2.98739737280869e-37 0.000542570320022534 3.02226357947691e-20 8.57492956381676e-59 2.8561488738123e-07 1.10039023747789e-08 3.24936430168307e-102 6.3454545304127e-08 3.42500501176006e-17 hgt0_Std.Error 0.0543948312973965 10.1318250572006 0.366789440587685 0.0423453726223874 9.65135595900306 0.273179527952317 0.0361031059207763 17.1025823111635 0.0447499166716409 0.0319035514861838 9.01263684093548 0.0911390672920558 0.0237991645877977 8.42434865398195 0.063380058773461 hgt0_zvalue 7.41136089709158 3.51137048180512 3.29876072644971 8.45373003027063 3.21377335801252 5.50460248701607 12.7533216258548 3.45880859967647 9.21816552325528 16.1673191711084 5.13270005180026 5.71448149208973 21.4658243761363 5.40878275196011 8.4310762436216 prot_Estimate 0.859205733632614 98.9428234201406 -6.02451379136132 NA NA NA NA NA NA NA NA NA NA NA NA prot_Pr…z.. 6.88427338202428e-19 2.09631602352917e-08 2.94171378745816e-20 NA NA NA NA NA NA NA NA NA NA NA NA prot_Std.Error 0.0967928354481331 17.6561952052848 0.653342710289155 NA NA NA NA NA NA NA NA NA NA NA NA prot_zvalue 8.87674929300964 5.60385871756365 -9.22106223347162 NA NA NA NA NA NA NA NA NA NA NA NA Sargan_df1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 sexMale_Estimate 0.154043421788007 333.799680049259 5.41175429817609 0.106307556057668 330.452608866758 5.83118942788808 1.80283907885782 997.747599807148 -0.452827875182598 1.02741625216018 411.365911332896 -0.789122421167432 1.02009164592608 409.820707458838 -0.746032636368145 sexMale_Pr…z.. 0.38807812932888 5.06413216642981e-24 5.80077629932476e-06 0.423490075745117 2.52735690930834e-27 6.12283824664132e-12 1.1689328480129e-65 2.02347084785411e-89 0.000647195788038449 1.69796551008584e-27 2.05327249429949e-54 0.00428270841484855 1.70848440093529e-51 2.36314216739034e-62 6.57521045473888e-05 sexMale_Std.Error 0.178475271469781 33.0216035385405 1.19371921154418 0.132821186086547 30.5174257711927 0.847955715223327 0.105343525210948 49.7632792630648 0.132754263303719 0.0945646985181925 26.4822313532216 0.276250047248363 0.0675715533063635 24.5920104216267 0.18692145837209 sexMale_zvalue 0.86310792817082 10.1085242471545 4.53352366774387 0.800381017440976 10.8283251459136 6.87676174970095 17.113904962338 20.0498764266063 -3.41102322376347 10.8646912458831 15.5336574870174 -2.85655126226267 15.0964658352764 16.6647907361992 -3.99115565898846 svymthRound_Estimate 0.20990165085783 121.78985943172 4.84745570027424 0.322893837128574 135.494858749214 4.07024693316581 0.433164820953121 190.07735139541 0.0137438264666969 1.00582859923509 218.549980922774 -0.369567838754916 0.929266902426869 207.078222946319 -0.0985678389223824 svymthRound_Pr…z.. 0.00846239710392287 5.96047652813855e-17 2.07373887977152e-19 9.66146445882893e-11 4.48931446042076e-34 5.64723572160763e-36 0 0 1.57416908709431e-66 0 0 2.42696379701225e-102 0 0 1.84569897952709e-27 svymthRound_Std.Error 0.0797183179471441 14.5577085129475 0.538050140685815 0.0498896912188091 11.133488331472 0.325043349284718 0.00120472816008751 0.739269879490032 0.000797655931686456 0.00746867714609297 1.9315711781906 0.0172056989832505 0.00539330635998817 1.46167854745858 0.00907867488118012 svymthRound_zvalue 2.63304164291327 8.36600480930094 9.00930105527994 6.47215545416802 12.1700274626596 12.5221664806331 359.553993426746 257.11496798237 17.2302692435808 134.672925279848 113.146221785884 -21.4793853545086 172.300040161061 141.671520941705 -10.8570733297996 vars_var.y hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id vars_vars.c sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound vars_vars.x prot prot prot cal cal cal wealthIdx wealthIdx wealthIdx p.A.prot p.A.prot p.A.prot p.A.nProt p.A.nProt p.A.nProt vars_vars.z indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id Weakinstruments_df1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Weakinstruments_df2 18957 18962 18999 18957 18962 18999 25092 25102 30013 18587 18591 18845 18587 18591 18845 Weakinstruments_p.value 1.42153759923994e-19 4.45734829676713e-19 5.72345606957941e-20 1.77770827184424e-37 4.03760292920738e-37 5.47447735093002e-38 0 0 0 0 0 0 0 0 0 Weakinstruments_statistic 82.0931934821266 79.8251182827386 83.8989817367586 164.392129625299 162.747072038429 166.75260665498 7029.47383089383 7038.38467113128 12942.6315513372 1710.98122418591 1715.15052113399 1725.71954882902 5097.88462603711 5110.7741807338 5136.55662964887 wgt0_Estimate -0.00163274724538111 0.492582112313709 0.00999798623641602 -0.000658938519302931 0.601258436431587 0.00326074237566435 0.00112485055604169 1.27282038539707 -0.00512158791392237 0.000716628918444932 0.761704518610475 -0.00601345031606092 0.000922100117259348 0.792700893714085 -0.00668277875606482 wgt0_Pr…z.. 4.88365163639597e-08 2.33136555228405e-20 7.95432753711715e-07 0.00032843149807424 2.0921134733036e-48 0.00667886646012294 2.26123807446765e-11 6.67525280062144e-56 6.51923753120087e-127 2.43477572076212e-06 8.2201479288098e-69 5.19751747217521e-44 1.68237436753105e-15 4.81415543564975e-82 2.54848840100353e-105 wgt0_Std.Error 0.00029928487659495 0.0532753838702833 0.00202532507408065 0.000183457551985601 0.0411255751282477 0.00120214094164169 0.000168187467853553 0.08080475140115 0.000213715312589078 0.000152036990658929 0.0434474820359048 0.00043218241369976 0.00011580150512068 0.0413159097814445 0.000306609919182859 wgt0_zvalue -5.45549532591606 9.24596082710666 4.93648469787221 -3.59177647456371 14.6200614716414 2.71244598924594 6.68807593334564 15.7518012657231 -23.9645341827701 4.71351685756907 17.531614789115 -13.9141485757875 7.96276452796019 19.1863351892132 -21.7957030675165 Wu.Hausman_df1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Wu.Hausman_df2 18956 18961 18998 18956 18961 18998 25091 25101 30012 18586 18590 18844 18586 18590 18844 Wu.Hausman_p.value 1.53929570343279e-118 3.13415891402799e-08 0 2.88592507054107e-108 7.6495944085204e-07 0 0.0221987672063003 0.0099360023036833 0 1.80909125272768e-238 2.14946499922491e-35 0 3.15182965429765e-108 1.7681125741529e-17 0 Wu.Hausman_statistic 543.467268879953 30.6481856102772 5652.51924792859 494.955883488045 24.4605456760994 5583.56513052781 5.23078768861684 6.6473469952822 25949.7118056025 1119.87022468742 154.793296861581 4826.92242730041 494.903094649183 72.530787010352 7607.83405438193 cal_Estimate NA NA NA 0.0238724384575419 2.71948246216953 -0.168054407187466 NA NA NA NA NA NA NA NA NA cal_Pr…z.. NA NA NA 1.44956616452661e-33 9.21076021290446e-10 5.67614501764414e-39 NA NA NA NA NA NA NA NA NA cal_Std.Error NA NA NA 0.00197718112735887 0.444177077282291 0.0128692506794877 NA NA NA NA NA NA NA NA NA cal_zvalue NA NA NA 12.0739764947235 6.1225187008946 -13.0586007975839 NA NA NA NA NA NA NA NA NA wealthIdx_Estimate NA NA NA NA NA NA 0.144503490136948 69.1816142883022 -1.91414470908345 NA NA NA NA NA NA wealthIdx_Pr…z.. NA NA NA NA NA NA 3.72983264926432e-06 2.23442991281176e-07 0 NA NA NA NA NA NA wealthIdx_Std.Error NA NA NA NA NA NA 0.0312379492766376 13.358888551386 0.0371054140359243 NA NA NA NA NA NA wealthIdx_zvalue NA NA NA NA NA NA 4.62589553677969 5.17869536991717 -51.5866689219593 NA NA NA NA NA NA p.A.prot_Estimate NA NA NA NA NA NA NA NA NA 0.00148073028434642 0.221916473012486 -0.00520794333267238 NA NA NA p.A.prot_Pr…z.. NA NA NA NA NA NA NA NA NA 2.50759287066563e-156 8.30126393398654e-33 3.00201194005694e-197 NA NA NA p.A.prot_Std.Error NA NA NA NA NA NA NA NA NA 5.55884799941827e-05 0.0186022369560791 0.000173813943639721 NA NA NA p.A.prot_zvalue NA NA NA NA NA NA NA NA NA 26.6373587567312 11.9295584469998 -29.9627476577329 NA NA NA p.A.nProt_Estimate NA NA NA NA NA NA NA NA NA NA NA NA 0.0141317656200726 2.11856940494335 -0.0494468877742109 p.A.nProt_Pr…z.. NA NA NA NA NA NA NA NA NA NA NA NA 2.61782083774363e-226 4.81511329043196e-35 0 p.A.nProt_Std.Error NA NA NA NA NA NA NA NA NA NA NA NA 0.000440019589949091 0.17153115470458 0.00128926108222202 p.A.nProt_zvalue NA NA NA NA NA NA NA NA NA NA NA NA 32.1162192385744 12.3509307017263 -38.3528894620707 5.1.2.4 Program Line by Line Set Up Parameters vars.z &lt;- c(&#39;indi.id&#39;) vars.z &lt;- NULL vars.c &lt;- c(&#39;sex&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) 5.1.2.4.1 Lapply df.reg.out &lt;- as_tibble( bind_rows(lapply(list.vars.y, regf.iv, vars.x=var.x1, vars.c=vars.c, vars.z=vars.z, df=df))) 5.1.2.4.2 Nested Lapply Test lapply(list.vars.y, function(y) (mean(df[[var.x1]], na.rm=TRUE) + mean(df[[y]], na.rm=TRUE))) ## [[1]] ## [1] 98.3272 ## ## [[2]] ## [1] 13626.51 ## ## [[3]] ## [1] 26.11226 lapplytwice &lt;- lapply( list.vars.x, function(x) ( lapply(list.vars.y, function(y) (mean(df[[x]], na.rm=TRUE) + mean(df[[y]], na.rm=TRUE))))) # lapplytwice 5.1.2.4.3 Nested Lapply All df.reg.out.all &lt;- bind_rows( lapply(list.vars.x, function(x) ( bind_rows( lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) ))) # df.reg.out.all %&gt;% # kable() %&gt;% # kable_styling_fc_wide() 5.1.2.4.4 Nested Lapply Select df.reg.out.all &lt;- (lapply(list.vars.x, function(x) ( bind_rows(lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) %&gt;% select(vars_var.y, starts_with(x)) %&gt;% select(vars_var.y, ends_with(&#39;value&#39;)) ))) %&gt;% reduce(full_join) df.reg.out.all %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_tvalue cal_tvalue wealthIdx_tvalue p.A.prot_tvalue p.A.nProt_tvalue hgt 18.8756010031786 23.4421863484661 13.508899618216 3.83682180045518 32.5448257554855 wgt 16.3591125056062 17.3686031309332 14.1390521528113 1.36958319982295 12.0961557911467 vil.id -14.9385580468907 -19.6150110809452 34.0972558327347 8.45943342783186 17.7801422421419 5.2 Decomposition 5.2.1 Decompose RHS Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. One runs a number of regressions. With different outcomes, and various right hand side variables. What is the remaining variation in the left hand side variable if right hand side variable one by one is set to the average of the observed values. Dependency: R4Econ/linreg/ivreg/ivregdfrow.R The code below does not work with categorical variables (except for dummies). Dummy variable inputs need to be converted to zero/one first. The examples are just to test the code with different types of variables. # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) # Source Dependency source(&#39;C:/Users/fan/R4Econ/linreg/ivreg/ivregdfrow.R&#39;) Data Cleaning. # Convert Variable for Sex which is categorical to Numeric df &lt;- df df$male &lt;- (as.numeric(factor(df$sex)) - 1) summary(factor(df$sex)) ## Female Male ## 16446 18619 summary(df$male) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 0.000 1.000 0.531 1.000 1.000 df.use &lt;- df %&gt;% filter(S.country == &#39;Guatemala&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) dim(df.use) ## [1] 2022 16 Setting Up Parameters. # Define Left Hand Side Variab les var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) vars.y &lt;- c(var.y1, var.y2) # Define Right Hand Side Variables vars.x &lt;- c(&#39;prot&#39;) vars.c &lt;- c(&#39;male&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) # vars.z &lt;- c(&#39;p.A.prot&#39;) vars.z &lt;- c(&#39;vil.id&#39;) # vars.z &lt;- NULL vars.xc &lt;- c(vars.x, vars.c) # Other variables to keep vars.other.keep &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;, &#39;svymthRound&#39;) # Decompose sequence vars.tomean.first &lt;- c(&#39;male&#39;, &#39;hgt0&#39;) var.tomean.first.name.suffix &lt;- &#39;_mh02m&#39; vars.tomean.second &lt;- c(vars.tomean.first, &#39;hgt0&#39;, &#39;wgt0&#39;) var.tomean.second.name.suffix &lt;- &#39;_mh0me2m&#39; vars.tomean.third &lt;- c(vars.tomean.second, &#39;prot&#39;) var.tomean.third.name.suffix &lt;- &#39;_mh0mep2m&#39; vars.tomean.fourth &lt;- c(vars.tomean.third, &#39;svymthRound&#39;) var.tomean.fourth.name.suffix &lt;- &#39;_mh0mepm2m&#39; list.vars.tomean = list( # vars.tomean.first, vars.tomean.second, vars.tomean.third, vars.tomean.fourth ) list.vars.tomean.name.suffix &lt;- list( # var.tomean.first.name.suffix, var.tomean.second.name.suffix, var.tomean.third.name.suffix, var.tomean.fourth.name.suffix ) 5.2.1.1 Obtain Regression Coefficients from somewhere # Regressions # regf.iv from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R df.reg.out &lt;- as_tibble( bind_rows(lapply(vars.y, regf.iv, vars.x=vars.x, vars.c=vars.c, vars.z=vars.z, df=df))) # Regressions # reg1 &lt;- regf.iv(var.y = var.y1, vars.x, vars.c, vars.z, df.use) # reg2 &lt;- regf.iv(var.y = var.y2, vars.x, vars.c, vars.z, df.use) # df.reg.out &lt;- as_tibble(bind_rows(reg1, reg2)) # df.reg.out # Select Variables str.esti.suffix &lt;- &#39;_Estimate&#39; arr.esti.name &lt;- paste0(vars.xc, str.esti.suffix) str.outcome.name &lt;- &#39;vars_var.y&#39; arr.columns2select &lt;- c(arr.esti.name, str.outcome.name) arr.columns2select ## [1] &quot;prot_Estimate&quot; &quot;male_Estimate&quot; &quot;wgt0_Estimate&quot; &quot;hgt0_Estimate&quot; &quot;svymthRound_Estimate&quot; &quot;vars_var.y&quot; # Generate dataframe for coefficients df.coef &lt;- df.reg.out[,c(arr.columns2select)] %&gt;% mutate_at(vars(arr.esti.name), as.numeric) %&gt;% column_to_rownames(str.outcome.name) df.coef %&gt;% kable() %&gt;% kable_styling_fc() prot_Estimate male_Estimate wgt0_Estimate hgt0_Estimate svymthRound_Estimate hgt -0.2714772 1.244735 0.0004430 0.6834853 1.133919 wgt -59.0727542 489.852902 0.7696158 75.4867897 250.778883 str(df.coef) ## &#39;data.frame&#39;: 2 obs. of 5 variables: ## $ prot_Estimate : num -0.271 -59.073 ## $ male_Estimate : num 1.24 489.85 ## $ wgt0_Estimate : num 0.000443 0.769616 ## $ hgt0_Estimate : num 0.683 75.487 ## $ svymthRound_Estimate: num 1.13 250.78 5.2.1.2 Decomposition Step 1 # Decomposition Step 1: gather df.decompose_step1 &lt;- df.use %&gt;% filter(svymthRound %in% c(12, 18, 24)) %&gt;% select(one_of(c(vars.other.keep, vars.xc, vars.y))) %&gt;% drop_na() %&gt;% gather(variable, value, -one_of(c(vars.other.keep, vars.xc))) options(repr.matrix.max.rows=20, repr.matrix.max.cols=20) dim(df.decompose_step1) ## [1] 1382 10 head(df.decompose_step1, 10) %&gt;% kable() %&gt;% kable_styling_fc() S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value Guatemala 3 1352 18 13.3 1 2545.2 47.4 hgt 70.2 Guatemala 3 1352 24 46.3 1 2545.2 47.4 hgt 75.8 Guatemala 3 1354 12 1.0 1 3634.3 51.2 hgt 66.3 Guatemala 3 1354 18 9.8 1 3634.3 51.2 hgt 69.2 Guatemala 3 1354 24 15.4 1 3634.3 51.2 hgt 75.3 Guatemala 3 1356 12 8.6 1 3911.8 51.9 hgt 68.1 Guatemala 3 1356 18 17.8 1 3911.8 51.9 hgt 74.1 Guatemala 3 1356 24 30.5 1 3911.8 51.9 hgt 77.1 Guatemala 3 1357 12 1.0 1 3791.4 52.6 hgt 71.5 Guatemala 3 1357 18 12.7 1 3791.4 52.6 hgt 77.8 5.2.1.3 Decomposition Step 2 # Decomposition Step 2: mutate_at(vars, funs(mean = mean(.))) # the xc averaging could have taken place earlier, no difference in mean across variables df.decompose_step2 &lt;- df.decompose_step1 %&gt;% group_by(variable) %&gt;% mutate_at(vars(c(vars.xc, &#39;value&#39;)), funs(mean = mean(.))) %&gt;% ungroup() options(repr.matrix.max.rows=20, repr.matrix.max.cols=20) dim(df.decompose_step2) ## [1] 1382 16 head(df.decompose_step2,10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean Guatemala 3 1352 18 13.3 1 2545.2 47.4 hgt 70.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1352 24 46.3 1 2545.2 47.4 hgt 75.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1354 12 1.0 1 3634.3 51.2 hgt 66.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1354 18 9.8 1 3634.3 51.2 hgt 69.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1354 24 15.4 1 3634.3 51.2 hgt 75.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1356 12 8.6 1 3911.8 51.9 hgt 68.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1356 18 17.8 1 3911.8 51.9 hgt 74.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1356 24 30.5 1 3911.8 51.9 hgt 77.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1357 12 1.0 1 3791.4 52.6 hgt 71.5 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1357 18 12.7 1 3791.4 52.6 hgt 77.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 5.2.1.4 Decomposition Step 3 Non-Loop ff_lr_decompose_valadj &lt;- function(df, df.coef, vars.tomean, str.esti.suffix) { new_value &lt;- (df$value + rowSums((df[paste0(vars.tomean, &#39;_mean&#39;)] - df[vars.tomean]) *df.coef[df$variable, paste0(vars.tomean, str.esti.suffix)])) return(new_value) } 5.2.1.5 Decomposition Step 3 With Loop df.decompose_step3 &lt;- df.decompose_step2 for (i in 1:length(list.vars.tomean)) { var.decomp.cur &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[i]])) vars.tomean &lt;- list.vars.tomean[[i]] var.decomp.cur df.decompose_step3 &lt;- df.decompose_step3 %&gt;% mutate((!!var.decomp.cur) := ff_lr_decompose_valadj(., df.coef, vars.tomean, str.esti.suffix)) } dim(df.decompose_step3) ## [1] 1382 19 head(df.decompose_step3, 10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean value_mh0me2m value_mh0mep2m value_mh0mepm2m Guatemala 3 1352 18 13.3 1 2545.2 47.4 hgt 70.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 73.19390 71.19903 71.68148 Guatemala 3 1352 24 46.3 1 2545.2 47.4 hgt 75.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 78.79390 85.75778 79.43671 Guatemala 3 1354 12 1.0 1 3634.3 51.2 hgt 66.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 63.61689 58.28285 65.56882 Guatemala 3 1354 18 9.8 1 3634.3 51.2 hgt 69.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 66.51689 63.57185 64.05430 Guatemala 3 1354 24 15.4 1 3634.3 51.2 hgt 75.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 72.61689 71.19213 64.87106 Guatemala 3 1356 12 8.6 1 3911.8 51.9 hgt 68.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 64.33707 61.06626 68.35222 Guatemala 3 1356 18 17.8 1 3911.8 51.9 hgt 74.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 70.33707 69.56385 70.04630 Guatemala 3 1356 24 30.5 1 3911.8 51.9 hgt 77.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 73.33707 76.01161 69.69055 Guatemala 3 1357 12 1.0 1 3791.4 52.6 hgt 71.5 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 66.83353 61.49949 68.78545 Guatemala 3 1357 18 12.7 1 3791.4 52.6 hgt 77.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 73.13353 70.97578 71.45823 5.2.1.6 Decomposition Step 4 Variance df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;)) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% select(matches(&#39;value&#39;)) %&gt;% select(ends_with(&quot;_var&quot;)) %&gt;% mutate_if(is.numeric, funs( frac = (./value_var))) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable() %&gt;% kable_styling_fc_wide() value_var value_mean_var value_mh0me2m_var value_mh0mep2m_var value_mh0mepm2m_var value_var_frac value_mean_var_frac value_mh0me2m_var_frac value_mh0mep2m_var_frac value_mh0mepm2m_var_frac 21.864 NA 25.35 49.047 23.06 1 NA 1.159 2.243 1.055 2965693.245 NA 2949187.64 4192769.518 3147506.60 1 NA 0.994 1.414 1.061 5.2.1.7 Graphical Results Graphically, difficult to pick up exact differences in variance, a 50 percent reduction in variance visually does not look like 50 percent. Intuitively, we are kind of seeing standard deviation, not variance on the graph if we think abou the x-scale. head(df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;), -value_mean), 10) %&gt;% kable() %&gt;% kable_styling_fc() variable value value_mh0me2m value_mh0mep2m value_mh0mepm2m hgt 70.2 73.19390 71.19903 71.68148 hgt 75.8 78.79390 85.75778 79.43671 hgt 66.3 63.61689 58.28285 65.56882 hgt 69.2 66.51689 63.57185 64.05430 hgt 75.3 72.61689 71.19213 64.87106 hgt 68.1 64.33707 61.06626 68.35222 hgt 74.1 70.33707 69.56385 70.04630 hgt 77.1 73.33707 76.01161 69.69055 hgt 71.5 66.83353 61.49949 68.78545 hgt 77.8 73.13353 70.97578 71.45823 df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;), -value_mean) %&gt;% rename(outcome = variable) %&gt;% gather(variable, value, -outcome) %&gt;% ggplot(aes(x=value, color = variable, fill = variable)) + geom_line(stat = &quot;density&quot;) + facet_wrap(~ outcome, scales=&#39;free&#39;, nrow=2) 5.2.1.8 Additional Decomposition Testings head(df.decompose_step2[vars.tomean.first],3) head(df.decompose_step2[paste0(vars.tomean.first, &#39;_mean&#39;)], 3) head(df.coef[df.decompose_step2$variable, paste0(vars.tomean.first, str.esti.suffix)], 3) df.decompose.tomean.first &lt;- df.decompose_step2 %&gt;% mutate(pred_new = df.decompose_step2$value + rowSums((df.decompose_step2[paste0(vars.tomean.first, &#39;_mean&#39;)] - df.decompose_step2[vars.tomean.first]) *df.coef[df.decompose_step2$variable, paste0(vars.tomean.first, str.esti.suffix)])) %&gt;% select(variable, value, pred_new) head(df.decompose.tomean.first, 10) df.decompose.tomean.first %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, sd = sd)) %&gt;% kable() %&gt;% kable_styling_fc() variable value_mean pred_new_mean value_sd pred_new_sd hgt 73.41216 73.41216 4.675867 4.534947 wgt 8807.87656 8807.87656 1722.118824 1695.221845 Note the r-square from regression above matches up with the 1 - ratio below. This is the proper decomposition method that is equivalent to r2. df.decompose_step2 %&gt;% mutate(pred_new = df.decompose_step2$value + rowSums((df.decompose_step2[paste0(vars.tomean.second, &#39;_mean&#39;)] - df.decompose_step2[vars.tomean.second]) *df.coef[df.decompose_step2$variable, paste0(vars.tomean.second, str.esti.suffix)])) %&gt;% select(variable, value, pred_new) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% mutate(ratio = (pred_new_var/value_var)) %&gt;% kable() %&gt;% kable_styling_fc() variable value_mean pred_new_mean value_var pred_new_var ratio hgt 73.41216 73.41216 2.186374e+01 25.3504 1.1594724 wgt 8807.87656 8807.87656 2.965693e+06 2949187.6357 0.9944345 "],
["nonlinear-regression.html", "Chapter 6 Nonlinear Regression 6.1 Logit Regression", " Chapter 6 Nonlinear Regression 6.1 Logit Regression 6.1.1 Binary Logit Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Data Preparation df_mtcars &lt;- mtcars # X-variables to use on RHS ls_st_xs &lt;- c(&#39;mpg&#39;, &#39;qsec&#39;) ls_st_xs &lt;- c(&#39;mpg&#39;) ls_st_xs &lt;- c(&#39;qsec&#39;) ls_st_xs &lt;- c(&#39;wt&#39;) ls_st_xs &lt;- c(&#39;mpg&#39;, &#39;wt&#39;, &#39;vs&#39;) svr_binary &lt;- &#39;hpLowHigh&#39; svr_binary_lb0 &lt;- &#39;LowHP&#39; svr_binary_lb1 &lt;- &#39;HighHP&#39; svr_outcome &lt;- &#39;am&#39; sdt_name &lt;- &#39;mtcars&#39; # Discretize hp df_mtcars &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := cut(hp, breaks=c(-Inf, 210, Inf), labels=c(svr_binary_lb0, svr_binary_lb1))) 6.1.1.1 Logit Regresion and Prediction logit regression with glm, and predict using estimation data. Prediction and estimation with one variable. LOGIT REGRESSION R DATA ANALYSIS EXAMPLES Generalized Linear Models # Regress rs_logit &lt;- glm(as.formula(paste(svr_outcome, &quot;~&quot;, paste(ls_st_xs, collapse=&quot;+&quot;))) ,data = df_mtcars, family = &quot;binomial&quot;) summary(rs_logit) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~&quot;, paste(ls_st_xs, ## collapse = &quot;+&quot;))), family = &quot;binomial&quot;, data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.73603 -0.25477 -0.04891 0.13402 1.90321 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 22.69008 13.95112 1.626 0.1039 ## mpg -0.01786 0.33957 -0.053 0.9581 ## wt -6.73804 3.01400 -2.236 0.0254 * ## vs -4.44046 2.84247 -1.562 0.1182 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 13.092 on 28 degrees of freedom ## AIC: 21.092 ## ## Number of Fisher Scoring iterations: 7 # Predcit Using Regression Data df_mtcars$p_mpg &lt;- predict(rs_logit, newdata = df_mtcars, type = &quot;response&quot;) 6.1.1.1.1 Prediction with Observed Binary Input Logit regression with a continuous variable and a binary variable. Predict outcome with observed continuous variable as well as observed binary input variable. # Regress rs_logit_bi &lt;- glm(as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary,&quot;) + &quot;, paste(ls_st_xs, collapse=&quot;+&quot;))) , data = df_mtcars, family = &quot;binomial&quot;) summary(rs_logit_bi) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary, ## &quot;) + &quot;, paste(ls_st_xs, collapse = &quot;+&quot;))), family = &quot;binomial&quot;, ## data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.45771 -0.09563 -0.00875 0.00555 1.87612 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.8285 18.0390 0.212 0.8319 ## factor(hpLowHigh)HighHP 6.9907 5.5176 1.267 0.2052 ## mpg 0.8985 0.8906 1.009 0.3131 ## wt -6.7291 3.3166 -2.029 0.0425 * ## vs -5.9206 4.1908 -1.413 0.1577 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.2297 on 31 degrees of freedom ## Residual deviance: 8.9777 on 27 degrees of freedom ## AIC: 18.978 ## ## Number of Fisher Scoring iterations: 9 # Predcit Using Regresion Data df_mtcars$p_mpg_hp &lt;- predict(rs_logit_bi, newdata = df_mtcars, type = &quot;response&quot;) # Predicted Probabilities am on mgp with or without hp binary scatter &lt;- ggplot(df_mtcars, aes(x=p_mpg_hp, y=p_mpg)) + geom_point(size=1) + # geom_smooth(method=lm) + # Trend line geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Predicted Probabilities &#39;, svr_outcome, &#39; on &#39;, ls_st_xs, &#39; with or without hp binary&#39;), x = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; indicator, 1 is high&#39;), y = paste0(&#39;prediction with only &#39;, ls_st_xs), caption = &#39;mtcars; prediction based on observed data&#39;) + theme_bw() print(scatter) 6.1.1.1.2 Prediction with Binary set to 0 and 1 Now generate two predictions. One set where binary input is equal to 0, and another where the binary inputs are equal to 1. Ignore whether in data binary input is equal to 0 or 1. Use the same regression results as what was just derived. Note that given the example here, the probability changes a lot when we # Previous regression results summary(rs_logit_bi) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary, ## &quot;) + &quot;, paste(ls_st_xs, collapse = &quot;+&quot;))), family = &quot;binomial&quot;, ## data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.45771 -0.09563 -0.00875 0.00555 1.87612 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.8285 18.0390 0.212 0.8319 ## factor(hpLowHigh)HighHP 6.9907 5.5176 1.267 0.2052 ## mpg 0.8985 0.8906 1.009 0.3131 ## wt -6.7291 3.3166 -2.029 0.0425 * ## vs -5.9206 4.1908 -1.413 0.1577 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.2297 on 31 degrees of freedom ## Residual deviance: 8.9777 on 27 degrees of freedom ## AIC: 18.978 ## ## Number of Fisher Scoring iterations: 9 # Two different dataframes, mutate the binary regressor df_mtcars_bi0 &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := svr_binary_lb0) df_mtcars_bi1 &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := svr_binary_lb1) # Predcit Using Regresion Data df_mtcars$p_mpg_hp_bi0 &lt;- predict(rs_logit_bi, newdata = df_mtcars_bi0, type = &quot;response&quot;) df_mtcars$p_mpg_hp_bi1 &lt;- predict(rs_logit_bi, newdata = df_mtcars_bi1, type = &quot;response&quot;) # Predicted Probabilities and Binary Input scatter &lt;- ggplot(df_mtcars, aes(x=p_mpg_hp_bi0)) + geom_point(aes(y=p_mpg_hp), size=4, shape=4, color=&quot;red&quot;) + geom_point(aes(y=p_mpg_hp_bi1), size=2, shape=8) + # geom_smooth(method=lm) + # Trend line geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Predicted Probabilities and Binary Input&#39;, &#39;\\ncross(shape=4)/red is predict actual binary data&#39;, &#39;\\nstar(shape=8)/black is predict set binary = 1 for all&#39;), x = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; = 0 for all&#39;), y = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; = 1&#39;), caption = paste0(sdt_name)) + theme_bw() print(scatter) 6.1.1.1.3 Prediction with Binary set to 0 and 1 Difference What is the difference in probability between binary = 0 vs binary = 1. How does that relate to the probability of outcome of interest when binary = 0 for all. In the binary logit case, the relationship will be hump–shaped by construction between \\(A_i\\) and \\(\\alpha_i\\). In the exponential wage cases, the relationship is convex upwards. # Generate Gap Variable df_mtcars &lt;- df_mtcars %&gt;% mutate(alpha_i = p_mpg_hp_bi1 - p_mpg_hp_bi0) %&gt;% mutate(A_i = p_mpg_hp_bi0) # Binary Marginal Effects and Prediction without Binary scatter &lt;- ggplot(df_mtcars, aes(x=A_i)) + geom_point(aes(y=alpha_i), size=4, shape=4, color=&quot;red&quot;) + geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Binary Marginal Effects and Prediction without Binary&#39;), x = &#39;P(binary=0) for all&#39;, y = &#39;P(binary=1) - P(binary=0) gap&#39;, caption = paste0(sdt_name)) + theme_bw() print(scatter) 6.1.1.1.4 X variables and A and alpha Given the x-variables included in the logit regression, how do they relate to A_i and alpha_i # Generate Gap Variable df_mtcars &lt;- df_mtcars %&gt;% mutate(alpha_i = p_mpg_hp_bi1 - p_mpg_hp_bi0) %&gt;% mutate(A_i = p_mpg_hp_bi0) # Binary Marginal Effects and Prediction without Binary ggplot.A.alpha.x &lt;- function(svr_x, df, svr_alpha = &#39;alpha_i&#39;, svr_A = &quot;A_i&quot;){ scatter &lt;- ggplot(df, aes(x=!!sym(svr_x))) + geom_point(aes(y=alpha_i), size=4, shape=4, color=&quot;red&quot;) + geom_point(aes(y=A_i), size=2, shape=8, color=&quot;blue&quot;) + geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;A (blue) and alpha (red) vs x variables=&#39;, svr_x), x = svr_x, y = &#39;Probabilities&#39;, caption = paste0(sdt_name)) + theme_bw() return(scatter) } # Plot over multiple lapply(ls_st_xs, ggplot.A.alpha.x, df = df_mtcars) ## [[1]] ## ## [[2]] ## ## [[3]] "],
["optimization.html", "Chapter 7 Optimization 7.1 Bisection", " Chapter 7 Optimization 7.1 Bisection 7.1.1 Bisection Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. See the ff_opti_bisect_pmap_multi function from Fan’s REconTools Package, which provides a resuable function based on the algorithm worked out here. The bisection specific code does not need to do much. list variables in file for grouping, each group is an individual for whom we want to calculate optimal choice for using bisection. string variable name of input where functions are evaluated, these are already contained in the dataframe, existing variable names, row specific, rowwise computation over these, each rowwise calculation using different rows. scalar and array values that are applied to every rowwise calculation, all rowwise calculations using the same scalars and arrays. string output variable name This is how I implement the bisection algorithm, when we know the bounding minimum and maximum to be below and above zero already. Evaluate \\(f^0_a = f(a^0)\\) and \\(f^0_b = f(b^0)\\), min and max points. Evaluate at \\(f^0_p = f(p^0)\\), where \\(p_0 = \\frac{a^0+b^0}{2}\\). if \\(f^i_a \\cdot f^i_p &lt; 0\\), then \\(b_{i+1} = p_i\\), else, \\(a_{i+1} = p_i\\) and \\(f^{i+1}_a = p_i\\). iteratre until convergence. Generate New columns of a and b as we iteratre, do not need to store p, p is temporary. Evaluate the function below which we have already tested, but now, in the dataframe before generating all permutations, tb_states_choices, now the fl_N element will be changing with each iteration, it will be row specific. fl_N are first min and max, then each subsequent ps. 7.1.1.1 Initialize Matrix Prepare Input Data: # Parameters fl_rho = 0.20 svr_id_var = &#39;INDI_ID&#39; # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = 4) ar_nN_alpha = seq(0.1, 0.9, length.out = 4) # Choice Grid for nutritional feasible choices for each fl_N_agg = 100 fl_N_min = 0 # Mesh Expand tb_states_choices &lt;- as_tibble(cbind(ar_nN_A, ar_nN_alpha)) %&gt;% rowid_to_column(var=svr_id_var) # Convert Matrix to Tibble ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_states_choices &lt;- tb_states_choices %&gt;% rename_all(~c(ar_st_col_names)) Prepare Function: # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } Initialize the matrix with \\(a_0\\) and \\(b_0\\), the initial min and max points: # common prefix to make reshaping easier st_bisec_prefix &lt;- &#39;bisec_&#39; svr_a_lst &lt;- paste0(st_bisec_prefix, &#39;a_0&#39;) svr_b_lst &lt;- paste0(st_bisec_prefix, &#39;b_0&#39;) svr_fa_lst &lt;- paste0(st_bisec_prefix, &#39;fa_0&#39;) svr_fb_lst &lt;- paste0(st_bisec_prefix, &#39;fb_0&#39;) # Add initial a and b tb_states_choices_bisec &lt;- tb_states_choices %&gt;% mutate(!!sym(svr_a_lst) := fl_N_min, !!sym(svr_b_lst) := fl_N_agg) # Evaluate function f(a_0) and f(b_0) tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% rowwise() %&gt;% mutate(!!sym(svr_fa_lst) := ffi_nonlin_dplyrdo(fl_A, fl_alpha, !!sym(svr_a_lst), ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho), !!sym(svr_fb_lst) := ffi_nonlin_dplyrdo(fl_A, fl_alpha, !!sym(svr_b_lst), ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Summarize dim(tb_states_choices_bisec) ## [1] 4 7 # summary(tb_states_choices_bisec) 7.1.1.2 Iterate and Solve for f(p), update f(a) and f(b) Implement the DPLYR based Concurrent bisection algorithm. # fl_tol = float tolerance criteria # it_tol = number of interations to allow at most fl_tol &lt;- 10^-2 it_tol &lt;- 100 # fl_p_dist2zr = distance to zero to initalize fl_p_dist2zr &lt;- 1000 it_cur &lt;- 0 while (it_cur &lt;= it_tol &amp;&amp; fl_p_dist2zr &gt;= fl_tol ) { it_cur &lt;- it_cur + 1 # New Variables svr_a_cur &lt;- paste0(st_bisec_prefix, &#39;a_&#39;, it_cur) svr_b_cur &lt;- paste0(st_bisec_prefix, &#39;b_&#39;, it_cur) svr_fa_cur &lt;- paste0(st_bisec_prefix, &#39;fa_&#39;, it_cur) svr_fb_cur &lt;- paste0(st_bisec_prefix, &#39;fb_&#39;, it_cur) # Evaluate function f(a_0) and f(b_0) # 1. generate p # 2. generate f_p # 3. generate f_p*f_a tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% rowwise() %&gt;% mutate(p = ((!!sym(svr_a_lst) + !!sym(svr_b_lst))/2)) %&gt;% mutate(f_p = ffi_nonlin_dplyrdo(fl_A, fl_alpha, p, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) %&gt;% mutate(f_p_t_f_a = f_p*!!sym(svr_fa_lst)) # fl_p_dist2zr = sum(abs(p)) fl_p_dist2zr &lt;- mean(abs(tb_states_choices_bisec %&gt;% pull(f_p))) # Update a and b tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% mutate(!!sym(svr_a_cur) := case_when(f_p_t_f_a &lt; 0 ~ !!sym(svr_a_lst), TRUE ~ p)) %&gt;% mutate(!!sym(svr_b_cur) := case_when(f_p_t_f_a &lt; 0 ~ p, TRUE ~ !!sym(svr_b_lst))) # Update f(a) and f(b) tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% mutate(!!sym(svr_fa_cur) := case_when(f_p_t_f_a &lt; 0 ~ !!sym(svr_fa_lst), TRUE ~ f_p)) %&gt;% mutate(!!sym(svr_fb_cur) := case_when(f_p_t_f_a &lt; 0 ~ f_p, TRUE ~ !!sym(svr_fb_lst))) # Save from last svr_a_lst &lt;- svr_a_cur svr_b_lst &lt;- svr_b_cur svr_fa_lst &lt;- svr_fa_cur svr_fb_lst &lt;- svr_fb_cur # Summar current round print(paste0(&#39;it_cur:&#39;, it_cur, &#39;, fl_p_dist2zr:&#39;, fl_p_dist2zr)) summary(tb_states_choices_bisec %&gt;% select(one_of(svr_a_cur, svr_b_cur, svr_fa_cur, svr_fb_cur))) } ## [1] &quot;it_cur:1, fl_p_dist2zr:1597.93916362849&quot; ## [1] &quot;it_cur:2, fl_p_dist2zr:676.06602535902&quot; ## [1] &quot;it_cur:3, fl_p_dist2zr:286.850590132782&quot; ## [1] &quot;it_cur:4, fl_p_dist2zr:117.225493866655&quot; ## [1] &quot;it_cur:5, fl_p_dist2zr:37.570593471664&quot; ## [1] &quot;it_cur:6, fl_p_dist2zr:4.60826664896022&quot; ## [1] &quot;it_cur:7, fl_p_dist2zr:14.4217689135683&quot; ## [1] &quot;it_cur:8, fl_p_dist2zr:8.38950830086659&quot; ## [1] &quot;it_cur:9, fl_p_dist2zr:3.93347761455868&quot; ## [1] &quot;it_cur:10, fl_p_dist2zr:1.88261338941038&quot; ## [1] &quot;it_cur:11, fl_p_dist2zr:0.744478952222305&quot; ## [1] &quot;it_cur:12, fl_p_dist2zr:0.187061801237917&quot; ## [1] &quot;it_cur:13, fl_p_dist2zr:0.117844913432613&quot; ## [1] &quot;it_cur:14, fl_p_dist2zr:0.0275365951418891&quot; ## [1] &quot;it_cur:15, fl_p_dist2zr:0.0515488156908255&quot; ## [1] &quot;it_cur:16, fl_p_dist2zr:0.0191152349149135&quot; ## [1] &quot;it_cur:17, fl_p_dist2zr:0.00385372194545752&quot; 7.1.1.3 Reshape Wide to long to Wide To view results easily, how iterations improved to help us find the roots, convert table from wide to long. Pivot twice. This allows us to easily graph out how bisection is working out iterationby iteration. Here, we will first show what the raw table looks like, the wide only table, and then show the long version, and finally the version that is medium wide. 7.1.1.3.1 Table One–Very Wide Show what the tb_states_choices_bisec looks like. Variables are formatted like: bisec_xx_yy, where yy is the iteration indicator, and xx is either a, b, fa, or fb. kable(head(t(tb_states_choices_bisec), 25)) %&gt;% kable_styling_fc() INDI_ID 1.000000e+00 2.0000000 3.0000000 4.0000000 fl_A -2.000000e+00 -0.6666667 0.6666667 2.0000000 fl_alpha 1.000000e-01 0.3666667 0.6333333 0.9000000 bisec_a_0 0.000000e+00 0.0000000 0.0000000 0.0000000 bisec_b_0 1.000000e+02 100.0000000 100.0000000 100.0000000 bisec_fa_0 1.000000e+02 100.0000000 100.0000000 100.0000000 bisec_fb_0 -1.288028e+04 -1394.7069782 -323.9421599 -51.9716069 p 1.544952e+00 8.5838318 24.8359680 65.0367737 f_p -7.637200e-03 -0.0052211 -0.0016162 -0.0009405 f_p_t_f_a -3.800000e-04 -0.0000237 -0.0000025 -0.0000002 bisec_a_1 0.000000e+00 0.0000000 0.0000000 50.0000000 bisec_b_1 5.000000e+01 50.0000000 50.0000000 100.0000000 bisec_fa_1 1.000000e+02 100.0000000 100.0000000 22.5557704 bisec_fb_1 -5.666956e+03 -595.7345364 -106.5105843 -51.9716069 bisec_a_2 0.000000e+00 0.0000000 0.0000000 50.0000000 bisec_b_2 2.500000e+01 25.0000000 25.0000000 75.0000000 bisec_fa_2 1.000000e+02 100.0000000 100.0000000 22.5557704 bisec_fb_2 -2.464562e+03 -224.1460032 -0.6857375 -14.8701831 bisec_a_3 0.000000e+00 0.0000000 12.5000000 62.5000000 bisec_b_3 1.250000e+01 12.5000000 25.0000000 75.0000000 bisec_fa_3 1.000000e+02 100.0000000 50.8640414 3.7940196 bisec_fb_3 -1.041574e+03 -51.1700464 -0.6857375 -14.8701831 bisec_a_4 0.000000e+00 6.2500000 18.7500000 62.5000000 bisec_b_4 6.250000e+00 12.5000000 25.0000000 68.7500000 bisec_fa_4 1.000000e+02 29.4271641 25.2510409 3.7940196 # str(tb_states_choices_bisec) 7.1.1.3.2 Table Two–Very Wide to Very Long We want to treat the iteration count information that is the suffix of variable names as a variable by itself. Additionally, we want to treat the a,b,fa,fb as a variable. Structuring the data very long like this allows for easy graphing and other types of analysis. Rather than dealing with many many variables, we have only 3 core variables that store bisection iteration information. Here we use the very nice pivot_longer function. Note that to achieve this, we put a common prefix in front of the variables we wanted to convert to long. THis is helpful, because we can easily identify which variables need to be reshaped. # New variables svr_bisect_iter &lt;- &#39;biseciter&#39; svr_abfafb_long_name &lt;- &#39;varname&#39; svr_number_col &lt;- &#39;value&#39; svr_id_bisect_iter &lt;- paste0(svr_id_var, &#39;_bisect_ier&#39;) # Pivot wide to very long tb_states_choices_bisec_long &lt;- tb_states_choices_bisec %&gt;% pivot_longer( cols = starts_with(st_bisec_prefix), names_to = c(svr_abfafb_long_name, svr_bisect_iter), names_pattern = paste0(st_bisec_prefix, &quot;(.*)_(.*)&quot;), values_to = svr_number_col ) # Print # summary(tb_states_choices_bisec_long) kable(head(tb_states_choices_bisec_long %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 15)) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha varname biseciter value 1 -2 0.1 a 0 0.000 1 -2 0.1 b 0 100.000 1 -2 0.1 fa 0 100.000 1 -2 0.1 fb 0 -12880.284 1 -2 0.1 a 1 0.000 1 -2 0.1 b 1 50.000 1 -2 0.1 fa 1 100.000 1 -2 0.1 fb 1 -5666.956 1 -2 0.1 a 2 0.000 1 -2 0.1 b 2 25.000 1 -2 0.1 fa 2 100.000 1 -2 0.1 fb 2 -2464.562 1 -2 0.1 a 3 0.000 1 -2 0.1 b 3 12.500 1 -2 0.1 fa 3 100.000 kable(tail(tb_states_choices_bisec_long %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 15)) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha varname biseciter value 4 2 0.9 b 14 65.0390625 4 2 0.9 fa 14 0.0047633 4 2 0.9 fb 14 -0.0043628 4 2 0.9 a 15 65.0360107 4 2 0.9 b 15 65.0390625 4 2 0.9 fa 15 0.0002003 4 2 0.9 fb 15 -0.0043628 4 2 0.9 a 16 65.0360107 4 2 0.9 b 16 65.0375366 4 2 0.9 fa 16 0.0002003 4 2 0.9 fb 16 -0.0020812 4 2 0.9 a 17 65.0360107 4 2 0.9 b 17 65.0367737 4 2 0.9 fa 17 0.0002003 4 2 0.9 fb 17 -0.0009405 7.1.1.3.3 Table Two–Very Very Long to Wider Again But the previous results are too long, with the a, b, fa, and fb all in one column as different categories, they are really not different categories, they are in fact different types of variables. So we want to spread those four categories of this variable into four columns, each one representing the a, b, fa, and fb values. The rows would then be uniquly identified by the iteration counter and individual ID. # Pivot wide to very long to a little wide tb_states_choices_bisec_wider &lt;- tb_states_choices_bisec_long %&gt;% pivot_wider( names_from = !!sym(svr_abfafb_long_name), values_from = svr_number_col ) # Print # summary(tb_states_choices_bisec_wider) kable(head(tb_states_choices_bisec_wider %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 10)) %&gt;% kable_styling_fc_wide() INDI_ID fl_A fl_alpha biseciter a b fa fb 1 -2 0.1 0 0.000000 100.0000 100.00000 -12880.283918 1 -2 0.1 1 0.000000 50.0000 100.00000 -5666.955763 1 -2 0.1 2 0.000000 25.0000 100.00000 -2464.562178 1 -2 0.1 3 0.000000 12.5000 100.00000 -1041.574253 1 -2 0.1 4 0.000000 6.2500 100.00000 -408.674764 1 -2 0.1 5 0.000000 3.1250 100.00000 -126.904283 1 -2 0.1 6 0.000000 1.5625 100.00000 -1.328965 1 -2 0.1 7 0.781250 1.5625 54.69612 -1.328965 1 -2 0.1 8 1.171875 1.5625 27.46061 -1.328965 1 -2 0.1 9 1.367188 1.5625 13.23495 -1.328965 kable(head(tb_states_choices_bisec_wider %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 10)) %&gt;% kable_styling_fc_wide() INDI_ID fl_A fl_alpha biseciter a b fa fb 1 -2 0.1 0 0.000000 100.0000 100.00000 -12880.283918 1 -2 0.1 1 0.000000 50.0000 100.00000 -5666.955763 1 -2 0.1 2 0.000000 25.0000 100.00000 -2464.562178 1 -2 0.1 3 0.000000 12.5000 100.00000 -1041.574253 1 -2 0.1 4 0.000000 6.2500 100.00000 -408.674764 1 -2 0.1 5 0.000000 3.1250 100.00000 -126.904283 1 -2 0.1 6 0.000000 1.5625 100.00000 -1.328965 1 -2 0.1 7 0.781250 1.5625 54.69612 -1.328965 1 -2 0.1 8 1.171875 1.5625 27.46061 -1.328965 1 -2 0.1 9 1.367188 1.5625 13.23495 -1.328965 7.1.1.4 Graph Bisection Iteration Results Actually we want to graph based on the long results, not the wider. Wider easier to view in table. # Graph results lineplot &lt;- tb_states_choices_bisec_long %&gt;% mutate(!!sym(svr_bisect_iter) := as.numeric(!!sym(svr_bisect_iter))) %&gt;% filter(!!sym(svr_abfafb_long_name) %in% c(&#39;a&#39;, &#39;b&#39;)) %&gt;% ggplot(aes(x=!!sym(svr_bisect_iter), y=!!sym(svr_number_col), colour=!!sym(svr_abfafb_long_name), linetype=!!sym(svr_abfafb_long_name), shape=!!sym(svr_abfafb_long_name))) + facet_wrap( ~ INDI_ID) + geom_line() + geom_point() + labs(title = &#39;Bisection Iteration over individuals Until Convergence&#39;, x = &#39;Bisection Iteration&#39;, y = &#39;a (left side point) and b (right side point) values&#39;, caption = &#39;DPLYR concurrent bisection nonlinear multple individuals&#39;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) print(lineplot) "],
["mathmatics-and-statistics.html", "Chapter 8 Mathmatics and Statistics 8.1 Distributions 8.2 Analytical Solutions 8.3 Inequality Models", " Chapter 8 Mathmatics and Statistics 8.1 Distributions 8.1.1 Integrate Over Normal Guassian Process Shock Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Some Common parameters fl_eps_mean = 10 fl_eps_sd = 50 fl_cdf_min = 0.000001 fl_cdf_max = 0.999999 ar_it_draws &lt;- seq(1, 1000) 8.1.1.1 Randomly Sample and Integrate (Monte Carlo integration) Compare randomly drawn normal shock mean and known mean. How does simulated mean change with draws. Actual integral equals to \\(10\\), as sample size increases, the sample mean approaches the integration results, but this is expensive, even with ten thousand draws, not very exact. # Simulate Draws set.seed(123) ar_fl_means &lt;- sapply(ar_it_draws, function(x) return(mean(rnorm(x[1], mean=fl_eps_mean, sd=fl_eps_sd)))) ar_fl_sd &lt;- sapply(ar_it_draws, function(x) return(sd(rnorm(x[1], mean=fl_eps_mean, sd=fl_eps_sd)))) mt_sample_means &lt;- cbind(ar_it_draws, ar_fl_means, ar_fl_sd) colnames(mt_sample_means) &lt;- c(&#39;draw_count&#39;, &#39;mean&#39;, &#39;sd&#39;) tb_sample_means &lt;- as_tibble(mt_sample_means) # Graph # x-labels x.labels &lt;- c(&#39;n=1&#39;, &#39;n=10&#39;, &#39;n=100&#39;, &#39;n=1000&#39;) x.breaks &lt;- c(1, 10, 100, 1000) # Graph Results--Draw plt_mean &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=mean)) + geom_line(size=0.75) + labs(title = paste0(&#39;Sample Average as Sample Size Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Sample Size&#39;, y = &#39;Sample Mean&#39;, caption = &#39;Mean of Sample Integrates to Mean&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_mean) plt_sd &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=sd)) + geom_line(size=0.75) + labs(title = paste0(&#39;Sample Standard Deviation as Sample Size Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Sample Size&#39;, y = &#39;Sample Standard Deviation&#39;, caption = &#39;Standard Deviation of Sample Integrates to True Standard Deviation&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_sd) 8.1.1.2 Integration By Symmetric Uneven Rectangle Draw on grid from probability space, and then find use norm inverse to find corresponding x point. Under this approach, each rectangle is suppose to approximate the same same. So even area, but uneven width. Resulting integration is rectangle based, but rectangle width differ. Th rectangle have wider width as they move away from the mean, and thin width close to the mean. This is much more stable than the random draw method, and approximates the true answer more accurately. mt_fl_means &lt;- sapply(ar_it_draws, function(x) { fl_prob_break = (fl_cdf_max - fl_cdf_min)/(x[1]) ar_eps_bounds &lt;- qnorm(seq(fl_cdf_min, fl_cdf_max, by=(fl_cdf_max - fl_cdf_min)/(x[1])), mean = fl_eps_mean, sd = fl_eps_sd) ar_eps_val &lt;- (tail(ar_eps_bounds, -1) + head(ar_eps_bounds, -1))/2 ar_eps_prb &lt;- rep(fl_prob_break/(fl_cdf_max - fl_cdf_min), x[1]) ar_eps_fev &lt;- dnorm(ar_eps_val, mean = fl_eps_mean, sd = fl_eps_sd) fl_cdf_total_approx &lt;- sum(ar_eps_fev*diff(ar_eps_bounds)) fl_mean_approx &lt;- sum(ar_eps_val*(ar_eps_fev*diff(ar_eps_bounds))) fl_sd_approx &lt;- sqrt(sum((ar_eps_val-fl_mean_approx)^2*(ar_eps_fev*diff(ar_eps_bounds)))) return(list(cdf=fl_cdf_total_approx, mean=fl_mean_approx, sd=fl_sd_approx)) }) mt_sample_means &lt;- cbind(ar_it_draws, as_tibble(t(mt_fl_means)) %&gt;% unnest()) colnames(mt_sample_means) &lt;- c(&#39;draw_count&#39;, &#39;cdf&#39;, &#39;mean&#39;, &#39;sd&#39;) tb_sample_means &lt;- as_tibble(mt_sample_means) # Graph # x-labels x.labels &lt;- c(&#39;n=1&#39;, &#39;n=10&#39;, &#39;n=100&#39;, &#39;n=1000&#39;) x.breaks &lt;- c(1, 10, 100, 1000) # Graph Results--Draw plt_mean &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=mean)) + geom_line(size=0.75) + labs(title = paste0(&#39;Average as Uneven Rectangle Count Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Number of Uneven Rectangles for Approximation&#39;, y = &#39;Integrated Mean&#39;, caption = &#39;Integral Approximation as Uneven Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_mean) plt_sd &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=sd)) + geom_line(size=0.75) + labs(title = paste0(&#39;Standard Deviation as Uneven Rectangle Count Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Number of Uneven Rectangles for Approximation&#39;, y = &#39;Standard Deviation&#39;, caption = &#39;Integral Approximation as Uneven Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_sd) plt_cdf &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=cdf)) + geom_line(size=0.75) + labs(title = paste0(&#39;Aggregate Probability as Uneven Rectangle Count Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Number of Uneven Rectangles for Approximation&#39;, y = &#39;Aggregate Probability&#39;, caption = &#39;Aggregate Probability Approximation as Uneven Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_cdf) 8.1.1.3 Integration By Constant Width Rectangle (Trapezoidal rule) This is implementing even width recentagle, even along x-axis take points, and measure \\(f(x)\\). Rectangle width are the same. This is even width, but uneven area. Note that this method approximates the true answer much better and more quickly. mt_fl_means &lt;- sapply(ar_it_draws, function(x) { fl_eps_min &lt;- qnorm(fl_cdf_min, mean = fl_eps_mean, sd = fl_eps_sd) fl_eps_max &lt;- qnorm(fl_cdf_max, mean = fl_eps_mean, sd = fl_eps_sd) fl_gap &lt;- (fl_eps_max-fl_eps_min)/(x[1]) ar_eps_bounds &lt;- seq(fl_eps_min, fl_eps_max, by=fl_gap) ar_eps_val &lt;- (tail(ar_eps_bounds, -1) + head(ar_eps_bounds, -1))/2 ar_eps_prb &lt;- dnorm(ar_eps_val, mean = fl_eps_mean, sd = fl_eps_sd)*fl_gap fl_cdf_total_approx &lt;- sum(ar_eps_prb) fl_mean_approx &lt;- sum(ar_eps_val*ar_eps_prb) fl_sd_approx &lt;- sqrt(sum((ar_eps_val-fl_mean_approx)^2*ar_eps_prb)) return(list(cdf=fl_cdf_total_approx, mean=fl_mean_approx, sd=fl_sd_approx)) }) mt_sample_means &lt;- cbind(ar_it_draws, as_tibble(t(mt_fl_means)) %&gt;% unnest()) colnames(mt_sample_means) &lt;- c(&#39;draw_count&#39;, &#39;cdf&#39;, &#39;mean&#39;, &#39;sd&#39;) tb_sample_means &lt;- as_tibble(mt_sample_means) # Graph # x-labels x.labels &lt;- c(&#39;n=1&#39;, &#39;n=10&#39;, &#39;n=100&#39;, &#39;n=1000&#39;) x.breaks &lt;- c(1, 10, 100, 1000) # Graph Results--Draw plt_mean &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=mean)) + geom_line(size=0.75) + labs(title = paste0(&#39;Average as Even Rectangle Count Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Number of Even Rectangles for Approximation&#39;, y = &#39;Integrated Mean&#39;, caption = &#39;Integral Approximation as Even Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_mean) plt_sd &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=sd)) + geom_line(size=0.75) + labs(title = paste0(&#39;Standard Deviation as Even Rectangle Count Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Number of Even Rectangles for Approximation&#39;, y = &#39;Standard Deviation&#39;, caption = &#39;Integral Approximation as Even Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_sd) plt_cdf &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=cdf)) + geom_line(size=0.75) + labs(title = paste0(&#39;Aggregate Probability as Even Rectangle Count Increases\\n True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd), x = &#39;Number of Even Rectangles for Approximation&#39;, y = &#39;Aggregate Probability&#39;, caption = &#39;Aggregate Probability Approximation as Even Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_cdf) 8.2 Analytical Solutions 8.2.1 Linear Scalar f(x)=0 Solutions Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 8.2.1.1 Ratio Here are some common ratios. 8.2.1.1.1 Unif Draw Min and Max Ratio We want to draw numbers such that we have some mean \\(b\\), and that the possible maximum and minimum value drawn are at most \\(a\\) times apart. Given \\(b\\) and \\(a\\), solve for \\(x\\). \\[ f(x) = \\frac{b+x}{b-x} - a = 0 \\] \\[ b \\cdot a - x \\cdot a = b + x \\\\ b \\cdot a - b = x + x \\cdot a \\\\ b \\left(a - 1\\right) = x \\left( a+ 1\\right) \\\\ x = \\frac{b\\left(a-1\\right)}{a+1}\\\\ \\] Uniformly draw b &lt;- 100 a &lt;- 2 x &lt;- (b*(a-1))/(a+1) ar_unif_draws &lt;- runif(100, min=b-x, max=b+x) fl_max_min_ratio &lt;- max(ar_unif_draws)/min(ar_unif_draws) cat(&#39;fl_max_min_ratio =&#39;, fl_max_min_ratio, &#39;is close to a =&#39;, a, &#39;\\n&#39;) ## fl_max_min_ratio = 1.965882 is close to a = 2 8.3 Inequality Models 8.3.1 Gini Discrete Sample Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. This works out how the ff_dist_gini_vector_pos function works from Fan’s REconTools Package. 8.3.1.1 Gini Formula for Discrete Sample There is an vector values (all positive). This could be height information for N individuals. It could also be income information for N individuals. Calculate the GINI coefficient treating the given vector as population. This is not an estimation exercise where we want to estimate population gini based on a sample. The given array is the population. The population is discrete, and only has these N individuals in the length n vector. Note that when the sample size is small, there is a limit to inequality using the formula defined below given each \\(N\\). So for small \\(N\\), can not really compare inequality across arrays with different \\(N\\), can only compare arrays with the same \\(N\\). In another word, if 1 of N individual holds all resource, as \\(N\\) increases, GINI will asymptote to 1, but it is very far away from 1 for low N. The GINI formula used here is: \\[ GINI = 1 - \\frac{2}{N+1} \\cdot \\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right) \\cdot \\left( \\sum_{i=1}^N x_i \\right)^{-1} \\] Derive the formula in the steps below. Step 1 Area Formula \\[ \\Gamma = \\sum_{i=1}^N \\frac{1}{N} \\cdot \\left( \\sum_{j=1}^{i} \\left( \\frac{x_j}{\\sum_{\\widehat{j}=1}^N x_{\\widehat{j}} } \\right) \\right) \\] Step 2 Total Area Given Perfect equality With perfect equality \\(x_i=a\\) for all \\(i\\), so need to divide by that. \\[ \\Gamma^{\\text{equal}} = \\sum_{i=1}^N \\frac{1}{N} \\cdot \\left( \\sum_{j=1}^{i} \\left( \\frac{a}{\\sum_{\\widehat{j}=1}^N a } \\right) \\right) = \\frac{N+1}{N}\\cdot\\frac{1}{2} \\] As the number of elements of the vecotr increases: \\[ \\lim_{N \\rightarrow \\infty}\\Gamma^{\\text{equal}} = \\lim_{N \\rightarrow \\infty} \\frac{N+1}{N}\\cdot\\frac{1}{2} = \\frac{1}{2} \\] Step 3 Arriving at Finite Vector Gini Formula Given what we have from above, we obtain the gini formula, divide by total area below 45 degree line. \\[ GINI = 1 - \\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right) \\cdot \\left( N \\cdot \\sum_{i=1}^N x_i \\right)^{-1} \\cdot \\left( \\frac{N+1}{N}\\cdot\\frac{1}{2} \\right)^{-1} = 1 - \\frac{2}{N+1} \\cdot \\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right) \\cdot \\left( \\sum_{i=1}^N x_i \\right)^{-1} \\] Step 4 Maximum Inequality given N Suppose \\(x_i=0\\) for all \\(i&lt;N\\), then: \\[ GINI^{x_i = 0 \\text{ except } i=N} = 1 - \\frac{2}{N+1} \\cdot X_N \\cdot \\left( X_N \\right)^{-1} = 1 - \\frac{2}{N+1} \\] \\[ \\lim_{N \\rightarrow \\infty} GINI^{x_i = 0 \\text{ except } i=N} = 1 - \\lim_{N \\rightarrow \\infty} \\frac{2}{N+1} = 1 \\] Note that for small N, for example if \\(N=10\\), even when one person holds all income, all others have 0 income, the formula will not produce gini is zero, but that gini is equal to \\(\\frac{2}{11}\\approx 0.1818\\). If \\(N=2\\), inequality is at most, \\(\\frac{2}{3}\\approx 0.667\\). \\[ MostUnequalGINI\\left(N\\right) = 1 - \\frac{2}{N+1} = \\frac{N-1}{N+1} \\] 8.3.1.2 Implement GINI Formula The GINI formula just derived is trivial to compute. scalar: \\(\\frac{2}{N+1}\\) cumsum: \\(\\sum_{j=1}^{i} x_j\\) sum of cumsum: \\(\\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right)\\) sum: \\(\\sum_{i=1}^N X_i\\) There are no package dependencies. Define the formula here: # Formula, directly implement the GINI formula Following Step 4 above fv_dist_gini_vector_pos_test &lt;- function(ar_pos) { # Check length and given warning it_n &lt;- length(ar_pos) if (it_n &lt;= 100) warning(&#39;Data vector has n=&#39;,it_n,&#39;, max-inequality/max-gini=&#39;,(it_n-1)/(it_n + 1)) # Sort ar_pos &lt;- sort(ar_pos) # formula implement fl_gini &lt;- 1 - ((2/(it_n+1)) * sum(cumsum(ar_pos))*(sum(ar_pos))^(-1)) return(fl_gini) } Generate a number of examples Arrays for testing # Example Arrays of data ar_equal_n1 = c(1) ar_ineql_n1 = c(100) ar_equal_n2 = c(1,1) ar_ineql_alittle_n2 = c(1,2) ar_ineql_somewht_n2 = c(1,2^3) ar_ineql_alotine_n2 = c(1,2^5) ar_ineql_veryvry_n2 = c(1,2^8) ar_ineql_mostmst_n2 = c(1,2^13) ar_equal_n10 = c(2,2,2,2,2,2, 2, 2, 2, 2) ar_ineql_some_n10 = c(1,2,3,5,8,13,21,34,55,89) ar_ineql_very_n10 = c(1,2^2,3^2,5^2,8^2,13^2,21^2,34^2,55^2,89^2) ar_ineql_extr_n10 = c(1,2^2,3^3,5^4,8^5,13^6,21^7,34^8,55^9,89^10) Now test the example arrays above using the function based no our formula: ## ## Small N=1 Hard-Code ## ar_equal_n1: 0 ## ar_ineql_n1: 0 ## ## Small N=2 Hard-Code, converge to 1/3, see formula above ## ar_ineql_alittle_n2: 0.1111111 ## ar_ineql_somewht_n2: 0.2592593 ## ar_ineql_alotine_n2: 0.3131313 ## ar_ineql_veryvry_n2: 0.3307393 ## ## Small N=10 Hard-Code, convege to 9/11=0.8181, see formula above ## ar_equal_n10: 0 ## ar_ineql_some_n10: 0.5395514 ## ar_ineql_very_n10: 0.7059554 ## ar_ineql_extr_n10: 0.8181549 8.3.2 Atkinson Family Utility Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 8.3.2.1 Individual Outcomes and Preference How does the Aktinson Family utility function work? THe Atkinson Family Utility has the following functional form. \\[ V^{\\text{social}} = \\left( \\alpha \\cdot A^{\\lambda} + \\beta \\cdot B^{\\lambda} \\right)^{\\frac{1}{\\lambda}} \\] Several key issues here: \\(V^{\\text{social}}\\) is the utility of some social planner \\(A\\) and \\(B\\) are allocations for Alex and Ben. \\(\\alpha\\) and \\(\\beta\\) are biases that a social planner has for Alex and Ben: \\(\\alpha+\\beta=1\\), \\(\\alpha&gt;0\\), and \\(\\beta&gt;0\\) \\(-\\infty &lt; \\lambda \\le 1\\) is a measure of inequality aversion \\(\\lambda=1\\) is when the planner cares about weighted total allocations (efficient, Utilitarian) \\(\\lambda=-\\infty\\) is when the planner cares about only the minimum between \\(A\\) and \\(B\\) allocations (equality, Rawlsian) What if only care about Alex? Clearly, if the planner only cares about Ben, \\(\\beta=1\\), then: \\[ V^{\\text{social}} = \\left( B^{\\lambda} \\right)^{\\frac{1}{\\lambda}} = B \\] Clearly, regardless of the value of \\(\\lambda\\), as \\(B\\) increases \\(V\\) increases. What Happens to V when A or B increases? What is the derivative of \\(V\\) with respect to \\(A\\) or \\(B\\)? \\[ \\frac{\\partial V}{\\partial A} = \\frac{1}{\\lambda} \\left( \\alpha A^{\\lambda} + \\beta B^{\\lambda} \\right)^{\\frac{1}{\\lambda}-1} \\cdot \\lambda \\alpha A^{\\lambda -1} \\] \\[ \\frac{\\partial V}{\\partial A} = \\left( \\alpha A^{\\lambda} + \\beta B^{\\lambda} \\right)^{\\frac{1-\\lambda}{\\lambda}} \\cdot \\alpha A^{\\lambda -1} &gt;0 \\] Note that \\(\\frac{\\partial V}{\\partial A}&gt;0\\). When \\(\\lambda &lt;0\\), \\(Z^{\\lambda}&gt;0\\). For example \\(10^{-2}=\\frac{1}{100}\\). And For example \\(0.1^{\\frac{3}{-2}}=\\frac{1}{0.1^{1.5}}\\). Still Positive. While the overall \\(V\\) increases with increasing \\(A\\), but if we did not have the outter power term, the situation is different. In particular, when \\(\\lambda &lt; 0\\): \\[ \\text{ if } \\lambda &lt;0 \\thinspace\\thinspace \\text{ then } \\thinspace\\thinspace \\frac{d \\left(\\alpha A^{\\lambda} + \\beta B^{\\lambda}\\right)}{dA}=\\alpha\\lambda A^{\\lambda -1}&lt;0 \\] Without the outter \\(\\frac{1}{\\lambda}\\) power, negative \\(\\lambda\\) would lead to decreasing weighted sum. But: \\[ \\text{ if } \\lambda &lt;0 \\thinspace\\thinspace \\text{ then } \\thinspace\\thinspace \\frac{dG^{\\frac{1}{\\lambda}}}{dG}=\\frac{1}{\\lambda}\\cdot G^{\\frac{1-\\lambda}{\\lambda}}&lt;0 \\] so when \\(G\\) is increasing and \\(\\lambda &lt;0\\), \\(V\\) would decrease. But when \\(G\\left(A,B\\right)\\) is decreasing, as is the case with increasing \\(A\\) when \\(\\lambda &lt;0\\), \\(V\\) will actually increase. This confirms that \\(\\frac{\\partial V}{\\partial A}&gt;0\\) for \\(\\lambda &lt;0\\). The result is symmetric for \\(\\lambda &gt;0\\). 8.3.2.2 Indifference Curve Graph Given \\(V^{\\ast}\\), we can show the combinations of \\(A\\) and \\(B\\) points that provide the same utility. We want to be able to potentially draw multiple indifference curves at the same time. Note that indifference curves are defined by \\(\\alpha\\), \\(\\lambda\\) only. Each indifference curve is a set of \\(A\\) and \\(B\\) coordinates. So to generate multiple indifference curves means to generate many sets of \\(A\\), \\(B\\) associated with different planner preferences, and then these could be graphed out. # A as x-axis, need bounds on A fl_A_min = 0.01 fl_A_max = 3 it_A_grid = 10000 # Define parameters # ar_lambda &lt;- 1 - (10^(c(seq(-2,2, length.out=3)))) ar_lambda &lt;- c(1, 0.6, 0.06, -6) ar_beta &lt;- seq(0.25, 0.75, length.out = 3) ar_beta &lt;- c(0.3, 0.5, 0.7) ar_v_star &lt;- seq(1, 2, length.out = 1) tb_pref &lt;- as_tibble(cbind(ar_lambda)) %&gt;% expand_grid(ar_beta) %&gt;% expand_grid(ar_v_star) %&gt;% rename_all(~c(&#39;lambda&#39;, &#39;beta&#39;, &#39;vstar&#39;)) %&gt;% rowid_to_column(var = &quot;indiff_id&quot;) # Generate indifference points with apply and anonymous function # tb_pref, whatever is selected from it, must be all numeric # if there are strings, would cause conversion error. ls_df_indiff &lt;- apply(tb_pref, 1, function(x){ indiff_id &lt;- x[1] lambda &lt;- x[2] beta &lt;- x[3] vstar &lt;- x[4] ar_fl_A_indiff &lt;- seq(fl_A_min, fl_A_max, length.out=it_A_grid) ar_fl_B_indiff &lt;- (((vstar^lambda) - (beta*ar_fl_A_indiff^(lambda)))/(1-beta))^(1/lambda) mt_A_B_indiff &lt;- cbind(indiff_id, lambda, beta, vstar, ar_fl_A_indiff, ar_fl_B_indiff) colnames(mt_A_B_indiff) &lt;- c(&#39;indiff_id&#39;, &#39;lambda&#39;, &#39;beta&#39;, &#39;vstar&#39;, &#39;indiff_A&#39;, &#39;indiff_B&#39;) tb_A_B_indiff &lt;- as_tibble(mt_A_B_indiff) %&gt;% rowid_to_column(var = &quot;A_grid_id&quot;) %&gt;% filter(indiff_B &gt;= 0 &amp; indiff_B &lt;= max(ar_fl_A_indiff)) return(tb_A_B_indiff) }) df_indiff &lt;- do.call(rbind, ls_df_indiff) %&gt;% drop_na() Note that many more A grid points are needed to fully plot out the leontief line. # Labeling st_title &lt;- paste0(&#39;Indifference Curves Aktinson Atkinson Utility (CES)&#39;) st_subtitle &lt;- paste0(&#39;Each Panel Different beta=A\\&#39;s Weight lambda=inequality aversion\\n&#39;, &#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/math/func_ineq/htmlpdfr/fs_atkinson_ces.html&#39;) st_caption &lt;- paste0(&#39;Indifference Curve 2 Individuals, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;A&#39; st_y_label &lt;- &#39;B&#39; # Graphing plt_indiff &lt;- df_indiff %&gt;% mutate(lambda = as_factor(lambda), beta = as_factor(beta), vstar = as_factor(vstar)) %&gt;% ggplot(aes(x=indiff_A, y=indiff_B, colour=lambda)) + facet_wrap( ~ beta) + geom_line(size=1) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_indiff) "],
["index-and-code-links.html", "A Index and Code Links A.1 Array, Matrix, Dataframe links A.2 Summarize Data links A.3 Functions links A.4 Panel links A.5 Linear Regression links A.6 Nonlinear Regression links A.7 Optimization links A.8 Mathmatics and Statistics links", " A Index and Code Links A.1 Array, Matrix, Dataframe links A.1.1 Section 1.1 List links Multi-dimensional Named Lists: rmd | r | pdf | html Initiate Empty List. Named one and two dimensional lists. r: vector(mode = “list”, length = it_N) + names(list) &lt;- paste0(‘e’,seq()) + dimnames(ls2d)[[1]] &lt;- paste0(‘r’,seq()) + dimnames(ls2d)[[2]] &lt;- paste0(‘c’,seq()) tidyr: unnest() A.1.2 Section 1.2 Array links Arrays Operations in R: rmd | r | pdf | html Basic array operations in R. r: head() + tail() + na_if() Generate Special Arrays: rmd | r | pdf | html Generate special arrays: log spaced array r: seq() String Operations: rmd | r | pdf | html Split, concatenate, subset strings r: paste0() + sub() + gsub() + grepl() + sprintf() + tail() + strsplit() + basename() + dirname() Meshgrid Matrices, Arrays and Scalars: rmd | r | pdf | html Meshgrid Matrices, Arrays and Scalars to form all combination dataframe. tidyr: expand_grid() + expand.grid() A.1.3 Section 1.3 Matrix links Matrix Basics: rmd | r | pdf | html Generate and combine fixed and random matrixes R: rbind() + matrix Linear Algebra Operations: rmd | r | pdf | html A.1.4 Section 1.4 Variables in Dataframes links Tibble Basics: rmd | r | pdf | html generate tibbles, rename tibble variables, tibble row and column names rename numeric sequential columns with string prefix and suffix dplyr: as_tibble(mt) + rename_all(~c(ar_names)) + rename_at(vars(starts_with(“xx”)), funs(str_replace(., “yy”, “yyyy”)) + rename_at(vars(num_range(’’,ar_it)), funs(paste0(st,.))) + rowid_to_column() + colnames + rownames Label and Combine Factor Variables: rmd | r | pdf | html Convert numeric variables to factor variables, generate joint factors, and label factors. Graph MPG and 1/4 Miles Time (qsec) from the mtcars dataset over joint shift-type (am) and engine-type (vs) categories. forcats: as_factor() + fct_recode() + fct_cross() Randomly Draw Subsets of Rows from Matrix: rmd | r | pdf | html Given matrix, randomly sample rows, or select if random value is below threshold. r: rnorm() + sample() + df[sample(dim(df)[1], it_M, replace=FALSE),] dplyr: case_when() + mutate(var = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(var == 1) Generate Variables Conditional on Other Variables: rmd | r | pdf | html Use case_when to generate elseif conditional variables: NA, approximate difference, etc. dplyr: case_when() + na_if() + mutate(var = na_if(case_when(rnorm(n())&lt; 0 ~ -99, TRUE ~ mpg), -99)) r: e-notation + all.equal() + isTRUE(all.equal(a,b,tol)) + is.na() + NA_real_ + NA_character_ + NA_integer_ R Tibble Dataframe String Manipulations: rmd | r | pdf | html A.2 Summarize Data links A.2.1 Section 2.1 Counting Observation links Counting Basics: rmd | r | pdf | html uncount to generate panel skeleton from years in survey dplyr: uncount(yr_n) + group_by() + mutate(yr = row_number() + start_yr) A.2.2 Section 2.2 Sorting, Indexing, Slicing links Sorted Index, Interval Index and Expand Value from One Row: rmd | r | pdf | html Sort and generate index for rows Generate negative and positive index based on deviations Populate Values from one row to other rows dplyr: arrange() + row_number() + mutate(lowest = min(Sepal.Length)) + case_when(row_number()==x ~ Septal.Length) + mutate(Sepal.New = Sepal.Length[Sepal.Index == 1]) A.2.3 Section 2.3 Group Statistics links Count Unique Groups and Mean within Groups: rmd | r | pdf | html Unique groups defined by multiple values and count obs within group. Mean, sd, observation count for non-NA within unique groups. dplyr: group_by() + summarise(n()) + summarise_if(is.numeric, funs(mean = mean(., na.rm = TRUE), n = sum(is.na(.)==0))) By Groups, One Variable All Statistics: rmd | r | pdf | html Pick stats, overall, and by multiple groups, stats as matrix or wide row with name=(ctsvar + catevar + catelabel). tidyr: group_by() + summarize_at(, funs()) + rename(!!var := !!sym(var)) + mutate(!!var := paste0(var,‘str’,!!!syms(vars))) + gather() + unite() + spread(varcates, value) By within Individual Groups Variables, Averages: rmd | r | pdf | html By Multiple within Individual Groups Variables. Averages for all numeric variables within all groups of all group variables. Long to Wide to very Wide. tidyr: *gather() + group_by() + summarise_if(is.numeric, funs(mean(., na.rm = TRUE))) + mutate(all_m_cate = paste0(variable, ’_c’, value)) + unite() + spread()* A.2.4 Section 2.4 Distributional Statistics links Tibble Basics: rmd | r | pdf | html input multiple variables with comma separated text strings quantitative/continuous and categorical/discrete variables histogram and summary statistics tibble: ar_one &lt;- c(107.72,101.28) + ar_two &lt;- c(101.72,101.28) + mt_data &lt;- cbind(ar_one, ar_two) + as_tibble(mt_data) A.2.5 Section 2.5 Summarize Multiple Variables links Apply the Same Function over Columns of Matrix: rmd | r | pdf | html Replace NA values in selected columns by alternative values. Cumulative sum over multiple variables. Rename various various with common prefix and suffix appended. r: cumsum() + gsub() + mutate_at(vars(contains(‘V’)), .funs = list(cumu = ~cumsum(.))) + rename_at(vars(contains(“V”) ), list(~gsub(“M”, \"\", .))) dplyr: rename_at() + mutate_at() + rename_at(vars(starts_with(“V”)), funs(str_replace(., “V”, “var”))) + mutate_at(vars(one_of(c(‘var1’, ‘var2’))), list(~replace_na(., 99))) A.3 Functions links A.3.1 Section 3.1 Dataframe Mutate links Nonlinear Function over Rows: rmd | r | pdf | html Evaluate nonlinear function f(x_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. dplyr: rowwise() + mutate(out = funct(inputs)) Evaluate Functions over Rows of Meshes Matrices: rmd | r | pdf | html Mesh states and choices together and rowwise evaluate many matrixes. Cumulative sum over multiple variables. Rename various various with common prefix and suffix appended. r: ffi &lt;- function(fl_A, ar_B) tidyr: expand_grid() + rowwise() + df %&gt;% rowwise() %&gt;% mutate(var = ffi(fl_A, ar_B)) ggplot2: geom_line() + facet_wrap() + geom_hline() + facet_wrap(. ~ var_id, scales = ‘free’) + geom_hline(yintercept=0, linetype=“dashed”, color=“red”, size=1) + A.3.2 Section 3.2 Dataframe Do Anything links Evaluate Function Do Anything Group Stack Results: rmd | r | pdf | html Group dataframe by categories, compute category specific output scalar or arrays based on within category variable information. dplyr: group_by(ID) + do(inc = rnorm(.\\(N, mean=.\\)mn, sd=.$sd)) + unnest(c(inc)) + left_join(df, by=“ID”) Expand Each Dataframe Row into More Rows: rmd | r | pdf | html Generate row value specific arrays of varying Length, and stack expanded dataframe. dplyr: do() + unnest() + left_join() + df %&gt;% group_by(ID) %&gt;% do(inc = rnorm(.\\(Q, mean=.\\)mean, sd=.$sd)) %&gt;% unnest(c(inc)) A.3.3 Section 3.3 Apply and pmap links Apply and Mutate over Rows: rmd | r | pdf | html Evaluate function f(x_i,y_i,c), where c is a constant and x and y vary over each row of a matrix, with index i indicating rows. Get same results using apply, sapply, and dplyr mutate. r: do.call() + apply(mt, 1, func) + sapply(ls_ar, func, ar1, ar2) purrr: rowwise() + unnest(out) + pmap(func) + unlist() A.4 Panel links A.4.1 Section 4.1 Generate and Join links TIDYVERSE Generate Panel Data Structures: rmd | r | pdf | html Build skeleton panel frame with N observations and T periods. tidyr: rowid_to_column() + uncount() + group_by() + row_number() + ungroup() R DPLYR Join Multiple Dataframes Together: rmd | r | pdf | html Join dataframes together with one or multiple keys. Stack dataframes together. dplyr: filter() + rename(!!sym(vsta) := !!sym(vstb)) + mutate(var = rnom(n())) + left_join(df, by=(c(‘id’=‘id’, ‘vt’=‘vt’))) + left_join(df, by=setNames(c(‘id’, ‘vt’), c(‘id’, ‘vt’))) + bind_rows() A.4.2 Section 4.2 Wide and Long links TIDYR Pivot Wider and Pivot Longer Examples: rmd | r | pdf | html Long roster to wide roster and cumulative sum attendance by date. dplyr: mutate(var = case_when(rnorm(n()) &lt; 0 ~ 1, TRUE ~ 0)) + rename_at(vars(num_range(’‘, ar_it)), list(~paste0(st_prefix, . ,’’))) + mutate_at(vars(contains(str)), list(~replace_na(., 0))) + mutate_at(vars(contains(str)), list(~cumsum(.))) A.5 Linear Regression links A.5.1 Section 5.1 OLS and IV links IV/OLS Regression: rmd | r | pdf | html R Instrumental Variables and Ordinary Least Square Regression store all Coefficients and Diagnostics as Dataframe Row. aer: library(aer) + ivreg(as.formula, diagnostics = TRUE) M Outcomes and N RHS Alternatives: rmd | r | pdf | html There are M outcome variables and N alternative explanatory variables. Regress all M outcome variables on N endogenous/independent right hand side variables one by one, with controls and/or IVs, collect coefficients. dplyr: bind_rows(lapply(listx, function(x)(bind_rows(lapply(listy, regf.iv))) + starts_with() + ends_with() + reduce(full_join) A.5.2 Section 5.2 Decomposition links Regression Decomposition: rmd | r | pdf | html Post multiple regressions, fraction of outcome variables’ variances explained by multiple subsets of right hand side variables. dplyr: gather() + group_by(var) + mutate_at(vars, funs(mean = mean(.))) + rowSums(matmat) + mutate_if(is.numeric, funs(frac = (./value_var)))* A.6 Nonlinear Regression links A.6.1 Section 6.1 Logit Regression links Logit Regression: rmd | r | pdf | html Logit regression testing and prediction. stats: glm(as.formula(), data, family=‘binomial’) + predict(rs, newdata, type = “response”) A.7 Optimization links A.7.1 Section 7.1 Bisection links Concurrent Bisection over Dataframe Rows: rmd | r | pdf | html Post multiple regressions, fraction of outcome variables’ variances explained by multiple subsets of right hand side variables. tidyr: *pivot_longer(cols = starts_with(‘abc’), names_to = c(‘a’, ‘b’), names_pattern = paste0(‘prefix’, \"(.)_(.)\"), values_to = val) + pivot_wider(names_from = !!sym(name), values_from = val) + mutate(!!sym(abc) := case_when(efg &lt; 0 ~ !!sym(opq), TRUE ~ iso))* gglot2: geom_line() + facet_wrap() + geom_hline() A.8 Mathmatics and Statistics links A.8.1 Section 8.1 Distributions links Integrate Normal Shocks: rmd | r | pdf | html Random Sampling (Monte Carlo) integrate shocks. Trapezoidal rule (symmetric rectangles) integrate normal shock. A.8.2 Section 8.2 Analytical Solutions links linear solve x with f(x) = 0: rmd | r | pdf | html Evaluate and solve statistically relevant problems with one equation and one unknown that permit analytical solutions. A.8.3 Section 8.3 Inequality Models links Gini for Discrete Samples: rmd | r | pdf | html Given sample of data points that are discrete, compute the approximate gini coefficient. r: sort() + cumsum() + sum() CES abd Atkinson Utility: rmd | r | pdf | html Analyze how changing individual outcomes shift utility given inequality preference parameters. Draw Cobb-Douglas, Utilitarian and Leontief indifference curve r: apply(mt, 1, funct(x){}) + do.call(rbind, ls_mt) tidyr: expand_grid() ggplot2: geom_line() + facet_wrap() README_appendix: rmd | r | pdf | html "]
]
