[["index.html", "Data Structures, Estimation and Optimization with R Preface", " Data Structures, Estimation and Optimization with R Fan Wang 2021-02-02 Preface This is a work-in-progress website consisting of R panel data and optimization examples for Statistics/Econometrics/Economic Analysis. Materials gathered from various projects in which R code is used. Files are from the R4Econ repository. This is not a R package, but a list of examples in PDF/HTML/Rmd formats. REconTools is a package that can be installed with tools used in projects involving R. Bullet points show which base R, tidyverse or other functions/commands are used to achieve various objectives. An effort is made to use only base R (R Core Team 2019) and tidyverse (Wickham 2019) packages whenever possible to reduce dependencies. The goal of this repository is to make it easier to find/re-use codes produced for various projects. Some functions also rely on or correspond to functions from REconTools (Wang 2020). From other repositories: For dynamic borrowing and savings problems, see MEconTools and Dynamic Asset Repository; For code examples, see also Matlab Example Code, Stata Example Code, Python Example Code; For intro econ with Matlab, see Intro Mathematics for Economists, and for intro stat with R, see Intro Statistics for Undergraduates. See here for all of Fans public repositories. The site is built using Bookdown (Xie 2020). Please contact FanWangEcon for issues or problems. "],["array-matrix-dataframe.html", "Chapter 1 Array, Matrix, Dataframe 1.1 List 1.2 Array 1.3 Matrix 1.4 Variables in Dataframes", " Chapter 1 Array, Matrix, Dataframe 1.1 List 1.1.1 Lists Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). r list tutorial r vector vs list r initialize empty multiple element list r name rows and columns of 2 dimensional list r row and colum names of list list dimnames r named list to string 1.1.1.1 Iteratively Build Up a List of Strings Build up a list of strings, where the strings share common components. Iteratre over lists to generate variations in elements of the string list. # common string components st_base_name &lt;- &#39;snwx_v_planner_docdense&#39; st_base_middle &lt;- &#39;b1_xi0_manna_88&#39; # numeric values to loop over ar_st_beta_val &lt;- c(&#39;bt60&#39;, &#39;bt70&#39;, &#39;bt80&#39;, &#39;bt90&#39;) ar_st_edu_type &lt;- c(&#39;e1lm2&#39;, &#39;e2hm2&#39;) # initialize string list ls_snm &lt;- vector(mode = &quot;list&quot;, length = length(ar_st_beta_val)*length(ar_st_edu_type)) # generate list it_ctr = 0 for (st_beta_val in ar_st_beta_val) { for (st_edu_type in ar_st_edu_type) { it_ctr = it_ctr + 1 # snm_file_name &lt;- &#39;snwx_v_planner_docdense_e2hm2_b1_xi0_manna_88_bt90&#39; snm_file_name &lt;- paste(st_base_name, st_edu_type, st_base_middle, st_beta_val, sep =&#39;_&#39;) ls_snm[it_ctr] &lt;- snm_file_name } } # print for (snm in ls_snm) { print(snm) } ## [1] &quot;snwx_v_planner_docdense_e1lm2_b1_xi0_manna_88_bt60&quot; ## [1] &quot;snwx_v_planner_docdense_e2hm2_b1_xi0_manna_88_bt60&quot; ## [1] &quot;snwx_v_planner_docdense_e1lm2_b1_xi0_manna_88_bt70&quot; ## [1] &quot;snwx_v_planner_docdense_e2hm2_b1_xi0_manna_88_bt70&quot; ## [1] &quot;snwx_v_planner_docdense_e1lm2_b1_xi0_manna_88_bt80&quot; ## [1] &quot;snwx_v_planner_docdense_e2hm2_b1_xi0_manna_88_bt80&quot; ## [1] &quot;snwx_v_planner_docdense_e1lm2_b1_xi0_manna_88_bt90&quot; ## [1] &quot;snwx_v_planner_docdense_e2hm2_b1_xi0_manna_88_bt90&quot; # if string in string grepl(&#39;snwx_v_planner&#39;, snm) ## [1] TRUE 1.1.1.2 Named List of Matrixes Save a list of matrixes. Retrieve Element of that list via loop. # Define an array to loop over ar_fl_mean &lt;- c(10, 20, 30) # store restuls in named list ls_mt_res = vector(mode = &quot;list&quot;, length = length(ar_fl_mean)) ar_st_names &lt;- paste0(&#39;mean&#39;, ar_fl_mean) names(ls_mt_res) &lt;- ar_st_names # Loop and generat a list of dataframes for (it_fl_mean in seq(1, length(ar_fl_mean))) { fl_mean = ar_fl_mean[it_fl_mean] # dataframe set.seed(it_fl_mean) tb_combine &lt;- as_tibble( matrix(rnorm(4,mean=fl_mean,sd=1), nrow=2, ncol=3) ) %&gt;% rowid_to_column(var = &quot;id&quot;) %&gt;% rename_all(~c(c(&#39;id&#39;,&#39;var1&#39;,&#39;varb&#39;,&#39;vartheta&#39;))) ls_mt_res[[it_fl_mean]] = tb_combine } # Retrieve elements print(ls_mt_res[[1]]) print(ls_mt_res$mean10) print(ls_mt_res[[&#39;mean10&#39;]]) # Print via Loop for (it_fl_mean in seq(1, length(ar_fl_mean))) { tb_combine = ls_mt_res[[it_fl_mean]] print(tb_combine) } 1.1.1.3 One Dimensional Named List define list slice list print r named list as a single line string R Unlist named list into one string with preserving list names # Define Lists ls_num &lt;- list(1,2,3) ls_str &lt;- list(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;) ls_num_str &lt;- list(1,2,&#39;3&#39;) # Named Lists ar_st_names &lt;- c(&#39;e1&#39;,&#39;e2&#39;,&#39;e3&#39;) ls_num_str_named &lt;- ls_num_str names(ls_num_str_named) &lt;- ar_st_names # Add Element to Named List ls_num_str_named$e4 &lt;- &#39;this is added&#39; Initiate an empty list and add to it # Initiate List ls_abc &lt;- vector(mode = &quot;list&quot;, length = 0) # Add Named Elements to List Sequentially ls_abc$a = 1 ls_abc$b = 2 ls_abc$c = &#39;abc\\&#39;s third element&#39; # Get all Names Added to List ar_st_list_names &lt;- names(ls_abc) # Print list in a loop print(ls_abc) ## $a ## [1] 1 ## ## $b ## [1] 2 ## ## $c ## [1] &quot;abc&#39;s third element&quot; for (it_list_ele_ctr in seq(1,length(ar_st_list_names))) { st_list_ele_name &lt;- ar_st_list_names[it_list_ele_ctr] st_list_ele_val &lt;- ls_abc[it_list_ele_ctr] print(paste0(st_list_ele_name,&#39;=&#39;,st_list_ele_val)) } ## [1] &quot;a=1&quot; ## [1] &quot;b=2&quot; ## [1] &quot;c=abc&#39;s third element&quot; 1.1.1.4 Named List Print Function r print input as string r print parameter code as string How to convert variable (object) name into String The function below ffi_lst2str is also a function in REconTools: ff_sup_lst2str. # list to String printing function ffi_lst2str &lt;- function(ls_list, st_desc, bl_print=TRUE) { # string desc if(missing(st_desc)){ st_desc &lt;- deparse(substitute(ls_list)) } # create string st_string_from_list = paste0(paste0(st_desc, &#39;:&#39;), paste(names(ls_list), ls_list, sep=&quot;=&quot;, collapse=&quot;;&quot; )) if (bl_print){ print(st_string_from_list) } } # print full ffi_lst2str(ls_num) ## [1] &quot;ls_num:=1;=2;=3&quot; ffi_lst2str(ls_str) ## [1] &quot;ls_str:=1;=2;=3&quot; ffi_lst2str(ls_num_str) ## [1] &quot;ls_num_str:=1;=2;=3&quot; ffi_lst2str(ls_num_str_named) ## [1] &quot;ls_num_str_named:e1=1;e2=2;e3=3;e4=this is added&quot; # print subset ffi_lst2str(ls_num[2:3]) ## [1] &quot;ls_num[2:3]:=2;=3&quot; ffi_lst2str(ls_str[2:3]) ## [1] &quot;ls_str[2:3]:=2;=3&quot; ffi_lst2str(ls_num_str[2:4]) ## [1] &quot;ls_num_str[2:4]:=2;=3;=NULL&quot; ffi_lst2str(ls_num_str_named[c(&#39;e2&#39;,&#39;e3&#39;,&#39;e4&#39;)]) ## [1] &quot;ls_num_str_named[c(\\&quot;e2\\&quot;, \\&quot;e3\\&quot;, \\&quot;e4\\&quot;)]:e2=2;e3=3;e4=this is added&quot; 1.1.1.5 Two Dimensional Unnamed List Generate a multiple dimensional list: Initiate with an N element empty list Reshape list to M by Q Fill list elements Get list element by row and column number List allows for different data types to be stored together. Note that element specific names in named list are not preserved when the list is reshaped to be two dimensional. Two dimensional list, however, could have row and column names. # Dimensions it_M &lt;- 2 it_Q &lt;- 3 it_N &lt;- it_M*it_Q # Initiate an Empty MxQ=N element list ls_2d_flat &lt;- vector(mode = &quot;list&quot;, length = it_N) ls_2d &lt;- ls_2d_flat # Named flat ls_2d_flat_named &lt;- ls_2d_flat names(ls_2d_flat_named) &lt;- paste0(&#39;e&#39;,seq(1,it_N)) ls_2d_named &lt;- ls_2d_flat_named # Reshape dim(ls_2d) &lt;- c(it_M, it_Q) # named 2d list can not carry 1d name after reshape dim(ls_2d_named) &lt;- c(it_M, it_Q) Print Various objects generated above, print list flattened. # display ffi_lst2str(ls_2d_flat_named) ## [1] &quot;ls_2d_flat_named:e1=NULL;e2=NULL;e3=NULL;e4=NULL;e5=NULL;e6=NULL&quot; # print(ls_2d_flat_named) ffi_lst2str(ls_2d_named) ## [1] &quot;ls_2d_named:=NULL;=NULL;=NULL;=NULL;=NULL;=NULL&quot; print(ls_2d_named) ## [,1] [,2] [,3] ## [1,] NULL NULL NULL ## [2,] NULL NULL NULL Select element from list: # Select Values, double bracket to select from 2dim list print(&#39;ls_2d[[1,2]]&#39;) ## [1] &quot;ls_2d[[1,2]]&quot; print(ls_2d[[1,2]]) ## NULL 1.1.1.6 Define Two Dimensional Named LIst For naming two dimensional lists, rowname and colname does not work. Rather, we need to use dimnames. Note that in addition to dimnames, we can continue to have element specific names. Both can co-exist. But note that the element specific names are not preserved after dimension transform, so need to be redefined afterwards. How to select an element of a two dimensional list: row and column names: dimnames, ls_2d_flat_named[[row2,col2]] named elements: names, ls_2d_flat_named[[e5]] select by index: index, ls_2d_flat_named[[5]] converted two dimensional named list to tibble/matrix Neither dimnames nor names are required, but both can be used to select elements. # Dimensions it_M &lt;- 3 it_Q &lt;- 4 it_N &lt;- it_M*it_Q # Initiate an Empty MxQ=N element list ls_2d_flat_named &lt;- vector(mode = &quot;list&quot;, length = it_N) dim(ls_2d_flat_named) &lt;- c(it_M, it_Q) # Fill with values for (it_Q_ctr in seq(1,it_Q)) { for (it_M_ctr in seq(1,it_M)) { # linear index ls_2d_flat_named[[it_M_ctr, it_Q_ctr]] &lt;- (it_Q_ctr-1)*it_M+it_M_ctr } } # Replace row names, note rownames does not work dimnames(ls_2d_flat_named)[[1]] &lt;- paste0(&#39;row&#39;,seq(1,it_M)) dimnames(ls_2d_flat_named)[[2]] &lt;- paste0(&#39;col&#39;,seq(1,it_Q)) # Element Specific Names names(ls_2d_flat_named) &lt;- paste0(&#39;e&#39;,seq(1,it_N)) # Convert to Matrix tb_2d_flat_named &lt;- as_tibble(ls_2d_flat_named) %&gt;% unnest() mt_2d_flat_named &lt;- as.matrix(tb_2d_flat_named) Print various objects generated above: # These are not element names, can still name each element # display print(&#39;ls_2d_flat_named&#39;) ## [1] &quot;ls_2d_flat_named&quot; print(ls_2d_flat_named) ## col1 col2 col3 col4 ## row1 1 4 7 10 ## row2 2 5 8 11 ## row3 3 6 9 12 ## attr(,&quot;names&quot;) ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; &quot;e6&quot; &quot;e7&quot; &quot;e8&quot; &quot;e9&quot; &quot;e10&quot; &quot;e11&quot; &quot;e12&quot; print(&#39;tb_2d_flat_named&#39;) ## [1] &quot;tb_2d_flat_named&quot; print(tb_2d_flat_named) print(&#39;mt_2d_flat_named&#39;) ## [1] &quot;mt_2d_flat_named&quot; print(mt_2d_flat_named) ## col1 col2 col3 col4 ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Select elements from list: # Select elements with with dimnames ffi_lst2str(ls_2d_flat_named[[&#39;row2&#39;,&#39;col2&#39;]]) ## [1] &quot;ls_2d_flat_named[[\\&quot;row2\\&quot;, \\&quot;col2\\&quot;]]:=5&quot; # Select elements with element names ffi_lst2str(ls_2d_flat_named[[&#39;e5&#39;]]) ## [1] &quot;ls_2d_flat_named[[\\&quot;e5\\&quot;]]:=5&quot; # Select elements with index ffi_lst2str(ls_2d_flat_named[[5]]) ## [1] &quot;ls_2d_flat_named[[5]]:=5&quot; 1.2 Array 1.2.1 Array Basics Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.2.1.1 Multidimesional Arrays 1.2.1.1.1 Repeat one Number by the Size of an Array ar_a &lt;- c(1,2,3) ar_b &lt;- c(1,2,3/1,2,3) rep(0, length(ar_a)) ## [1] 0 0 0 1.2.1.1.2 Generate 2 Dimensional Array # Multidimensional Array # 1 is r1c1t1, 1.5 in r2c1t1, 0 in r1c2t1, etc. # Three dimensions, row first, column second, and tensor third x &lt;- array(c(1, 1.5, 0, 2, 0, 4, 0, 3), dim=c(2, 2, 2)) dim(x) ## [1] 2 2 2 print(x) ## , , 1 ## ## [,1] [,2] ## [1,] 1.0 0 ## [2,] 1.5 2 ## ## , , 2 ## ## [,1] [,2] ## [1,] 0 0 ## [2,] 4 3 1.2.1.2 Array Slicing 1.2.1.2.1 Get a Subset of Array Elements, N Cuts from M Points There is an array with M elements, get N elements from the M elements. First cut including the starting and ending points. it_M &lt;- 5 it_N &lt;- 4 ar_all_elements = seq(1,10,10) 1.2.1.2.2 Remove Elements of Array Select elements with direct indexing, or with head and tail functions. Get the first two elements of three elements array. # Remove last element of array vars.group.bydf &lt;- c(&#39;23&#39;,&#39;dfa&#39;, &#39;wer&#39;) vars.group.bydf[-length(vars.group.bydf)] ## [1] &quot;23&quot; &quot;dfa&quot; # Use the head function to remove last element head(vars.group.bydf, -1) ## [1] &quot;23&quot; &quot;dfa&quot; head(vars.group.bydf, 2) ## [1] &quot;23&quot; &quot;dfa&quot; Get last two elements of array. # Remove first element of array vars.group.bydf &lt;- c(&#39;23&#39;,&#39;dfa&#39;, &#39;wer&#39;) vars.group.bydf[2:length(vars.group.bydf)] ## [1] &quot;dfa&quot; &quot;wer&quot; # Use Tail function tail(vars.group.bydf, -1) ## [1] &quot;dfa&quot; &quot;wer&quot; tail(vars.group.bydf, 2) ## [1] &quot;dfa&quot; &quot;wer&quot; Select all except for the first and the last element of an array. # define array ar_amin &lt;- c(0, 0.25, 0.50, 0.75, 1) # select without head and tail tail(head(ar_amin, -1), -1) ## [1] 0.25 0.50 0.75 Select the first and the last element of an array. The extreme values. # define array ar_amin &lt;- c(0, 0.25, 0.50, 0.75, 1) # select head and tail c(head(ar_amin, 1), tail(ar_amin, 1)) ## [1] 0 1 1.2.1.3 NA in Array 1.2.1.3.1 Check if NA is in Array # Convert Inf and -Inf to NA x &lt;- c(1, -1, Inf, 10, -Inf) na_if(na_if(x, -Inf), Inf) ## [1] 1 -1 NA 10 NA 1.2.1.4 Complex Number Handling numbers with real and imaginary components. Two separate issues, given an array of numbers that includes real as well as imaginary numbers, keep subset that only has real components. Additionally, for the same array, generate an equal length version of the array that includes the real components of all numbers. Define complex numbers. # Define a complex number cx_number_a &lt;- 0+0.0460246857561777i # Define another complex number cx_number_b &lt;- complex(real = 0.02560982, imaginary = 0.0460246857561777) # An array of numbers some of which are complex ar_cx_number &lt;- c(0.02560982+0.000000000i, 0.00000000+0.044895305i, 0.00000000+0.009153429i, 0.05462045+0.000000000i, 0.00000000+0.001198538i, 0.00000000+0.019267050i) Extract real components from a complex array. # equi-length real component ar_fl_number_re &lt;- Re(ar_cx_number) print(ar_fl_number_re) ## [1] 0.02560982 0.00000000 0.00000000 0.05462045 0.00000000 0.00000000 # equi-length img component ar_fl_number_im &lt;- Im(ar_cx_number) print(ar_fl_number_im) ## [1] 0.000000000 0.044895305 0.009153429 0.000000000 0.001198538 0.019267050 Keep only real elements of array. # subset of array that is real ar_fl_number_re_subset &lt;- Re(ar_cx_number[Re(ar_cx_number)!=0]) print(ar_fl_number_re_subset) ## [1] 0.02560982 0.05462045 1.2.1.5 Number Formatting 1.2.1.5.1 e notation Case one: 1.149946e+00 this is approximately: 1.14995 Case two: 9.048038e-01 this is approximately: 0.90480 Case three: 9.048038e-01 this is approximately: 0.90480 1.2.2 Generate Arrays Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.2.2.1 Generate Special Arrays 1.2.2.1.1 Log Space Arrays Often need to generate arrays on log rather than linear scale, below is log 10 scaled grid. # Parameters it.lower.bd.inc.cnt &lt;- 3 fl.log.lower &lt;- -10 fl.log.higher &lt;- -9 fl.min.rescale &lt;- 0.01 it.log.count &lt;- 4 # Generate ar.fl.log.rescaled &lt;- exp(log(10)*seq(log10(fl.min.rescale), log10(fl.min.rescale + (fl.log.higher-fl.log.lower)), length.out=it.log.count)) ar.fl.log &lt;- ar.fl.log.rescaled + fl.log.lower - fl.min.rescale # Print ar.fl.log ## [1] -10.000000 -9.963430 -9.793123 -9.000000 1.2.3 String Arrays Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.2.3.1 String Replace r string wildcard replace between regex R - replace part of a string using wildcards # String replacement gsub(x = paste0(unique(df.slds.stats.perc$it.inner.counter), &#39;:&#39;, unique(df.slds.stats.perc$z_n_a_n), collapse = &#39;;&#39;), pattern = &quot;\\n&quot;, replacement = &quot;&quot;) gsub(x = var, pattern = &quot;\\n&quot;, replacement = &quot;&quot;) gsub(x = var.input, pattern = &quot;\\\\.&quot;, replacement = &quot;_&quot;) String replaces a segment, search by wildcard. Given the string below, delete all text between carriage return and pound sign: st_tex_text &lt;- &quot;\\n% Lat2ex Comments\\n\\\\newcommand{\\\\exa}{\\\\text{from external file: } \\\\alpha + \\\\beta}\\n% More LaLatex Comments\\n&quot; st_clean_a1 &lt;- gsub(&quot;\\\\%.*?\\\\\\n&quot;, &quot;&quot;, st_tex_text) st_clean_a2 &lt;- gsub(&quot;L.*?x&quot;, &quot;[LATEX]&quot;, st_tex_text) print(paste0(&#39;st_tex_text:&#39;, st_tex_text)) ## [1] &quot;st_tex_text:\\n% Lat2ex Comments\\n\\\\newcommand{\\\\exa}{\\\\text{from external file: } \\\\alpha + \\\\beta}\\n% More LaLatex Comments\\n&quot; print(paste0(&#39;st_clean_a1:&#39;, st_clean_a1)) ## [1] &quot;st_clean_a1:\\n\\\\newcommand{\\\\exa}{\\\\text{from external file: } \\\\alpha + \\\\beta}\\n&quot; print(paste0(&#39;st_clean_a2:&#39;, st_clean_a2)) ## [1] &quot;st_clean_a2:\\n% [LATEX] Comments\\n\\\\newcommand{\\\\exa}{\\\\text{from external file: } \\\\alpha + \\\\beta}\\n% More [LATEX] Comments\\n&quot; String delete after a particular string: st_tex_text &lt;- &quot;\\\\end{equation}\\n}\\n% Even more comments from Latex preamble&quot; st_clean_a1 &lt;- gsub(&quot;\\\\\\n%.*&quot;,&quot;&quot;, st_tex_text) print(paste0(&#39;st_tex_text:&#39;, st_tex_text)) ## [1] &quot;st_tex_text:\\\\end{equation}\\n}\\n% Even more comments from Latex preamble&quot; print(paste0(&#39;st_clean_a1:&#39;, st_clean_a1)) ## [1] &quot;st_clean_a1:\\\\end{equation}\\n}&quot; 1.2.3.1.1 Search If and Which String Contains r if string contains r if string contains either or grepl Use grepl to search either of multiple substrings in a text Search for a single substring in a single string: st_example_a &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; st_example_b &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/_main.html&#39; grepl(&#39;_main&#39;, st_example_a) ## [1] FALSE grepl(&#39;_main&#39;, st_example_b) ## [1] TRUE Search for if one of a set of substring exists in a set of strings. In particular which one of the elements of ls_spn contains at least one of the elements of ls_str_if_contains. In the example below, only the first path does not contain either the word aggregate or index in the path. This can be used after all paths have been found recursively in some folder to select only desired paths from the full set of possibilities: ls_spn &lt;- c(&quot;C:/Users/fan/R4Econ//panel/basic/fs_genpanel.Rmd&quot;, &quot;C:/Users/fan/R4Econ//summarize/aggregate/main.Rmd&quot;, &quot;C:/Users/fan/R4Econ//summarize/index/fs_index_populate.Rmd&quot;) ls_str_if_contains &lt;- c(&quot;aggregate&quot;, &quot;index&quot;) str_if_contains &lt;- paste(ls_str_if_contains, collapse = &quot;|&quot;) grepl(str_if_contains, ls_spn) ## [1] FALSE TRUE TRUE 1.2.3.2 String Split Given some string, generated for example by cut, get the lower cut starting points, and also the higher end point # Extract 0.216 and 0.500 as lower and upper bounds st_cut_cate &lt;- &#39;(0.216,0.500]&#39; # Extract Lower Part substring(strsplit(st_cut_cate, &quot;,&quot;)[[1]][1], 2) ## [1] &quot;0.216&quot; # Extract second part except final bracket Option 1 intToUtf8(rev(utf8ToInt(substring(intToUtf8(rev(utf8ToInt(strsplit(st_cut_cate, &quot;,&quot;)[[1]][2]))), 2)))) ## [1] &quot;0.500&quot; # Extract second part except final bracket Option 2 gsub(strsplit(st_cut_cate, &quot;,&quot;)[[1]][2], pattern = &quot;]&quot;, replacement = &quot;&quot;) ## [1] &quot;0.500&quot; 1.2.3.3 String Concatenate # Simple Collapse vars.group.by &lt;- c(&#39;abc&#39;, &#39;efg&#39;) paste0(vars.group.by, collapse=&#39;|&#39;) ## [1] &quot;abc|efg&quot; 1.2.3.4 String Add Leading Zero # Add Leading zero for integer values to allow for sorting when # integers are combined into strings it_z_n &lt;- 1 it_a_n &lt;- 192 print(sprintf(&quot;%02d&quot;, it_z_n)) ## [1] &quot;01&quot; print(sprintf(&quot;%04d&quot;, it_a_n)) ## [1] &quot;0192&quot; 1.2.3.5 Substring Components Given a string, with certain structure, get components. r time string get month and year and day snm_full &lt;- &quot;20100701&quot; snm_year &lt;-substr(snm_full,0,4) snm_month &lt;-substr(snm_full,5,6) snm_day &lt;-substr(snm_full,7,8) print(paste0(&#39;full:&#39;, snm_full, &#39;, year:&#39;, snm_year, &#39;, month:&#39;, snm_month, &#39;, day:&#39;, snm_day)) ## [1] &quot;full:20100701, year:2010, month:07, day:01&quot; 1.2.4 Mesh Matrices, Arrays and Scalars Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). r expand.grid meshed array to matrix r meshgrid r array to matrix r reshape array to matrix dplyr permuations rows of matrix and element of array tidyr expand_grid mesh matrix and vector 1.2.4.1 Mesh Two or More Vectors with expand_grid In the example below, we have a matrix that is 2 by 2 (endogenous states), a vector that is 3 by 1 (choices), and another matrix that is 4 by 3 (exogenous states shocks). We want to generate a tibble dataset that meshes the matrix and the vector, so that all combinations show up. Additionally, we want to add some additional values that are common across all rows to the meshed dataframe. Note expand_grid is a from tidyr 1.0.0. # A. Generate the 5 by 2 Matrix (ENDO STATES) # it_child_count = N, the number of children it_N_child_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) fl_rho = 0.1 fl_lambda = 1.1 mt_nP_A_alpha = cbind(ar_nN_A, ar_nN_alpha, fl_rho, fl_lambda) ar_st_varnames &lt;- c(&#39;s_A&#39;, &#39;s_alpha&#39;, &#39;p_rho&#39;, &#39;p_lambda&#39;) tb_states_endo &lt;- as_tibble(mt_nP_A_alpha) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% rowid_to_column(var = &quot;state_id&quot;) # B. Choice Grid it_N_choice_cnt = 3 fl_max = 10 fl_min = 0 ar_nN_d = seq(fl_min, fl_max, length.out = it_N_choice_cnt) ar_st_varnames &lt;- c(&#39;c_food&#39;) tb_choices &lt;- as_tibble(ar_nN_d) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% rowid_to_column(var = &quot;choice_id&quot;) # C. Shock Grid set.seed(123) it_N_shock_cnt = 4 ar_nQ_shocks = exp(rnorm(it_N_shock_cnt, mean=0, sd=1)) ar_st_varnames &lt;- c(&#39;s_eps&#39;) tb_states_exo &lt;- as_tibble(ar_nQ_shocks) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% rowid_to_column(var = &quot;shock_id&quot;) # dataframe expand with other non expanded variables ar_st_varnames &lt;- tb_states_shk_choices &lt;- tb_states_endo %&gt;% expand_grid(tb_choices) %&gt;% expand_grid(tb_states_exo) %&gt;% select(state_id, choice_id, shock_id, s_A, s_alpha, s_eps, c_food, p_rho, p_lambda) # display kable(tb_states_shk_choices) %&gt;% kable_styling_fc() state_id choice_id shock_id s_A s_alpha s_eps c_food p_rho p_lambda 1 1 1 -2 0.1 0.5709374 0 0.1 1.1 1 1 2 -2 0.1 0.7943926 0 0.1 1.1 1 1 3 -2 0.1 4.7526783 0 0.1 1.1 1 1 4 -2 0.1 1.0730536 0 0.1 1.1 1 2 1 -2 0.1 0.5709374 5 0.1 1.1 1 2 2 -2 0.1 0.7943926 5 0.1 1.1 1 2 3 -2 0.1 4.7526783 5 0.1 1.1 1 2 4 -2 0.1 1.0730536 5 0.1 1.1 1 3 1 -2 0.1 0.5709374 10 0.1 1.1 1 3 2 -2 0.1 0.7943926 10 0.1 1.1 1 3 3 -2 0.1 4.7526783 10 0.1 1.1 1 3 4 -2 0.1 1.0730536 10 0.1 1.1 2 1 1 2 0.9 0.5709374 0 0.1 1.1 2 1 2 2 0.9 0.7943926 0 0.1 1.1 2 1 3 2 0.9 4.7526783 0 0.1 1.1 2 1 4 2 0.9 1.0730536 0 0.1 1.1 2 2 1 2 0.9 0.5709374 5 0.1 1.1 2 2 2 2 0.9 0.7943926 5 0.1 1.1 2 2 3 2 0.9 4.7526783 5 0.1 1.1 2 2 4 2 0.9 1.0730536 5 0.1 1.1 2 3 1 2 0.9 0.5709374 10 0.1 1.1 2 3 2 2 0.9 0.7943926 10 0.1 1.1 2 3 3 2 0.9 4.7526783 10 0.1 1.1 2 3 4 2 0.9 1.0730536 10 0.1 1.1 Using expand_grid directly over arrays # expand grid with dplyr expand_grid(x = 1:3, y = 1:2, z = -3:-1) 1.2.4.2 Mesh Arrays with expand.grid Given two arrays, mesh the two arrays together. # use expand.grid to generate all combinations of two arrays it_ar_A = 5 it_ar_alpha = 10 ar_A = seq(-2, 2, length.out=it_ar_A) ar_alpha = seq(0.1, 0.9, length.out=it_ar_alpha) mt_A_alpha = expand.grid(A = ar_A, alpha = ar_alpha) mt_A_meshed = mt_A_alpha[,1] dim(mt_A_meshed) = c(it_ar_A, it_ar_alpha) mt_alpha_meshed = mt_A_alpha[,2] dim(mt_alpha_meshed) = c(it_ar_A, it_ar_alpha) # display kable(mt_A_meshed) %&gt;% kable_styling_fc() -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 kable(mt_alpha_meshed) %&gt;% kable_styling_fc_wide() 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 Two Identical Arrays, individual attributes, each column is an individual for a matrix, and each row is also an individual. # use expand.grid to generate all combinations of two arrays it_ar_A = 5 ar_A = seq(-2, 2, length.out=it_ar_A) mt_A_A = expand.grid(Arow = ar_A, Arow = ar_A) mt_Arow = mt_A_A[,1] dim(mt_Arow) = c(it_ar_A, it_ar_A) mt_Acol = mt_A_A[,2] dim(mt_Acol) = c(it_ar_A, it_ar_A) # display kable(mt_Arow) %&gt;% kable_styling_fc() -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 kable(mt_Acol) %&gt;% kable_styling_fc() -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 1.3 Matrix 1.3.1 Generate Matrixes Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.3.1.1 Create a N by 2 Matrix from 3 arrays Names of each array become row names automatically. ar_row_one &lt;- c(-1,+1) ar_row_two &lt;- c(-3,-2) ar_row_three &lt;- c(0.35,0.75) mt_n_by_2 &lt;- rbind(ar_row_one, ar_row_two, ar_row_three) kable(mt_n_by_2) %&gt;% kable_styling_fc() ar_row_one -1.00 1.00 ar_row_two -3.00 -2.00 ar_row_three 0.35 0.75 1.3.1.2 Name Matrix Columns and Rows # An empty matrix with Logical NA mt_named &lt;- matrix(data=NA, nrow=2, ncol=2) colnames(mt_named) &lt;- paste0(&#39;c&#39;, seq(1,2)) rownames(mt_named) &lt;- paste0(&#39;r&#39;, seq(1,2)) mt_named ## c1 c2 ## r1 NA NA ## r2 NA NA 1.3.1.3 Generate NA Matrix Best way to allocate matrix in R, NULL vs NA? Allocate with NA or NA_real_ or NA_int_. Clarity in type definition is preferred. # An empty matrix with Logical NA mt_na &lt;- matrix(data=NA, nrow=2, ncol=2) str(mt_na) ## logi [1:2, 1:2] NA NA NA NA # An empty matrix with numerica NA mt_fl_na &lt;- matrix(data=NA_real_, nrow=2, ncol=2) mt_it_na &lt;- matrix(data=NA_integer_, nrow=2, ncol=2) str(mt_fl_na) ## num [1:2, 1:2] NA NA NA NA str(mt_fl_na) ## num [1:2, 1:2] NA NA NA NA 1.3.1.4 Generate Random Matrixes Random draw from the normal distribution, random draw from the uniform distribution, and combine resulting matrixes. # Generate 15 random normal, put in 5 rows, and 3 columns mt_rnorm &lt;- matrix(rnorm(15,mean=0,sd=1), nrow=5, ncol=3) # Generate 15 random normal, put in 5 rows, and 3 columns mt_runif &lt;- matrix(runif(15,min=0,max=1), nrow=5, ncol=5) # Combine mt_rnorm_runif &lt;- cbind(mt_rnorm, mt_runif) # Display kable(mt_rnorm_runif) %&gt;% kable_styling_fc_wide() 0.1292877 -0.4456620 -0.5558411 0.3181810 0.3688455 0.2659726 0.3181810 0.3688455 1.7150650 1.2240818 1.7869131 0.2316258 0.1524447 0.8578277 0.2316258 0.1524447 0.4609162 0.3598138 0.4978505 0.1428000 0.1388061 0.0458312 0.1428000 0.1388061 -1.2650612 0.4007715 -1.9666172 0.4145463 0.2330341 0.4422001 0.4145463 0.2330341 -0.6868529 0.1106827 0.7013559 0.4137243 0.4659625 0.7989248 0.4137243 0.4659625 1.3.1.5 Add Column to Matrix with Common Scalar Value Given some matrix of information, add a column, where all rows of the column have the same numerical value. Use the matrix created prior. - R add column to matrix - r append column to matrix constant value fl_new_first_col_val &lt;- 111 fl_new_last_col_val &lt;- 999 mt_with_more_columns &lt;- cbind(rep(fl_new_first_col_val, dim(mt_rnorm_runif)[1]), mt_rnorm_runif, rep(fl_new_last_col_val, dim(mt_rnorm_runif)[1])) # Display kable(mt_with_more_columns) %&gt;% kable_styling_fc_wide() 111 0.1292877 -0.4456620 -0.5558411 0.3181810 0.3688455 0.2659726 0.3181810 0.3688455 999 111 1.7150650 1.2240818 1.7869131 0.2316258 0.1524447 0.8578277 0.2316258 0.1524447 999 111 0.4609162 0.3598138 0.4978505 0.1428000 0.1388061 0.0458312 0.1428000 0.1388061 999 111 -1.2650612 0.4007715 -1.9666172 0.4145463 0.2330341 0.4422001 0.4145463 0.2330341 999 111 -0.6868529 0.1106827 0.7013559 0.4137243 0.4659625 0.7989248 0.4137243 0.4659625 999 1.3.2 Linear Algebra Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.3.2.1 Matrix Multiplication Multiply Together a 3 by 2 matrix and a 2 by 1 vector ar_row_one &lt;- c(-1,+1) ar_row_two &lt;- c(-3,-2) ar_row_three &lt;- c(0.35,0.75) mt_n_by_2 &lt;- rbind(ar_row_one, ar_row_two, ar_row_three) ar_row_four &lt;- c(3,4) # Matrix Multiplication mt_out &lt;- mt_n_by_2 %*% ar_row_four print(mt_n_by_2) ## [,1] [,2] ## ar_row_one -1.00 1.00 ## ar_row_two -3.00 -2.00 ## ar_row_three 0.35 0.75 print(ar_row_four) ## [1] 3 4 print(mt_out) ## [,1] ## ar_row_one 1.00 ## ar_row_two -17.00 ## ar_row_three 4.05 1.4 Variables in Dataframes 1.4.1 Generate Dataframe Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.4.1.1 Simple Meshed Dataframe Name Columns # 5 by 3 matrix mt_rnorm_a &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=5, ncol=3) # Column Names ar_st_varnames &lt;- c(&#39;id&#39;,&#39;var1&#39;,&#39;varb&#39;,&#39;vartheta&#39;) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_rnorm_a) %&gt;% rowid_to_column(var = &quot;id&quot;) %&gt;% rename_all(~c(ar_st_varnames)) # Display kable(tb_combine) %&gt;% kable_styling_fc() id var1 varb vartheta 1 -1.1655448 -0.8185157 0.6849361 2 -0.8185157 0.6849361 -0.3200564 3 0.6849361 -0.3200564 -1.1655448 4 -0.3200564 -1.1655448 -0.8185157 5 -1.1655448 -0.8185157 0.6849361 1.4.1.2 Generate Tibble given Matrixes and Arrays Given Arrays and Matrixes, Generate Tibble and Name Variables/Columns naming tibble columns tibble variable names dplyr rename tibble dplyr rename tibble all variables dplyr rename all columns by index dplyr tibble add index column see also: SO-51205520 # Base Inputs ar_col &lt;- c(-1,+1) mt_rnorm_a &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=2, ncol=2) mt_rnorm_b &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=2, ncol=4) # Combine Matrix mt_combine &lt;- cbind(ar_col, mt_rnorm_a, mt_rnorm_b) colnames(mt_combine) &lt;- c(&#39;ar_col&#39;, paste0(&#39;matcolvar_grpa_&#39;, seq(1,dim(mt_rnorm_a)[2])), paste0(&#39;matcolvar_grpb_&#39;, seq(1,dim(mt_rnorm_b)[2]))) # Variable Names ar_st_varnames &lt;- c(&#39;var_one&#39;, paste0(&#39;tibcolvar_ga_&#39;, c(1,2)), paste0(&#39;tibcolvar_gb_&#39;, c(1,2,3,4))) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_combine) %&gt;% rename_all(~c(ar_st_varnames)) # Add an index column to the dataframe, ID column tb_combine &lt;- tb_combine %&gt;% rowid_to_column(var = &quot;ID&quot;) # Change all gb variable names tb_combine &lt;- tb_combine %&gt;% rename_at(vars(starts_with(&quot;tibcolvar_gb_&quot;)), funs(str_replace(., &quot;_gb_&quot;, &quot;_gbrenamed_&quot;))) # Tibble back to matrix mt_tb_combine_back &lt;- data.matrix(tb_combine) # Display kable(mt_combine) %&gt;% kable_styling_fc_wide() ar_col matcolvar_grpa_1 matcolvar_grpa_2 matcolvar_grpb_1 matcolvar_grpb_2 matcolvar_grpb_3 matcolvar_grpb_4 -1 -1.3115224 -0.1294107 -0.1513960 -3.2273228 -0.1513960 -3.2273228 1 -0.5996083 0.8867361 0.3297912 -0.7717918 0.3297912 -0.7717918 kable(tb_combine) %&gt;% kable_styling_fc_wide() ID var_one tibcolvar_ga_1 tibcolvar_ga_2 tibcolvar_gbrenamed_1 tibcolvar_gbrenamed_2 tibcolvar_gbrenamed_3 tibcolvar_gbrenamed_4 1 -1 -1.3115224 -0.1294107 -0.1513960 -3.2273228 -0.1513960 -3.2273228 2 1 -0.5996083 0.8867361 0.3297912 -0.7717918 0.3297912 -0.7717918 kable(mt_tb_combine_back) %&gt;% kable_styling_fc_wide() ID var_one tibcolvar_ga_1 tibcolvar_ga_2 tibcolvar_gbrenamed_1 tibcolvar_gbrenamed_2 tibcolvar_gbrenamed_3 tibcolvar_gbrenamed_4 1 -1 -1.3115224 -0.1294107 -0.1513960 -3.2273228 -0.1513960 -3.2273228 2 1 -0.5996083 0.8867361 0.3297912 -0.7717918 0.3297912 -0.7717918 1.4.1.3 Rename Tibble with Numeric Column Names After reshaping, often could end up with variable names that are all numeric, intgers for example, how to rename these variables to add a common prefix for example. # Base Inputs ar_col &lt;- c(-1,+1) mt_rnorm_c &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=5, ncol=10) mt_combine &lt;- cbind(ar_col, mt_rnorm_c) # Variable Names ar_it_cols_ctr &lt;- seq(1, dim(mt_rnorm_c)[2]) ar_st_varnames &lt;- c(&#39;var_one&#39;, ar_it_cols_ctr) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_combine) %&gt;% rename_all(~c(ar_st_varnames)) # Add an index column to the dataframe, ID column tb_combine_ori &lt;- tb_combine %&gt;% rowid_to_column(var = &quot;ID&quot;) # Change all gb variable names tb_combine &lt;- tb_combine_ori %&gt;% rename_at( vars(num_range(&#39;&#39;,ar_it_cols_ctr)), funs(paste0(&quot;rho&quot;, . , &#39;var&#39;)) ) # Display kable(tb_combine_ori) %&gt;% kable_styling_fc_wide() ID var_one 1 2 3 4 5 6 7 8 9 10 1 -1 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 2 1 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 3 -1 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 4 1 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 5 -1 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 kable(tb_combine) %&gt;% kable_styling_fc_wide() ID var_one rho1var rho2var rho3var rho4var rho5var rho6var rho7var rho8var rho9var rho10var 1 -1 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 2 1 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 3 -1 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 4 1 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 5 -1 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 0.4345504 0.8001769 0.2865486 -1.2205120 1.4.1.4 Tibble Row and Column and Summarize Show what is in the table: 1, column and row names; 2, contents inside table. tb_iris &lt;- as_tibble(iris) print(rownames(tb_iris)) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; ## [21] &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; ## [41] &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; ## [61] &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; ## [81] &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; &quot;100&quot; ## [101] &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; &quot;109&quot; &quot;110&quot; &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;114&quot; &quot;115&quot; &quot;116&quot; &quot;117&quot; &quot;118&quot; &quot;119&quot; &quot;120&quot; ## [121] &quot;121&quot; &quot;122&quot; &quot;123&quot; &quot;124&quot; &quot;125&quot; &quot;126&quot; &quot;127&quot; &quot;128&quot; &quot;129&quot; &quot;130&quot; &quot;131&quot; &quot;132&quot; &quot;133&quot; &quot;134&quot; &quot;135&quot; &quot;136&quot; &quot;137&quot; &quot;138&quot; &quot;139&quot; &quot;140&quot; ## [141] &quot;141&quot; &quot;142&quot; &quot;143&quot; &quot;144&quot; &quot;145&quot; &quot;146&quot; &quot;147&quot; &quot;148&quot; &quot;149&quot; &quot;150&quot; colnames(tb_iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; colnames(tb_iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; summary(tb_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 1.4.1.5 Tibble Sorting dplyr arrange desc reverse dplyr sort # Sort in Ascending Order tb_iris %&gt;% select(Species, Sepal.Length, everything()) %&gt;% arrange(Species, Sepal.Length) %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() Species Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 4.3 3.0 1.1 0.1 setosa 4.4 2.9 1.4 0.2 setosa 4.4 3.0 1.3 0.2 setosa 4.4 3.2 1.3 0.2 setosa 4.5 2.3 1.3 0.3 setosa 4.6 3.1 1.5 0.2 setosa 4.6 3.4 1.4 0.3 setosa 4.6 3.6 1.0 0.2 setosa 4.6 3.2 1.4 0.2 setosa 4.7 3.2 1.3 0.2 # Sort in Descending Order tb_iris %&gt;% select(Species, Sepal.Length, everything()) %&gt;% arrange(desc(Species), desc(Sepal.Length)) %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() Species Sepal.Length Sepal.Width Petal.Length Petal.Width virginica 7.9 3.8 6.4 2.0 virginica 7.7 3.8 6.7 2.2 virginica 7.7 2.6 6.9 2.3 virginica 7.7 2.8 6.7 2.0 virginica 7.7 3.0 6.1 2.3 virginica 7.6 3.0 6.6 2.1 virginica 7.4 2.8 6.1 1.9 virginica 7.3 2.9 6.3 1.8 virginica 7.2 3.6 6.1 2.5 virginica 7.2 3.2 6.0 1.8 1.4.1.6 REconTools Summarize over Tible Use R4Econs summary tool. df_summ_stats &lt;- ff_summ_percentiles(tb_iris) kable(t(df_summ_stats)) %&gt;% kable_styling_fc_wide() stats n unique NAobs ZEROobs mean sd cv min p01 p05 p10 p25 p50 p75 p90 p95 p99 max Petal.Length 150 43 0 0 3.758000 1.7652982 0.4697441 1.0 1.149 1.300 1.4 1.6 4.35 5.1 5.80 6.100 6.700 6.9 Petal.Width 150 22 0 0 1.199333 0.7622377 0.6355511 0.1 0.100 0.200 0.2 0.3 1.30 1.8 2.20 2.300 2.500 2.5 Sepal.Length 150 35 0 0 5.843333 0.8280661 0.1417113 4.3 4.400 4.600 4.8 5.1 5.80 6.4 6.90 7.255 7.700 7.9 Sepal.Width 150 23 0 0 3.057333 0.4358663 0.1425642 2.0 2.200 2.345 2.5 2.8 3.00 3.3 3.61 3.800 4.151 4.4 1.4.2 Factor Label and Combine Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.4.2.1 Factor, Label, Cross and Graph Generate a Scatter plot with different colors representing different categories. There are multiple underlying factor/categorical variables, for example two binary variables. Generate scatter plot with colors for the combinations of these two binary variables. We combine here the vs and am variables from the mtcars dataset. vs is engine shape, am is auto or manual shift. We will generate a scatter plot of mpg and qsec over four categories with different colors. am: Transmission (0 = automatic, 1 = manual) vs: Engine (0 = V-shaped, 1 = straight) mpg: miles per galon qsec: 1/4 mile time # First make sure these are factors tb_mtcars &lt;- as_tibble(mtcars) %&gt;% mutate(vs = as_factor(vs), am = as_factor(am)) # Second Label the Factors am_levels &lt;- c(auto_shift = &quot;0&quot;, manual_shift = &quot;1&quot;) vs_levels &lt;- c(vshaped_engine = &quot;0&quot;, straight_engine = &quot;1&quot;) tb_mtcars &lt;- tb_mtcars %&gt;% mutate(vs = fct_recode(vs, !!!vs_levels), am = fct_recode(am, !!!am_levels)) # Third Combine Factors tb_mtcars_selected &lt;- tb_mtcars %&gt;% mutate(vs_am = fct_cross(vs, am, sep=&#39;_&#39;, keep_empty = FALSE)) %&gt;% select(mpg, qsec, vs_am) print(tb_mtcars_selected) Now we generate scatter plot based on the combined factors # Labeling st_title &lt;- paste0(&#39;Distribution of MPG and QSEC from mtcars&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/amto/tibble/htmlpdfr/fs_tib_factors.html&#39;) st_caption &lt;- paste0(&#39;mtcars dataset, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;MPG = Miles per Gallon&#39; st_y_label &lt;- &#39;QSEC = time for 1/4 Miles&#39; # Graphing plt_mtcars_scatter &lt;- ggplot(tb_mtcars_selected, aes(x=mpg, y=qsec, colour=vs_am, shape=vs_am)) + geom_jitter(size=3, width = 0.15) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_mtcars_scatter) 1.4.3 Drawly Random Rows Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.4.3.1 Draw Random Subset of Sample r random discrete We have a sample of N individuals in some dataframe. Draw without replacement a subset \\(M&lt;N\\) of rows. # parameters, it_M &lt; it_N it_N &lt;- 10 it_M &lt;- 5 # Draw it_m from indexed list of it_N set.seed(123) ar_it_rand_idx &lt;- sample(it_N, it_M, replace=FALSE) # dataframe df_full &lt;- as_tibble(matrix(rnorm(4,mean=0,sd=1), nrow=it_N, ncol=4)) %&gt;% rowid_to_column(var = &quot;ID&quot;) # random Subset df_rand_sub_a &lt;- df_full[ar_it_rand_idx,] # Random subset also df_rand_sub_b &lt;- df_full[sample(dim(df_full)[1], it_M, replace=FALSE),] # Print # Display kable(df_full) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 1 0.1292877 0.4609162 0.1292877 0.4609162 2 1.7150650 -1.2650612 1.7150650 -1.2650612 3 0.4609162 0.1292877 0.4609162 0.1292877 4 -1.2650612 1.7150650 -1.2650612 1.7150650 5 0.1292877 0.4609162 0.1292877 0.4609162 6 1.7150650 -1.2650612 1.7150650 -1.2650612 7 0.4609162 0.1292877 0.4609162 0.1292877 8 -1.2650612 1.7150650 -1.2650612 1.7150650 9 0.1292877 0.4609162 0.1292877 0.4609162 10 1.7150650 -1.2650612 1.7150650 -1.2650612 kable(df_rand_sub_a) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 3 0.4609162 0.1292877 0.4609162 0.1292877 10 1.7150650 -1.2650612 1.7150650 -1.2650612 2 1.7150650 -1.2650612 1.7150650 -1.2650612 8 -1.2650612 1.7150650 -1.2650612 1.7150650 6 1.7150650 -1.2650612 1.7150650 -1.2650612 kable(df_rand_sub_b) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 5 0.1292877 0.4609162 0.1292877 0.4609162 3 0.4609162 0.1292877 0.4609162 0.1292877 9 0.1292877 0.4609162 0.1292877 0.4609162 1 0.1292877 0.4609162 0.1292877 0.4609162 4 -1.2650612 1.7150650 -1.2650612 1.7150650 1.4.3.2 Random Subset of Panel There are \\(N\\) individuals, each could be observed \\(M\\) times, but then select a subset of rows only, so each person is randomly observed only a subset of times. Specifically, there there are 3 unique students with student ids, and the second variable shows the random dates in which the student showed up in class, out of the 10 classes available. # Define it_N &lt;- 3 it_M &lt;- 10 svr_id &lt;- &#39;student_id&#39; # dataframe set.seed(123) df_panel_rand &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(date = row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), date) %&gt;% rename(date_in_class = date) # Print kable(df_panel_rand) %&gt;% kable_styling_fc() student_id date_in_class 1 1 1 2 1 8 1 9 1 10 2 5 2 8 2 10 3 1 3 2 3 3 3 4 3 5 3 6 3 9 1.4.4 Generate Variables Conditional On Others Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.4.4.1 case_when Basic Example Given several other variables, and generate a new variable when these varaibles satisfy conditions. Note that case_when are ifelse type statements. So below group one is below 16 MPG when do qsec &gt;= 20 second line that is elseif, only those that are &gt;=16 are considered here then think about two dimensional mpg and qsec grid, the lower-right area, give another category to manual cars in that group # Get mtcars df_mtcars &lt;- mtcars # case_when with mtcars df_mtcars &lt;- df_mtcars %&gt;% mutate(mpg_qsec_am_grp = case_when(mpg &lt; 16 ~ &quot;&lt; 16 MPG&quot;, qsec &gt;= 20 ~ &quot;&gt; 16 MPG &amp; qsec &gt;= 20&quot;, am == 1 ~ &quot;&gt; 16 MPG &amp; asec &lt; 20 &amp; manual&quot;, TRUE ~ &quot;Others&quot;)) # # For dataframe # df.reg &lt;-df.reg %&gt;% na_if(-Inf) %&gt;% na_if(Inf) # # For a specific variable in dataframe # df.reg.use %&gt;% mutate(!!(var.input) := na_if(!!sym(var.input), 0)) # # # Setting to NA # df.reg.use &lt;- df.reg.guat %&gt;% filter(!!sym(var.mth) != 0) # df.reg.use.log &lt;- df.reg.use # df.reg.use.log[which(is.nan(df.reg.use$prot.imputed.log)),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==Inf),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==-Inf),] = NA # df.reg.use.log &lt;- df.reg.use.log %&gt;% drop_na(prot.imputed.log) # # df.reg.use.log$prot.imputed.log Now we generate scatter plot based on the combined factors # Labeling st_title &lt;- paste0(&#39;Use case_when To Generate ifelse Groupings&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/amto/tibble/htmlpdfr/fs_tib_na.html&#39;) st_caption &lt;- paste0(&#39;mtcars dataset, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;MPG = Miles per Gallon&#39; st_y_label &lt;- &#39;QSEC = time for 1/4 Miles&#39; # Graphing plt_mtcars_casewhen_scatter &lt;- ggplot(df_mtcars, aes(x=mpg, y=qsec, colour=mpg_qsec_am_grp, shape=mpg_qsec_am_grp)) + geom_jitter(size=3, width = 0.15) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_mtcars_casewhen_scatter) 1.4.4.2 Generate NA values if Variables have Certain Value In the example below, in one line: generate a random standard normal vector two set na methods: if the value of the standard normal is negative, set value to -999, otherwise MPG, replace the value -999 with NA case_when only with type specific NA values Assigning NA yields error in case_when note we need to conform NA to type generate new categorical variable based on NA condition using is.na with both string and numeric NAs jointly considered. fake NA string to be printed on chart # Get mtcars df_mtcars &lt;- mtcars # Make some values of mpg randomly NA # the NA has to conform to the type of the remaining values for the new variable # NA_real_, NA_character_, NA_integer_, NA_complex_ set.seed(2341) df_mtcars &lt;- df_mtcars %&gt;% mutate(mpg_wth_NA1 = na_if( case_when( rnorm(n(),mean=0,sd=1) &lt; 0 ~ -999, TRUE ~ mpg), -999)) %&gt;% mutate(mpg_wth_NA2 = case_when( rnorm(n(),mean=0,sd=1) &lt; 0 ~ NA_real_, TRUE ~ mpg)) %&gt;% mutate(mpg_wth_NA3 = case_when( rnorm(n(),mean=0,sd=1) &lt; 0 ~ NA_character_, TRUE ~ &quot;shock &gt; 0 string&quot;)) # Generate New Variables based on if mpg_wth_NA is NA or not # same variable as above, but now first a category based on if NA # And we generate a fake string &quot;NA&quot; variable, this is not NA # the String NA allows for it to be printed on figure df_mtcars &lt;- df_mtcars %&gt;% mutate(group_with_na = case_when(is.na(mpg_wth_NA2) &amp; is.na(mpg_wth_NA3) ~ &quot;Rand String and Rand Numeric both NA&quot;, mpg &lt; 16 ~ &quot;&lt; 16 MPG&quot;, qsec &gt;= 20 ~ &quot;&gt; 16 MPG &amp; qsec &gt;= 20&quot;, am == 1 ~ &quot;&gt; 16 MPG &amp; asec &lt; 20 &amp; manual&quot;, TRUE ~ &quot;Fake String NA&quot;)) # show kable(head(df_mtcars %&gt;% select(starts_with(&#39;mpg&#39;)),13)) %&gt;% kable_styling_fc() mpg mpg_wth_NA1 mpg_wth_NA2 mpg_wth_NA3 21.0 NA NA shock &gt; 0 string 21.0 21.0 21.0 NA 22.8 NA NA NA 21.4 NA 21.4 NA 18.7 NA 18.7 NA 18.1 18.1 NA shock &gt; 0 string 14.3 14.3 NA shock &gt; 0 string 24.4 NA 24.4 NA 22.8 22.8 22.8 NA 19.2 19.2 NA NA 17.8 NA NA NA 16.4 16.4 16.4 NA 17.3 NA NA shock &gt; 0 string # # Setting to NA # df.reg.use &lt;- df.reg.guat %&gt;% filter(!!sym(var.mth) != 0) # df.reg.use.log &lt;- df.reg.use # df.reg.use.log[which(is.nan(df.reg.use$prot.imputed.log)),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==Inf),] = NA # df.reg.use.log[which(df.reg.use$prot.imputed.log==-Inf),] = NA # df.reg.use.log &lt;- df.reg.use.log %&gt;% drop_na(prot.imputed.log) # # df.reg.use.log$prot.imputed.log Now we generate scatter plot based on the combined factors, but now with the NA category # Labeling st_title &lt;- paste0(&#39;Use na_if and is.na to Generate and Distinguish NA Values\\n&#39;, &#39;NA_real_, NA_character_, NA_integer_, NA_complex_&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/amto/tibble/htmlpdfr/fs_tib_na.html&#39;) st_caption &lt;- paste0(&#39;mtcars dataset, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;MPG = Miles per Gallon&#39; st_y_label &lt;- &#39;QSEC = time for 1/4 Miles&#39; # Graphing plt_mtcars_ifisna_scatter &lt;- ggplot(df_mtcars, aes(x=mpg, y=qsec, colour=group_with_na, shape=group_with_na)) + geom_jitter(size=3, width = 0.15) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_mtcars_ifisna_scatter) 1.4.4.3 Approximate Values Comparison r values almost the same all.equal From numeric approximation, often values are very close, and should be set to equal. Use isTRUE(all.equal). In the example below, we randomly generates four arrays. Two of the arrays have slightly higher variance, two arrays have slightly lower variance. They sd are to be 10 times below or 10 times above the tolerance comparison level. The values are not the same in any of the columns, but by allowing for almost true given some tolerance level, in the low standard deviation case, the values differences are within tolerance, so they are equal. This is an essential issue when dealing with optimization results. # Set tolerance tol_lvl = 1.5e-3 sd_lower_than_tol = tol_lvl/10 sd_higher_than_tol = tol_lvl*10 # larger SD set.seed(123) mt_runif_standard &lt;- matrix(rnorm(10,mean=0,sd=sd_higher_than_tol), nrow=5, ncol=2) # small SD set.seed(123) mt_rnorm_small_sd &lt;- matrix(rnorm(10,mean=0,sd=sd_lower_than_tol), nrow=5, ncol=2) # Generates Random Matirx tb_rnorm_runif &lt;- as_tibble(cbind(mt_rnorm_small_sd, mt_runif_standard)) # Are Variables the same, not for strict comparison tb_rnorm_runif_approxi_same &lt;- tb_rnorm_runif %&gt;% mutate(V1_V2_ALMOST_SAME = case_when(isTRUE(all.equal(V1, V2, tolerance=tol_lvl)) ~ paste0(&#39;TOL=&#39;,sd_lower_than_tol,&#39;, SAME ALMOST&#39;), TRUE ~ paste0(&#39;TOL=&#39;,sd_lower_than_tol,&#39;, NOT SAME ALMOST&#39;))) %&gt;% mutate(V3_V4_ALMOST_SAME = case_when(isTRUE(all.equal(V3, V4, tolerance=tol_lvl)) ~ paste0(&#39;TOL=&#39;,sd_higher_than_tol,&#39;, SAME ALMOST&#39;), TRUE ~ paste0(&#39;TOL=&#39;,sd_higher_than_tol,&#39;, NOT SAME ALMOST&#39;))) # Pring kable(tb_rnorm_runif_approxi_same) %&gt;% kable_styling_fc_wide() V1 V2 V3 V4 V1_V2_ALMOST_SAME V3_V4_ALMOST_SAME -0.0000841 0.0002573 -0.0084071 0.0257260 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST -0.0000345 0.0000691 -0.0034527 0.0069137 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 0.0002338 -0.0001898 0.0233806 -0.0189759 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 0.0000106 -0.0001030 0.0010576 -0.0103028 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 0.0000194 -0.0000668 0.0019393 -0.0066849 TOL=0.00015, SAME ALMOST TOL=0.015, NOT SAME ALMOST 1.4.5 String Dataframes Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 1.4.5.1 List of Strings to Tibble Datfare There are several lists of strings, store them as variables in a dataframe. # Sting data inputs ls_st_abc &lt;- c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) ls_st_efg &lt;- c(&#39;e&#39;, &#39;f&#39;, &#39;g&#39;) ls_st_opq &lt;- c(&#39;o&#39;, &#39;p&#39;, &#39;q&#39;) mt_str = cbind(ls_st_abc, ls_st_efg, ls_st_opq) # Column Names ar_st_varnames &lt;- c(&#39;id&#39;,&#39;var1&#39;,&#39;var2&#39;,&#39;var3&#39;) # Combine to tibble, add name col1, col2, etc. tb_st_combine &lt;- as_tibble(mt_str) %&gt;% rowid_to_column(var = &quot;id&quot;) %&gt;% rename_all(~c(ar_st_varnames)) # Display kable(tb_st_combine) %&gt;% kable_styling_fc() id var1 var2 var3 1 a e o 2 b f p 3 c g q 1.4.5.2 Find and Replace Find and Replace in Dataframe. # if string value is contained in variable (&quot;bridex.B&quot; %in% (df.reg.out.all$vars_var.y)) # if string value is not contained in variable: # 1. type is variable name # 2. Toyota|Mazda are strings to be excluded filter(mtcars, !grepl(&#39;Toyota|Mazda&#39;, type)) # filter does not contain string rs_hgt_prot_log_tidy %&gt;% filter(!str_detect(term, &#39;prot&#39;)) "],["summarize-data.html", "Chapter 2 Summarize Data 2.1 Counting Observation 2.2 Sorting, Indexing, Slicing 2.3 Group Statistics 2.4 Distributional Statistics 2.5 Summarize Multiple Variables", " Chapter 2 Summarize Data 2.1 Counting Observation 2.1.1 Uncount Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). In some panel, there are \\(N\\) individuals, each observed for \\(Y_i\\) years. Given a dataset with two variables, the individual index, and the \\(Y_i\\) variable, expand the dataframe so that there is a row for each individual indexs each unique year in the survey. Search: r duplicate row by variable Links: see: Create duplicate rows based on a variable Algorithm: generate testing frame, the individual attribute dataset with invariant information over panel uncount, duplicate rows by years in survey group and generate sorted index add indiviual specific stat year to index # 1. Array of Years in the Survey ar_years_in_survey &lt;- c(2,3,1,10,2,5) ar_start_yaer &lt;- c(1,2,3,1,1,1) ar_end_year &lt;- c(2,4,3,10,2,5) mt_combine &lt;- cbind(ar_years_in_survey, ar_start_yaer, ar_end_year) # This is the individual attribute dataset, attributes that are invariant acrosss years tb_indi_attributes &lt;- as_tibble(mt_combine) %&gt;% rowid_to_column(var = &quot;ID&quot;) # 2. Sort and generate variable equal to sorted index tb_indi_panel &lt;- tb_indi_attributes %&gt;% uncount(ar_years_in_survey) # 3. Panel now construct exactly which year in survey, note that all needed is sort index # Note sorting not needed, all rows identical now tb_indi_panel &lt;- tb_indi_panel %&gt;% group_by(ID) %&gt;% mutate(yr_in_survey = row_number()) tb_indi_panel &lt;- tb_indi_panel %&gt;% mutate(calendar_year = yr_in_survey + ar_start_yaer - 1) # Show results Head 10 tb_indi_panel %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() ID ar_start_yaer ar_end_year yr_in_survey calendar_year 1 1 2 1 1 1 1 2 2 2 2 2 4 1 2 2 2 4 2 3 2 2 4 3 4 3 3 3 1 3 4 1 10 1 1 4 1 10 2 2 4 1 10 3 3 4 1 10 4 4 2.2 Sorting, Indexing, Slicing 2.2.1 Sorting Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 2.2.1.1 Generate Sorted Index within Group with Repeating Values There is a variable, sort by this variable, then generate index from 1 to N representing sorted values of this index. If there are repeating values, still assign index, different index each value. r generate index sort dplyr mutate equals index # Sort and generate variable equal to sorted index df_iris &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% select(Sepal.Length, Sepal.Len.Index, everything()) # Show results Head 10 df_iris %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Width Petal.Length Petal.Width Species 4.3 1 3.0 1.1 0.1 setosa 4.4 2 2.9 1.4 0.2 setosa 4.4 3 3.0 1.3 0.2 setosa 4.4 4 3.2 1.3 0.2 setosa 4.5 5 2.3 1.3 0.3 setosa 4.6 6 3.1 1.5 0.2 setosa 4.6 7 3.4 1.4 0.3 setosa 4.6 8 3.6 1.0 0.2 setosa 4.6 9 3.2 1.4 0.2 setosa 4.7 10 3.2 1.3 0.2 setosa 2.2.1.2 Populate Value from Lowest Index to All other Rows We would like to calculate for example the ratio of each individuals highest to the the person with the lowest height in a dataset. We first need to generated sorted index from lowest to highest, and then populate the lowest height to all rows, and then divide. Search Terms: r spread value to all rows from one row r other rows equal to the value of one row Conditional assignment of one variable to the value of one of two other variables dplyr mutate conditional dplyr value from one row to all rows dplyr mutate equal to value in another cell Links: see: dplyr rank see: dplyr case_when 2.2.1.2.1 Short Method: mutate and min We just want the lowest value to be in its own column, so that we can compute various statistics using the lowest value variable and the original variable. # 1. Sort df_iris_m1 &lt;- iris %&gt;% mutate(Sepal.Len.Lowest.all = min(Sepal.Length)) %&gt;% select(Sepal.Length, Sepal.Len.Lowest.all, everything()) # Show results Head 10 df_iris_m1 %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Lowest.all Sepal.Width Petal.Length Petal.Width Species 5.1 4.3 3.5 1.4 0.2 setosa 4.9 4.3 3.0 1.4 0.2 setosa 4.7 4.3 3.2 1.3 0.2 setosa 4.6 4.3 3.1 1.5 0.2 setosa 5.0 4.3 3.6 1.4 0.2 setosa 5.4 4.3 3.9 1.7 0.4 setosa 4.6 4.3 3.4 1.4 0.3 setosa 5.0 4.3 3.4 1.5 0.2 setosa 4.4 4.3 2.9 1.4 0.2 setosa 4.9 4.3 3.1 1.5 0.1 setosa 2.2.1.2.2 Long Method: row_number and case_when This is the long method, using row_number, and case_when. The benefit of this method is that it generates several intermediate variables that might be useful. And the key final step is to set a new variable (A=Sepal.Len.Lowest.all) equal to another variables (B=Sepal.Lengths) value at the index that satisfies condition based a third variable (C=Sepal.Len.Index). # 1. Sort # 2. generate index # 3. value at lowest index (case_when) # 4. spread value from lowest index to other rows # Note step 4 does not require step 3 df_iris_m2 &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% mutate(Sepal.Len.Lowest.one = case_when(row_number()==1 ~ Sepal.Length)) %&gt;% mutate(Sepal.Len.Lowest.all = Sepal.Length[Sepal.Len.Index==1]) %&gt;% select(Sepal.Length, Sepal.Len.Index, Sepal.Len.Lowest.one, Sepal.Len.Lowest.all) # Show results Head 10 df_iris_m2 %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Len.Lowest.one Sepal.Len.Lowest.all 4.3 1 4.3 4.3 4.4 2 NA 4.3 4.4 3 NA 4.3 4.4 4 NA 4.3 4.5 5 NA 4.3 4.6 6 NA 4.3 4.6 7 NA 4.3 4.6 8 NA 4.3 4.6 9 NA 4.3 4.7 10 NA 4.3 2.2.1.3 Generate Sorted Index based on Deviations Generate Positive and Negative Index based on Ordered Deviation from some Number. There is a variable that is continuous, substract a number from this variable, and generate index based on deviations. Think of the index as generating intervals indicating where the value lies. 0th index indicates the largest value in sequence that is smaller than or equal to number \\(x\\), 1st index indicates the smallest value in sequence that is larger than number \\(x\\). The solution below is a little bit convoluated and long, there is likely a much quicker way. The process below shows various intermediary outputs that help arrive at deviation index Sepal.Len.Devi.Index from initial sorted index Sepal.Len.Index. search: dplyr arrange ignore na dplyr index deviation from order number sequence dplyr index below above dplyr index order below above value # 1. Sort and generate variable equal to sorted index # 2. Plus or minus deviations from some value # 3. Find the zero, which means, the number closests to zero including zero from the negative side # 4. Find the index at the highest zero and below deviation point # 5. Difference of zero index and original sorted index sc_val_x &lt;- 4.65 df_iris_deviate &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% mutate(Sepal.Len.Devi = (Sepal.Length - sc_val_x)) %&gt;% mutate(Sepal.Len.Devi.Neg = case_when(Sepal.Len.Devi &lt;= 0 ~ (-1)*(Sepal.Len.Devi))) %&gt;% arrange((Sepal.Len.Devi.Neg), desc(Sepal.Len.Index)) %&gt;% mutate(Sepal.Len.Index.Zero = case_when(row_number() == 1 ~ Sepal.Len.Index)) %&gt;% mutate(Sepal.Len.Devi.Index = Sepal.Len.Index - Sepal.Len.Index.Zero[row_number() == 1]) %&gt;% arrange(Sepal.Len.Index) %&gt;% select(Sepal.Length, Sepal.Len.Index, Sepal.Len.Devi, Sepal.Len.Devi.Neg, Sepal.Len.Index.Zero, Sepal.Len.Devi.Index) # Show results Head 10 df_iris_deviate %&gt;% head(20) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Len.Devi Sepal.Len.Devi.Neg Sepal.Len.Index.Zero Sepal.Len.Devi.Index 4.3 1 -0.35 0.35 NA -8 4.4 2 -0.25 0.25 NA -7 4.4 3 -0.25 0.25 NA -6 4.4 4 -0.25 0.25 NA -5 4.5 5 -0.15 0.15 NA -4 4.6 6 -0.05 0.05 NA -3 4.6 7 -0.05 0.05 NA -2 4.6 8 -0.05 0.05 NA -1 4.6 9 -0.05 0.05 9 0 4.7 10 0.05 NA NA 1 4.7 11 0.05 NA NA 2 4.8 12 0.15 NA NA 3 4.8 13 0.15 NA NA 4 4.8 14 0.15 NA NA 5 4.8 15 0.15 NA NA 6 4.8 16 0.15 NA NA 7 4.9 17 0.25 NA NA 8 4.9 18 0.25 NA NA 9 4.9 19 0.25 NA NA 10 4.9 20 0.25 NA NA 11 2.2.2 Group, Sort and Slice Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 2.2.2.1 Get Highest Values from Groups There is a dataframe with a grouping variable. Get N rows that have the highest sorted value for another numeric variable. In the example below, group by cyl and get the cars with the lowest mpg in each cyl group. Show all values. kable(mtcars %&gt;% arrange(cyl, mpg)) %&gt;% kable_styling_fc() mpg cyl disp hp drat wt qsec vs am gear carb Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Three groups min mpg each group: # use mtcars: slice_head gets the lowest sorted value df_groupby_top_mpg &lt;- mtcars %&gt;% arrange(cyl, mpg) %&gt;% group_by(cyl) %&gt;% slice_head(n=1) %&gt;% select(cyl, mpg) # display kable(df_groupby_top_mpg) %&gt;% kable_styling_fc() cyl mpg 4 21.4 6 17.8 8 10.4 2.3 Group Statistics 2.3.1 Cumulative Statistics within Group Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 2.3.1.1 Cumulative Mean There is a dataset where there are different types of individuals, perhaps household size, that is the grouping variable. Within each group, we compute the incremental marginal propensity to consume for each additional check. We now also want to know the average propensity to consume up to each check considering all allocated checks. We needed to calculatet this for Nygaard, Sørensen and Wang (2021). This can be dealt with by using the cumall function. Use the df_hgt_wgt as the testing dataset. In the example below, group by individual id, sort by survey month, and cumulative mean over the protein variable. In the protein example First select the testing dataset and variables. # Load the REconTools Dataset df_hgt_wgt data(&quot;df_hgt_wgt&quot;) # str(df_hgt_wgt) # Select several rows df_hgt_wgt_sel &lt;- df_hgt_wgt %&gt;% filter(S.country == &quot;Cebu&quot;) %&gt;% select(indi.id, svymthRound, prot) Second, arrange, groupby, and cumulative mean. The protein variable is protein for each survey month, from month 2 to higher as babies grow. The protein intake observed is increasing quickly, hence, the cumulative mean is lower than the observed value for the survey month of the baby. # Group by indi.id and sort by protein df_hgt_wgt_sel_cummean &lt;- df_hgt_wgt_sel %&gt;% arrange(indi.id, svymthRound) %&gt;% group_by(indi.id) %&gt;% mutate(prot_cummean = cummean(prot)) # display results REconTools::ff_summ_percentiles(df_hgt_wgt_sel_cummean) # display results df_hgt_wgt_sel_cummean %&gt;% filter(indi.id %in% c(17, 18)) %&gt;% kable() %&gt;% kable_styling_fc() indi.id svymthRound prot prot_cummean 17 0 0.5 0.5000000 17 2 0.7 0.6000000 17 4 0.5 0.5666667 17 6 0.5 0.5500000 17 8 6.1 1.6600000 17 10 5.0 2.2166667 17 12 6.4 2.8142857 17 14 20.1 4.9750000 17 16 20.1 6.6555556 17 18 23.0 8.2900000 17 20 24.9 9.8000000 17 22 20.1 10.6583333 17 24 10.1 10.6153846 17 102 NA NA 17 138 NA NA 17 187 NA NA 17 224 NA NA 17 258 NA NA 18 0 1.2 1.2000000 18 2 4.7 2.9500000 18 4 17.2 7.7000000 18 6 18.6 10.4250000 18 8 NA NA 18 10 16.8 NA 18 12 NA NA 18 14 NA NA 18 16 NA NA 18 18 NA NA 18 20 NA NA 18 22 15.7 NA 18 24 22.5 NA 18 102 NA NA 18 138 NA NA 18 187 NA NA 18 224 NA NA 18 258 NA NA Third, in the basic implementation above, if an incremental month has NA, no values computed at that point or after. This is the case for individual 18 above. To ignore NA, we have, from this. Note how results for individual 18 changes. # https://stackoverflow.com/a/49906718/8280804 # Group by indi.id and sort by protein df_hgt_wgt_sel_cummean_noNA &lt;- df_hgt_wgt_sel %&gt;% arrange(indi.id, svymthRound) %&gt;% group_by(indi.id, isna = is.na(prot)) %&gt;% mutate(prot_cummean = ifelse(isna, NA, cummean(prot))) # display results df_hgt_wgt_sel_cummean_noNA %&gt;% filter(indi.id %in% c(17, 18)) %&gt;% kable() %&gt;% kable_styling_fc() indi.id svymthRound prot isna prot_cummean 17 0 0.5 FALSE 0.5000000 17 2 0.7 FALSE 0.6000000 17 4 0.5 FALSE 0.5666667 17 6 0.5 FALSE 0.5500000 17 8 6.1 FALSE 1.6600000 17 10 5.0 FALSE 2.2166667 17 12 6.4 FALSE 2.8142857 17 14 20.1 FALSE 4.9750000 17 16 20.1 FALSE 6.6555556 17 18 23.0 FALSE 8.2900000 17 20 24.9 FALSE 9.8000000 17 22 20.1 FALSE 10.6583333 17 24 10.1 FALSE 10.6153846 17 102 NA TRUE NA 17 138 NA TRUE NA 17 187 NA TRUE NA 17 224 NA TRUE NA 17 258 NA TRUE NA 18 0 1.2 FALSE 1.2000000 18 2 4.7 FALSE 2.9500000 18 4 17.2 FALSE 7.7000000 18 6 18.6 FALSE 10.4250000 18 8 NA TRUE NA 18 10 16.8 FALSE 11.7000000 18 12 NA TRUE NA 18 14 NA TRUE NA 18 16 NA TRUE NA 18 18 NA TRUE NA 18 20 NA TRUE NA 18 22 15.7 FALSE 12.3666667 18 24 22.5 FALSE 13.8142857 18 102 NA TRUE NA 18 138 NA TRUE NA 18 187 NA TRUE NA 18 224 NA TRUE NA 18 258 NA TRUE NA 2.3.2 Groups Statistics Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 2.3.2.1 Aggrgate Groups only Unique Group and Count There are two variables that are numeric, we want to find all the unique groups of these two variables in a dataset and count how many times each unique group occurs r unique occurrence of numeric groups How to add count of unique values by group to R data.frame # Numeric value combinations unique Groups vars.group &lt;- c(&#39;hgt0&#39;, &#39;wgt0&#39;) # dataset subsetting df_use &lt;- df_hgt_wgt %&gt;% select(!!!syms(c(vars.group))) %&gt;% mutate(hgt0 = round(hgt0/5)*5, wgt0 = round(wgt0/2000)*2000) %&gt;% drop_na() # Group, count and generate means for each numeric variables # mutate_at(vars.group, funs(as.factor(.))) %&gt;% df.group.count &lt;- df_use %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(vars.group)) %&gt;% summarise(n_obs_group=n()) # Show results Head 10 df.group.count %&gt;% kable() %&gt;% kable_styling_fc() hgt0 wgt0 n_obs_group 40 2000 122 45 2000 4586 45 4000 470 50 2000 9691 50 4000 13106 55 2000 126 55 4000 1900 60 6000 18 2.3.2.2 Aggrgate Groups only Unique Group Show up With Means Several variables that are grouping identifiers. Several variables that are values which mean be unique for each group members. For example, a Panel of income for N households over T years with also household education information that is invariant over time. Want to generate a dataset where the unit of observation are households, rather than household years. Take average of all numeric variables that are household and year specific. A complicating factor potentially is that the number of observations differ within group, for example, income might be observed for all years for some households but not for other households. r dplyr aggregate group average Aggregating and analyzing data with dplyr column cant be modified because it is a grouping variable see also: Aggregating and analyzing data with dplyr # In the df_hgt_wgt from R4Econ, there is a country id, village id, # and individual id, and various other statistics vars.group &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;) vars.values &lt;- c(&#39;hgt&#39;, &#39;momEdu&#39;) # dataset subsetting df_use &lt;- df_hgt_wgt %&gt;% select(!!!syms(c(vars.group, vars.values))) # Group, count and generate means for each numeric variables df.group &lt;- df_use %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(vars.group)) %&gt;% summarise_if(is.numeric, funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE), n = sum(is.na(.)==0))) # Show results Head 10 df.group %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id hgt_mean momEdu_mean hgt_sd momEdu_sd hgt_n momEdu_n Cebu 1 1 61.80000 5.3 9.520504 0 7 18 Cebu 1 2 68.86154 7.1 9.058931 0 13 18 Cebu 1 3 80.45882 9.4 29.894231 0 17 18 Cebu 1 4 88.10000 13.9 35.533166 0 18 18 Cebu 1 5 97.70556 11.3 41.090366 0 18 18 Cebu 1 6 87.49444 7.3 35.586439 0 18 18 Cebu 1 7 90.79412 10.4 38.722385 0 17 18 Cebu 1 8 68.45385 13.5 10.011961 0 13 18 Cebu 1 9 86.21111 10.4 35.126057 0 18 18 Cebu 1 10 87.67222 10.5 36.508127 0 18 18 # Show results Head 10 df.group %&gt;% tail(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id hgt_mean momEdu_mean hgt_sd momEdu_sd hgt_n momEdu_n Guatemala 14 2014 66.97000 NaN 8.967974 NA 10 0 Guatemala 14 2015 71.71818 NaN 11.399984 NA 11 0 Guatemala 14 2016 66.33000 NaN 9.490352 NA 10 0 Guatemala 14 2017 76.40769 NaN 14.827871 NA 13 0 Guatemala 14 2018 74.55385 NaN 12.707846 NA 13 0 Guatemala 14 2019 70.47500 NaN 11.797390 NA 12 0 Guatemala 14 2020 60.28750 NaN 7.060036 NA 8 0 Guatemala 14 2021 84.96000 NaN 15.446193 NA 10 0 Guatemala 14 2022 79.38667 NaN 15.824749 NA 15 0 Guatemala 14 2023 66.50000 NaN 8.613113 NA 8 0 2.3.3 One Variable Group Summary Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). There is a categorical variable (based on one or the interaction of multiple variables), there is a continuous variable, obtain statistics for the continuous variable conditional on the categorical variable, but also unconditionally. Store results in a matrix, but also flatten results wide to row with appropriate keys/variable-names for all group statistics. Pick which statistics to be included in final wide row 2.3.3.1 Build Program # Single Variable Group Statistics (also generate overall statistics) ff_summ_by_group_summ_one &lt;- function( df, vars.group, var.numeric, str.stats.group = &#39;main&#39;, str.stats.specify = NULL, boo.overall.stats = TRUE){ # List of statistics # https://rdrr.io/cran/dplyr/man/summarise.html strs.center &lt;- c(&#39;mean&#39;, &#39;median&#39;) strs.spread &lt;- c(&#39;sd&#39;, &#39;IQR&#39;, &#39;mad&#39;) strs.range &lt;- c(&#39;min&#39;, &#39;max&#39;) strs.pos &lt;- c(&#39;first&#39;, &#39;last&#39;) strs.count &lt;- c(&#39;n_distinct&#39;) # Grouping of Statistics if (missing(str.stats.specify)) { if (str.stats.group == &#39;main&#39;) { strs.all &lt;- c(&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;, &#39;sd&#39;) } if (str.stats.group == &#39;all&#39;) { strs.all &lt;- c(strs.center, strs.spread, strs.range, strs.pos, strs.count) } } else { strs.all &lt;- str.stats.specify } # Start Transform df &lt;- df %&gt;% drop_na() %&gt;% mutate(!!(var.numeric) := as.numeric(!!sym(var.numeric))) # Overall Statistics if (boo.overall.stats) { df.overall.stats &lt;- df %&gt;% summarize_at(vars(var.numeric), funs(!!!strs.all)) if (length(strs.all) == 1) { # give it a name, otherwise if only one stat, name of stat not saved df.overall.stats &lt;- df.overall.stats %&gt;% rename(!!strs.all := !!sym(var.numeric)) } names(df.overall.stats) &lt;- paste0(var.numeric, &#39;.&#39;, names(df.overall.stats)) } # Group Sort df.select &lt;- df %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(c(vars.group, var.numeric))) # Table of Statistics df.table.grp.stats &lt;- df.select %&gt;% summarize_at(vars(var.numeric), funs(!!!strs.all)) # Add Stat Name if (length(strs.all) == 1) { # give it a name, otherwise if only one stat, name of stat not saved df.table.grp.stats &lt;- df.table.grp.stats %&gt;% rename(!!strs.all := !!sym(var.numeric)) } # Row of Statistics str.vars.group.combine &lt;- paste0(vars.group, collapse=&#39;_&#39;) if (length(vars.group) == 1) { df.row.grp.stats &lt;- df.table.grp.stats %&gt;% mutate(!!(str.vars.group.combine) := paste0(var.numeric, &#39;.&#39;, vars.group, &#39;.g&#39;, (!!!syms(vars.group)))) %&gt;% gather(variable, value, -one_of(vars.group)) %&gt;% unite(str.vars.group.combine, c(str.vars.group.combine, &#39;variable&#39;)) %&gt;% spread(str.vars.group.combine, value) } else { df.row.grp.stats &lt;- df.table.grp.stats %&gt;% mutate(vars.groups.combine := paste0(paste0(vars.group, collapse=&#39;.&#39;)), !!(str.vars.group.combine) := paste0(interaction(!!!(syms(vars.group))))) %&gt;% mutate(!!(str.vars.group.combine) := paste0(var.numeric, &#39;.&#39;, vars.groups.combine, &#39;.&#39;, (!!sym(str.vars.group.combine)))) %&gt;% ungroup() %&gt;% select(-vars.groups.combine, -one_of(vars.group)) %&gt;% gather(variable, value, -one_of(str.vars.group.combine)) %&gt;% unite(str.vars.group.combine, c(str.vars.group.combine, &#39;variable&#39;)) %&gt;% spread(str.vars.group.combine, value) } # Clean up name strings names(df.table.grp.stats) &lt;- gsub(x = names(df.table.grp.stats),pattern = &quot;_&quot;, replacement = &quot;\\\\.&quot;) names(df.row.grp.stats) &lt;- gsub(x = names(df.row.grp.stats),pattern = &quot;_&quot;, replacement = &quot;\\\\.&quot;) # Return list.return &lt;- list(df_table_grp_stats = df.table.grp.stats, df_row_grp_stats = df.row.grp.stats) # Overall Statistics, without grouping if (boo.overall.stats) { df.row.stats.all &lt;- c(df.row.grp.stats, df.overall.stats) list.return &lt;- append(list.return, list(df_overall_stats = df.overall.stats, df_row_stats_all = df.row.stats.all)) } # Return return(list.return) } 2.3.3.2 Test Load data and test # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) 2.3.3.2.1 Function Testing By Gender Groups Need two variables, a group variable that is a factor, and a numeric vars.group &lt;- &#39;sex&#39; var.numeric &lt;- &#39;hgt&#39; df.select &lt;- df %&gt;% select(one_of(vars.group, var.numeric)) %&gt;% drop_na() Main Statistics: # Single Variable Group Statistics ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.group = &#39;main&#39;)$df_table_grp_stats Specify Two Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;, &#39;sd&#39;))$df_table_grp_stats Specify One Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;))$df_table_grp_stats 2.3.3.2.2 Function Testing By Country and Gender Groups Need two variables, a group variable that is a factor, and a numeric. Now joint grouping variables. vars.group &lt;- c(&#39;S.country&#39;, &#39;sex&#39;) var.numeric &lt;- &#39;hgt&#39; df.select &lt;- df %&gt;% select(one_of(vars.group, var.numeric)) %&gt;% drop_na() Main Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.group = &#39;main&#39;)$df_table_grp_stats Specify Two Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;, &#39;sd&#39;))$df_table_grp_stats Specify One Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;))$df_table_grp_stats 2.3.4 Nested within Group Stats Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). By Multiple within Individual Groups Variables, Averages for All Numeric Variables within All Groups of All Group Variables (Long to very Wide). Suppose you have an individual level final outcome. The individual is observed for N periods, where each period the inputs differ. What inputs impacted the final outcome? Suppose we can divide N periods in which the individual is in the data into a number of years, a number of semi-years, a number of quarters, or uneven-staggered lengths. We might want to generate averages across individuals and within each of these different possible groups averages of inputs. Then we want to version of the data where each row is an individual, one of the variables is the final outcome, and the other variables are these different averages: averages for the 1st, 2nd, 3rd year in which indivdiual is in data, averages for 1st, , final quarter in which indivdiual is in data. 2.3.4.1 Build Function This function takes as inputs: vars.not.groups2avg: a list of variables that are not the within-indivdiual or across-individual grouping variables, but the variables we want to average over. Withnin indivdiual grouping averages will be calculated for these variables using the not-listed variables as within indivdiual groups (excluding vars.indi.grp groups). vars.indi.grp: a list or individual variables, and also perhaps villages, province, etc id variables that are higher than individual ID. Note the groups are are ACROSS individual higher level group variables. the remaining variables are all within individual grouping variables. the function output is a dataframe: each row is an individual initial variables individual ID and across individual groups from vars.indi.grp. other variables are all averages for the variables in vars.not.groups2avg if there are 2 within individual group variables, and the first has 3 groups (years), the second has 6 groups (semi-years), then there would be 9 average variables. each average variables has the original variable name from vars.not.groups2avg plus the name of the within individual grouping variable, and at the end c_x, where x is a integer representing the category within the group (if 3 years, x=1, 2, 3) # Data Function # https://fanwangecon.github.io/R4Econ/summarize/summ/ByGroupsSummWide.html f.by.groups.summ.wide &lt;- function(df.groups.to.average, vars.not.groups2avg, vars.indi.grp = c(&#39;S.country&#39;,&#39;ID&#39;), display=TRUE) { # 1. generate categoricals for full year (m.12), half year (m.6), quarter year (m.4) # 2. generate categoricals also for uneven years (m12t14) using # stagger (+2 rather than -1) # 3. reshape wide to long, so that all categorical date groups appear in var=value, # and categories in var=variable # 4. calculate mean for all numeric variables for all date groups # 5. combine date categorical variable and value, single var: # m.12.c1= first year average from m.12 averaging ######## ######## ######## ######## ####### # Step 1 ######## ######## ######## ######## ####### # 1. generate categoricals for full year (m.12), half year (m.6), quarter year (m.4) # 2. generate categoricals also for uneven years (m12t14) using stagger # (+2 rather than -1) ######## ######## ######## ######## ####### # S2: reshape wide to long, so that all categorical date groups appear in var=value, # and categories in var=variable; calculate mean for all # numeric variables for all date groups ######## ######## ######## ######## ####### df.avg.long &lt;- df.groups.to.average %&gt;% gather(variable, value, -one_of(c(vars.indi.grp, vars.not.groups2avg))) %&gt;% group_by(!!!syms(vars.indi.grp), variable, value) %&gt;% summarise_if(is.numeric, funs(mean(., na.rm = TRUE))) if (display){ dim(df.avg.long) options(repr.matrix.max.rows=10, repr.matrix.max.cols=20) print(df.avg.long) } ######## ######## ######## ######## ####### # S3 combine date categorical variable and value, single var: # m.12.c1= first year average from m.12 averaging; to do this make # data even longer first ######## ######## ######## ######## ####### # We already have the averages, but we want them to show up as variables, # mean for each group of each variable. df.avg.allvars.wide &lt;- df.avg.long %&gt;% ungroup() %&gt;% mutate(all_m_cate = paste0(variable, &#39;_c&#39;, value)) %&gt;% select(all_m_cate, everything(), -variable, -value) %&gt;% gather(variable, value, -one_of(vars.indi.grp), -all_m_cate) %&gt;% unite(&#39;var_mcate&#39;, variable, all_m_cate) %&gt;% spread(var_mcate, value) if (display){ dim(df.avg.allvars.wide) options(repr.matrix.max.rows=10, repr.matrix.max.cols=10) print(df.avg.allvars.wide) } return(df.avg.allvars.wide) } 2.3.4.2 Test Program In our sample dataset, the number of nutrition/height/income etc information observed within each country and month of age group are different. We have a panel dataset for children observed over different months of age. We have two key grouping variables: 1. country: data are observed for guatemala and cebu 2. month-age (survey month round=svymthRound): different months of age at which each individual child is observed A child could be observed for many months, or just a few months. A childs height information could be observed for more months-of-age than nutritional intake information. We eventually want to run regressions where the outcome is height/weight and the input is nutrition. The regressions will be at the month-of-age level. We need to know how many times different variables are observed at the month-of-age level. # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) 2.3.4.2.1 Generate Within Individual Groups In the data, children are observed for different number of months since birth. We want to calculate quarterly, semi-year, annual, etc average nutritional intakes. First generate these within-individual grouping variables. We can also generate uneven-staggered calendar groups as shown below. mth.var &lt;- &#39;svymthRound&#39; df.groups.to.average&lt;- df %&gt;% filter(!!sym(mth.var) &gt;= 0 &amp; !!sym(mth.var) &lt;= 24) %&gt;% mutate(m12t24=(floor((!!sym(mth.var) - 12) %/% 14) + 1), m8t24=(floor((!!sym(mth.var) - 8) %/% 18) + 1), m12 = pmax((floor((!!sym(mth.var)-1) %/% 12) + 1), 1), m6 = pmax((floor((!!sym(mth.var)-1) %/% 6) + 1), 1), m3 = pmax((floor((!!sym(mth.var)-1) %/% 3) + 1), 1)) # Show Results options(repr.matrix.max.rows=30, repr.matrix.max.cols=20) vars.arrange &lt;- c(&#39;S.country&#39;,&#39;indi.id&#39;,&#39;svymthRound&#39;) vars.groups.within.indi &lt;- c(&#39;m12t24&#39;, &#39;m8t24&#39;, &#39;m12&#39;, &#39;m6&#39;, &#39;m3&#39;) as.tibble(df.groups.to.average %&gt;% group_by(!!!syms(vars.arrange)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% select(!!!syms(vars.arrange), !!!syms(vars.groups.within.indi))) 2.3.4.2.2 Within Group Averages With the within-group averages created, we can generate averages for all variables within these groups. vars.not.groups2avg &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.indi.grp &lt;- c(&#39;S.country&#39;, &#39;indi.id&#39;) vars.groups.within.indi &lt;- c(&#39;m12t24&#39;, &#39;m8t24&#39;, &#39;m12&#39;, &#39;m6&#39;, &#39;m3&#39;) df.groups.to.average.select &lt;- df.groups.to.average %&gt;% select(one_of(c(vars.indi.grp, vars.not.groups2avg, vars.groups.within.indi))) df.avg.allvars.wide &lt;- f.by.groups.summ.wide(df.groups.to.average.select, vars.not.groups2avg, vars.indi.grp, display=FALSE) This is the tabular version of results dim(df.avg.allvars.wide) ## [1] 2023 38 names(df.avg.allvars.wide) ## [1] &quot;S.country&quot; &quot;indi.id&quot; &quot;cal_m12_c1&quot; &quot;cal_m12_c2&quot; &quot;cal_m12t24_c0&quot; &quot;cal_m12t24_c1&quot; &quot;cal_m3_c1&quot; ## [8] &quot;cal_m3_c2&quot; &quot;cal_m3_c3&quot; &quot;cal_m3_c4&quot; &quot;cal_m3_c5&quot; &quot;cal_m3_c6&quot; &quot;cal_m3_c7&quot; &quot;cal_m3_c8&quot; ## [15] &quot;cal_m6_c1&quot; &quot;cal_m6_c2&quot; &quot;cal_m6_c3&quot; &quot;cal_m6_c4&quot; &quot;cal_m8t24_c0&quot; &quot;cal_m8t24_c1&quot; &quot;prot_m12_c1&quot; ## [22] &quot;prot_m12_c2&quot; &quot;prot_m12t24_c0&quot; &quot;prot_m12t24_c1&quot; &quot;prot_m3_c1&quot; &quot;prot_m3_c2&quot; &quot;prot_m3_c3&quot; &quot;prot_m3_c4&quot; ## [29] &quot;prot_m3_c5&quot; &quot;prot_m3_c6&quot; &quot;prot_m3_c7&quot; &quot;prot_m3_c8&quot; &quot;prot_m6_c1&quot; &quot;prot_m6_c2&quot; &quot;prot_m6_c3&quot; ## [36] &quot;prot_m6_c4&quot; &quot;prot_m8t24_c0&quot; &quot;prot_m8t24_c1&quot; df.avg.allvars.wide[1:20,] %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country indi.id cal_m12_c1 cal_m12_c2 cal_m12t24_c0 cal_m12t24_c1 cal_m3_c1 cal_m3_c2 cal_m3_c3 cal_m3_c4 cal_m3_c5 cal_m3_c6 cal_m3_c7 cal_m3_c8 cal_m6_c1 cal_m6_c2 cal_m6_c3 cal_m6_c4 cal_m8t24_c0 cal_m8t24_c1 prot_m12_c1 prot_m12_c2 prot_m12t24_c0 prot_m12t24_c1 prot_m3_c1 prot_m3_c2 prot_m3_c3 prot_m3_c4 prot_m3_c5 prot_m3_c6 prot_m3_c7 prot_m3_c8 prot_m6_c1 prot_m6_c2 prot_m6_c3 prot_m6_c4 prot_m8t24_c0 prot_m8t24_c1 Cebu 1 132.15714 NaN 97.08333 342.6000 9.10 95.50 85.3 315.30 NaN NaN NaN NaN 52.300 238.63333 NaN NaN 52.300 238.6333 5.3571429 NaN 4.3666667 11.300000 0.65 3.65 2.6 13.15 NaN NaN NaN NaN 2.150 9.6333333 NaN NaN 2.150 9.633333 Cebu 2 90.72857 255.6500 81.46667 240.0286 83.35 12.30 155.1 144.35 228.0 152.85 305.0 347.60 47.825 147.93333 177.9000 333.4000 47.825 219.7444 3.1857143 8.550000 2.7333333 8.171429 3.20 1.25 5.2 4.10 5.4 5.15 7.7 13.95 2.225 4.4666667 5.233333 11.866667 2.225 7.188889 Cebu 3 96.80000 658.8167 31.56667 634.4429 0.50 28.85 57.0 280.95 459.3 549.95 612.0 890.85 14.675 206.30000 519.7333 797.9000 14.675 507.9778 4.5000000 21.116667 1.6833333 21.157143 1.05 2.15 2.3 11.40 18.5 18.05 18.0 27.05 1.600 8.3666667 18.200000 24.033333 1.600 16.866667 Cebu 4 27.45714 371.7000 24.55000 325.0143 4.50 25.95 39.4 45.95 221.2 271.00 581.3 442.85 15.225 43.76667 254.4000 489.0000 15.225 262.3889 0.8714286 6.850000 0.9000000 5.971429 0.75 1.10 1.2 0.60 1.8 4.85 10.1 9.75 0.925 0.8000000 3.833333 9.866667 0.925 4.833333 Cebu 5 101.34286 1080.8500 79.15000 959.9429 14.10 143.80 71.3 161.15 452.6 1345.20 1178.1 1082.00 78.950 131.20000 1047.6667 1114.0333 78.950 764.3000 2.4000000 19.483333 2.3166667 17.114286 1.35 3.00 3.4 2.35 7.1 23.15 24.5 19.50 2.175 2.7000000 17.800000 21.166667 2.175 13.888889 Cebu 6 185.35714 521.5333 162.23333 493.3286 23.85 184.70 169.1 355.65 653.4 506.50 416.8 523.00 104.275 293.46667 555.4667 487.6000 104.275 445.5111 8.4000000 15.116667 7.3833333 15.028571 0.85 7.40 9.8 16.25 26.8 14.10 11.4 12.15 4.125 14.1000000 18.333333 11.900000 4.125 14.777778 Cebu 7 157.25714 570.9800 145.50000 513.7833 8.30 137.80 407.8 200.40 390.6 637.10 688.1 569.55 73.050 269.53333 513.8500 609.0667 73.050 457.9375 3.3000000 20.440000 2.7833333 18.100000 0.95 1.70 8.6 4.60 16.4 23.00 21.5 20.65 1.325 5.9333333 19.700000 20.933333 1.325 15.000000 Cebu 8 471.92857 844.8333 379.20000 871.0429 158.95 423.00 417.5 861.05 691.3 897.95 637.1 972.35 290.975 713.20000 829.0667 860.6000 290.975 800.9556 13.6857143 32.716667 11.0166667 32.285714 3.90 11.35 10.8 27.25 42.7 26.45 25.8 37.45 7.625 21.7666667 31.866667 33.566667 7.625 29.066667 Cebu 9 32.27143 415.2167 16.58333 373.9571 5.05 10.40 15.1 89.95 142.4 203.60 753.2 594.25 7.725 65.00000 183.2000 647.2333 7.725 298.4778 0.9571429 18.283333 0.9166667 15.842857 0.50 0.50 0.5 2.10 4.2 10.85 39.5 22.15 0.500 1.5666667 8.633333 27.933333 0.500 12.711111 Cebu 10 67.18571 395.2500 68.58333 347.1857 9.55 26.40 164.6 116.90 296.6 303.00 385.1 541.90 17.975 132.80000 300.8667 489.6333 17.975 307.7667 2.0428571 8.466667 1.9333333 7.642857 0.85 0.50 4.9 3.35 7.5 6.05 9.2 11.00 0.675 3.8666667 6.533333 10.400000 0.675 6.933333 Cebu 11 14.90000 245.3833 11.80000 215.1143 0.50 5.20 30.0 31.45 126.2 223.05 239.6 330.20 2.850 30.96667 190.7667 300.0000 2.850 173.9111 1.0285714 6.833333 1.1166667 5.928571 0.80 1.70 1.2 0.50 3.6 6.35 7.3 8.70 1.250 0.7333333 5.433333 8.233333 1.250 4.800000 Cebu 12 453.61429 745.6833 419.51667 733.1857 325.60 483.65 463.0 546.90 766.8 676.85 998.5 677.55 404.625 518.93333 706.8333 784.5333 404.625 670.1000 14.7714286 22.133333 14.1666667 21.600000 7.40 13.60 26.6 17.40 18.6 20.35 25.6 23.95 10.500 20.4666667 19.766667 24.500000 10.500 21.577778 Cebu 13 47.51429 210.2500 36.81667 196.1714 17.45 40.00 28.5 94.60 216.9 195.15 281.3 186.50 28.725 72.56667 202.4000 218.1000 28.725 164.3556 1.9571429 6.800000 1.6666667 6.357143 0.70 1.50 2.1 3.60 8.3 4.75 6.2 8.40 1.100 3.1000000 5.933333 7.666667 1.100 5.566667 Cebu 14 608.85714 924.5833 527.30000 949.3857 259.05 554.30 688.1 973.60 525.5 1039.60 800.0 1071.40 406.675 878.43333 868.2333 980.9333 406.675 909.2000 24.6714286 28.050000 23.0833333 28.928571 15.05 28.85 32.6 26.15 12.7 29.75 29.5 33.30 21.950 28.3000000 24.066667 32.033333 21.950 28.133333 Cebu 15 74.67143 440.0833 64.21667 396.8429 62.40 39.60 80.1 119.30 292.5 237.45 607.8 632.65 51.000 106.23333 255.8000 624.3667 51.000 328.8000 2.2571429 10.633333 1.6333333 9.971429 1.65 0.90 1.5 4.60 9.4 5.80 13.6 14.60 1.275 3.5666667 7.000000 14.266667 1.275 8.277778 Cebu 16 128.45714 519.9333 90.50000 496.5429 4.80 11.00 205.3 331.15 290.8 354.50 563.3 778.25 7.900 289.20000 333.2667 706.6000 7.900 443.0222 4.6857143 18.083333 4.1000000 16.671429 0.65 2.70 8.7 8.70 7.8 12.95 25.9 24.45 1.675 8.7000000 11.233333 24.933333 1.675 14.955556 Cebu 17 130.78571 718.8667 97.48333 663.4000 5.50 7.80 249.9 319.50 774.6 892.90 551.9 600.45 6.650 296.30000 853.4667 584.2667 6.650 578.0111 2.8142857 19.716667 2.2166667 17.814286 0.60 0.50 6.1 5.70 20.1 21.55 24.9 15.10 0.550 5.8333333 21.066667 18.366667 0.550 15.088889 Cebu 18 172.64000 497.9500 172.64000 497.9500 29.60 234.40 NaN 335.20 NaN NaN NaN 497.95 132.000 335.20000 NaN 497.9500 132.000 443.7000 11.7000000 19.100000 11.7000000 19.100000 2.95 17.90 NaN 16.80 NaN NaN NaN 19.10 10.425 16.8000000 NaN 19.100000 10.425 18.333333 Cebu 19 74.45714 314.7333 80.10000 275.5714 3.65 95.70 171.3 75.60 131.3 350.50 304.6 375.75 49.675 107.50000 277.4333 352.0333 49.675 245.6556 2.5428571 10.466667 2.8833333 9.042857 0.50 2.95 6.7 2.10 3.4 11.40 12.3 12.15 1.725 3.6333333 8.733333 12.200000 1.725 8.188889 Cebu 20 110.90000 583.2000 80.51667 541.7714 7.30 120.65 77.8 221.30 391.2 582.10 466.1 738.85 63.975 173.46667 518.4667 647.9333 63.975 446.6222 3.2000000 16.966667 2.1833333 15.871429 0.50 3.85 2.7 5.50 7.9 20.15 11.8 20.90 2.175 4.5666667 16.066667 17.866667 2.175 12.833333 2.4 Distributional Statistics 2.4.1 Histogram 2.4.1.1 Generate Test Score Dataset Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). r generate text string as csv r tibble matrix hand input First, we will generate a test score dataset, directly from string. Below we type line by line a dataset with four variables in comma separated (csv) format, where the first row includes the variables names. These texts could be stored in a separate file, or they could be directly included in code and read in as csv 2.4.1.1.1 A Dataset with only Two Continuous Variable ar_test_scores_ec3 &lt;- c(107.72,101.28,105.92,109.31,104.27,110.27,91.92846154,81.8,109.0071429,103.07,98.97923077,101.91,96.49,97.79923077,99.07846154,99.17,103.51,112.2225,101.2964286,94.5,98.92,97.09,93.83989011,97.36304945,80.34,65.74,85.275,82.19708791,86.53758242,86.2025,86.63,82.57392857,83.66,79.76,75.55642857,86.32571429,66.41,76.06,44.225,82.28,47.77392857,70.005,69.13769231,73.52571429,60.51,56.04) ar_test_scores_ec1 &lt;- c(101.72,101.28,99.92,103.31,100.27,104.27,90.23615385,77.8,103.4357143,97.07,93.13307692,95.91,92.49,93.95307692,95.38615385,97.17,99.51,100.3475,95.83214286,92.5,94.92,91.09,90.4332967,93.52101648,80.34,59.74,79.525,77.67236264,81.59252747,82.3275,80.63,76.98464286,81.66,79.76,70.59214286,82.46857143,66.41,74.06,40.475,76.28,44.18464286,66.255,65.59923077,69.66857143,60.51,56.04) mt_test_scores &lt;- cbind(ar_test_scores_ec1, ar_test_scores_ec3) ar_st_varnames &lt;- c(&#39;course_total_ec1p&#39;,&#39;course_total_ec3p&#39;) tb_final_twovar &lt;- as_tibble(mt_test_scores) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_final_twovar) ## course_total_ec1p course_total_ec3p ## Min. : 40.48 Min. : 44.23 ## 1st Qu.: 76.46 1st Qu.: 79.91 ## Median : 86.35 Median : 89.28 ## Mean : 83.88 Mean : 87.90 ## 3rd Qu.: 95.89 3rd Qu.:100.75 ## Max. :104.27 Max. :112.22 ff_summ_percentiles(df = tb_final_twovar, bl_statsasrows = TRUE, col2varname = FALSE) 2.4.1.1.2 A Dataset with one Continuous Variable and Histogram ar_final_scores &lt;- c(94.28442509,95.68817475,97.25219512,77.89268293,95.08795497,93.27380863,92.3,84.25317073,86.08642991,84.69219512,71.43634146,76.21365854,71.68878049,77.46142589,79.29579268,43.7285453,63.80634146,67.92994774,100.8980488,100.0857143,99.93073171,98.4102439,97.93,97.10359756,96.97121951,96.60292683,96.23317073,93.92243902,93.82243902,92.75390244,92.65775261,92.20444653,91.73463415,90.38321161,89.37414634,86.95932458,79.58686411,78.70878049,77.2497561,76.88195122,76.52987805,74.72114313,74.27488676,71.30268293,63.70256098,37.90426829,2.292682927) mt_test_scores &lt;- cbind(seq(1,length(ar_final_scores)), ar_final_scores) ar_st_varnames &lt;- c(&#39;index&#39;, &#39;course_final&#39;) tb_onevar &lt;- as_tibble(mt_test_scores) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_onevar) ## index course_final ## Min. : 1.0 Min. : 2.293 ## 1st Qu.:12.5 1st Qu.: 76.372 ## Median :24.0 Median : 86.959 ## Mean :24.0 Mean : 82.415 ## 3rd Qu.:35.5 3rd Qu.: 94.686 ## Max. :47.0 Max. :100.898 ff_summ_percentiles(df = tb_onevar, bl_statsasrows = TRUE, col2varname = FALSE) 2.4.1.1.3 A Dataset with Multiple Variables #load in data empirically by hand txt_test_data &lt;- &quot;init_prof, later_prof, class_id, exam_score &#39;SW&#39;, &#39;SW&#39;, 1, 102 &#39;SW&#39;, &#39;SW&#39;, 1, 102 &#39;SW&#39;, &#39;SW&#39;, 1, 101 &#39;SW&#39;, &#39;SW&#39;, 1, 100 &#39;SW&#39;, &#39;SW&#39;, 1, 100 &#39;SW&#39;, &#39;SW&#39;, 1, 99 &#39;SW&#39;, &#39;SW&#39;, 1, 98.5 &#39;SW&#39;, &#39;SW&#39;, 1, 98.5 &#39;SW&#39;, &#39;SW&#39;, 1, 97 &#39;SW&#39;, &#39;SW&#39;, 1, 95 &#39;SW&#39;, &#39;SW&#39;, 1, 94 &#39;SW&#39;, &#39;SW&#39;, 1, 91 &#39;SW&#39;, &#39;SW&#39;, 1, 91 &#39;SW&#39;, &#39;SW&#39;, 1, 90 &#39;SW&#39;, &#39;SW&#39;, 1, 89 &#39;SW&#39;, &#39;SW&#39;, 1, 88.5 &#39;SW&#39;, &#39;SW&#39;, 1, 88 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 86 &#39;SW&#39;, &#39;SW&#39;, 1, 86 &#39;SW&#39;, &#39;SW&#39;, 1, 84 &#39;SW&#39;, &#39;SW&#39;, 1, 82 &#39;SW&#39;, &#39;SW&#39;, 1, 78.5 &#39;SW&#39;, &#39;SW&#39;, 1, 76 &#39;SW&#39;, &#39;SW&#39;, 1, 72 &#39;SW&#39;, &#39;SW&#39;, 1, 70.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67 &#39;SW&#39;, &#39;SW&#39;, 1, 63.5 &#39;SW&#39;, &#39;SW&#39;, 1, 60 &#39;SW&#39;, &#39;SW&#39;, 1, 59 &#39;SW&#39;, &#39;SW&#39;, 1, 44.5 &#39;SW&#39;, &#39;SW&#39;, 1, 44 &#39;SW&#39;, &#39;SW&#39;, 1, 42.5 &#39;SW&#39;, &#39;SW&#39;, 1, 40.5 &#39;SW&#39;, &#39;SW&#39;, 1, 40.5 &#39;SW&#39;, &#39;SW&#39;, 1, 36.5 &#39;SW&#39;, &#39;SW&#39;, 1, 35.5 &#39;SW&#39;, &#39;SW&#39;, 1, 21.5 &#39;SW&#39;, &#39;SW&#39;, 1, 4 &#39;MP&#39;, &#39;MP&#39;, 2, 105 &#39;MP&#39;, &#39;MP&#39;, 2, 103 &#39;MP&#39;, &#39;MP&#39;, 2, 102 &#39;MP&#39;, &#39;MP&#39;, 2, 101 &#39;MP&#39;, &#39;MP&#39;, 2, 101 &#39;MP&#39;, &#39;MP&#39;, 2, 100.5 &#39;MP&#39;, &#39;MP&#39;, 2, 100 &#39;MP&#39;, &#39;MP&#39;, 2, 99 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 96 &#39;MP&#39;, &#39;MP&#39;, 2, 95 &#39;MP&#39;, &#39;MP&#39;, 2, 91 &#39;MP&#39;, &#39;MP&#39;, 2, 89 &#39;MP&#39;, &#39;MP&#39;, 2, 85 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 83.5 &#39;MP&#39;, &#39;MP&#39;, 2, 82.5 &#39;MP&#39;, &#39;MP&#39;, 2, 81.5 &#39;MP&#39;, &#39;MP&#39;, 2, 80.5 &#39;MP&#39;, &#39;MP&#39;, 2, 80 &#39;MP&#39;, &#39;MP&#39;, 2, 77 &#39;MP&#39;, &#39;MP&#39;, 2, 77 &#39;MP&#39;, &#39;MP&#39;, 2, 75 &#39;MP&#39;, &#39;MP&#39;, 2, 75 &#39;MP&#39;, &#39;MP&#39;, 2, 71 &#39;MP&#39;, &#39;MP&#39;, 2, 70 &#39;MP&#39;, &#39;MP&#39;, 2, 68 &#39;MP&#39;, &#39;MP&#39;, 2, 63 &#39;MP&#39;, &#39;MP&#39;, 2, 56 &#39;MP&#39;, &#39;MP&#39;, 2, 56 &#39;MP&#39;, &#39;MP&#39;, 2, 55.5 &#39;MP&#39;, &#39;MP&#39;, 2, 49.5 &#39;MP&#39;, &#39;MP&#39;, 2, 48.5 &#39;MP&#39;, &#39;MP&#39;, 2, 47.5 &#39;MP&#39;, &#39;MP&#39;, 2, 44.5 &#39;MP&#39;, &#39;MP&#39;, 2, 34.5 &#39;MP&#39;, &#39;MP&#39;, 2, 29.5 &#39;CA&#39;, &#39;MP&#39;, 3, 103 &#39;CA&#39;, &#39;MP&#39;, 3, 103 &#39;CA&#39;, &#39;MP&#39;, 3, 101 &#39;CA&#39;, &#39;MP&#39;, 3, 96.5 &#39;CA&#39;, &#39;MP&#39;, 3, 93.5 &#39;CA&#39;, &#39;MP&#39;, 3, 93 &#39;CA&#39;, &#39;MP&#39;, 3, 93 &#39;CA&#39;, &#39;MP&#39;, 3, 92 &#39;CA&#39;, &#39;MP&#39;, 3, 90 &#39;CA&#39;, &#39;MP&#39;, 3, 90 &#39;CA&#39;, &#39;MP&#39;, 3, 89 &#39;CA&#39;, &#39;MP&#39;, 3, 86.5 &#39;CA&#39;, &#39;MP&#39;, 3, 84.5 &#39;CA&#39;, &#39;MP&#39;, 3, 83 &#39;CA&#39;, &#39;MP&#39;, 3, 83 &#39;CA&#39;, &#39;MP&#39;, 3, 82 &#39;CA&#39;, &#39;MP&#39;, 3, 78 &#39;CA&#39;, &#39;MP&#39;, 3, 75 &#39;CA&#39;, &#39;MP&#39;, 3, 74.5 &#39;CA&#39;, &#39;MP&#39;, 3, 70 &#39;CA&#39;, &#39;MP&#39;, 3, 54.5 &#39;CA&#39;, &#39;MP&#39;, 3, 52 &#39;CA&#39;, &#39;MP&#39;, 3, 50 &#39;CA&#39;, &#39;MP&#39;, 3, 42 &#39;CA&#39;, &#39;MP&#39;, 3, 36.5 &#39;CA&#39;, &#39;MP&#39;, 3, 28 &#39;CA&#39;, &#39;MP&#39;, 3, 26 &#39;CA&#39;, &#39;MP&#39;, 3, 11 &#39;CA&#39;, &#39;SN&#39;, 4, 103 &#39;CA&#39;, &#39;SN&#39;, 4, 103 &#39;CA&#39;, &#39;SN&#39;, 4, 102 &#39;CA&#39;, &#39;SN&#39;, 4, 102 &#39;CA&#39;, &#39;SN&#39;, 4, 101 &#39;CA&#39;, &#39;SN&#39;, 4, 100 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 95 &#39;CA&#39;, &#39;SN&#39;, 4, 95 &#39;CA&#39;, &#39;SN&#39;, 4, 92.5 &#39;CA&#39;, &#39;SN&#39;, 4, 92 &#39;CA&#39;, &#39;SN&#39;, 4, 91 &#39;CA&#39;, &#39;SN&#39;, 4, 90 &#39;CA&#39;, &#39;SN&#39;, 4, 85.5 &#39;CA&#39;, &#39;SN&#39;, 4, 84 &#39;CA&#39;, &#39;SN&#39;, 4, 82.5 &#39;CA&#39;, &#39;SN&#39;, 4, 81 &#39;CA&#39;, &#39;SN&#39;, 4, 77.5 &#39;CA&#39;, &#39;SN&#39;, 4, 77 &#39;CA&#39;, &#39;SN&#39;, 4, 72 &#39;CA&#39;, &#39;SN&#39;, 4, 71.5 &#39;CA&#39;, &#39;SN&#39;, 4, 69 &#39;CA&#39;, &#39;SN&#39;, 4, 68.5 &#39;CA&#39;, &#39;SN&#39;, 4, 68 &#39;CA&#39;, &#39;SN&#39;, 4, 67 &#39;CA&#39;, &#39;SN&#39;, 4, 65.5 &#39;CA&#39;, &#39;SN&#39;, 4, 62.5 &#39;CA&#39;, &#39;SN&#39;, 4, 62 &#39;CA&#39;, &#39;SN&#39;, 4, 61.5 &#39;CA&#39;, &#39;SN&#39;, 4, 61 &#39;CA&#39;, &#39;SN&#39;, 4, 57.5 &#39;CA&#39;, &#39;SN&#39;, 4, 54 &#39;CA&#39;, &#39;SN&#39;, 4, 52.5 &#39;CA&#39;, &#39;SN&#39;, 4, 51 &#39;CA&#39;, &#39;SN&#39;, 4, 50.5 &#39;CA&#39;, &#39;SN&#39;, 4, 50 &#39;CA&#39;, &#39;SN&#39;, 4, 49 &#39;CA&#39;, &#39;SN&#39;, 4, 43 &#39;CA&#39;, &#39;SN&#39;, 4, 39.5 &#39;CA&#39;, &#39;SN&#39;, 4, 32.5 &#39;CA&#39;, &#39;SN&#39;, 4, 25.5 &#39;CA&#39;, &#39;SN&#39;, 4, 18&quot; csv_test_data = read.csv(text=txt_test_data, header=TRUE) ar_st_varnames &lt;- c(&#39;first_half_professor&#39;, &#39;second_half_professor&#39;, &#39;course_id&#39;, &#39;exam_score&#39;) tb_test_data &lt;- as_tibble(csv_test_data) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_test_data) ## first_half_professor second_half_professor course_id exam_score ## Length:157 Length:157 Min. :1.000 Min. : 4.00 ## Class :character Class :character 1st Qu.:1.000 1st Qu.: 60.00 ## Mode :character Mode :character Median :2.000 Median : 82.00 ## Mean :2.465 Mean : 75.08 ## 3rd Qu.:4.000 3rd Qu.: 94.00 ## Max. :4.000 Max. :105.00 2.4.1.2 Test Score Distributions 2.4.1.2.1 Histogram ggplot(tb_final_twovar, aes(x=ar_test_scores_ec3)) + geom_histogram(bins=25) + labs(title = paste0(&#39;Sandbox: Final Distribution (Econ 2370, FW)&#39;), caption = paste0(&#39;FW Section, formula:&#39;, &#39;0.3*exam1Perc + 0.3*exam2Perc + &#39;, &#39;0.42*HWtotalPerc + 0.03*AttendancePerc \\n&#39;, &#39;+ perfect attendance + 0.03 per Extra Credit&#39;)) + theme_bw() ggplot(tb_test_data, aes(x=exam_score)) + geom_histogram(bins=16) + labs(title = paste0(&#39;Exam Distribution&#39;), caption = &#39;All Sections&#39;) + theme_bw() 2.5 Summarize Multiple Variables 2.5.1 Generate Replace Variables Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 2.5.1.1 Replace NA for Multiple Variables Replace some variables NA by some values, and other variables NAs by other values. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;date&#39; # NA dataframe df_NA &lt;- as_tibble(matrix(NA, nrow=it_N, ncol=it_M)) %&gt;% rowid_to_column(var = svr_id) %&gt;% rename_at(vars(starts_with(&quot;V&quot;)), funs(str_replace(., &quot;V&quot;, &quot;var&quot;))) kable(df_NA) %&gt;% kable_styling_fc() date var1 var2 var3 var4 var5 1 NA NA NA NA NA 2 NA NA NA NA NA 3 NA NA NA NA NA # Replace NA df_NA_replace &lt;- df_NA %&gt;% mutate_at(vars(one_of(c(&#39;var1&#39;, &#39;var2&#39;))), list(~replace_na(., 0))) %&gt;% mutate_at(vars(one_of(c(&#39;var3&#39;, &#39;var5&#39;))), list(~replace_na(., 99))) kable(df_NA_replace) %&gt;% kable_styling_fc() date var1 var2 var3 var4 var5 1 0 0 99 NA 99 2 0 0 99 NA 99 3 0 0 99 NA 99 2.5.1.2 Cumulative Sum Multiple Variables Each row is a different date, each column is the profit a firms earns on a date, we want to compute cumulatively how much a person is earning. Also renames variable names below jointly. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;date&#39; # random dataframe, daily profit of firms # dp_fx: daily profit firm ID something set.seed(123) df_daily_profit &lt;- as_tibble(matrix(rnorm(it_N*it_M), nrow=it_N, ncol=it_M)) %&gt;% rowid_to_column(var = svr_id) %&gt;% rename_at(vars(starts_with(&quot;V&quot;)), funs(str_replace(., &quot;V&quot;, &quot;dp_f&quot;))) kable(df_daily_profit) %&gt;% kable_styling_fc() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 # cumulative sum with suffix df_cumu_profit_suffix &lt;- df_daily_profit %&gt;% mutate_at(vars(contains(&#39;dp_f&#39;)), .funs = list(cumu = ~cumsum(.))) kable(df_cumu_profit_suffix) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 dp_f1_cumu dp_f2_cumu dp_f3_cumu dp_f4_cumu dp_f5_cumu 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 -0.7906531 0.1997961 -0.8041450 0.7784198 0.5114542 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 0.7680552 1.9148611 -1.4909979 1.1382337 -0.0443870 # cumulative sum variables naming to prefix df_cumu_profit &lt;- df_cumu_profit_suffix %&gt;% rename_at(vars(contains( &quot;_cumu&quot;) ), list(~paste(&quot;cp_f&quot;, gsub(&quot;_cumu&quot;, &quot;&quot;, .), sep = &quot;&quot;))) %&gt;% rename_at(vars(contains( &quot;cp_f&quot;) ), list(~gsub(&quot;dp_f&quot;, &quot;&quot;, .))) kable(df_cumu_profit) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 cp_f1 cp_f2 cp_f3 cp_f4 cp_f5 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 -0.7906531 0.1997961 -0.8041450 0.7784198 0.5114542 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 0.7680552 1.9148611 -1.4909979 1.1382337 -0.0443870 "],["functions.html", "Chapter 3 Functions 3.1 Dataframe Mutate 3.2 Dataframe Do Anything 3.3 Apply and pmap", " Chapter 3 Functions 3.1 Dataframe Mutate 3.1.1 Row Input Functions Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). We want evaluate nonlinear function f(Q_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. We would like to evaluate this nonlinear function concurrently across \\(N\\) individuals. The eventual goal is to find the \\(i\\) specific \\(Q\\) that solves the nonlinear equations. This is a continuation of R use Apply, Sapply and dplyr Mutate to Evaluate one Function Across Rows of a Matrix 3.1.1.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) ar_nN_N_choice = seq(1,it_N_child_cnt)/sum(seq(1,it_N_child_cnt)) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha, ar_nN_N_choice) # Convert Matrix to Tibble ar_st_col_names = c(&#39;fl_A&#39;, &#39;fl_alpha&#39;, &#39;fl_N&#39;) tb_nN_by_nQ_A_alpha &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rename_all(~c(ar_st_col_names)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha fl_N -2 0.1 0.0666667 -1 0.3 0.1333333 0 0.5 0.2000000 1 0.7 0.2666667 2 0.9 0.3333333 3.1.1.2 Mutate over Simple Function For this example, use a very simple function with only one type of input, all inputs are scalars. # Define Implicit Function ffi_nonlinear &lt;- function(fl_A, fl_alpha){ fl_out &lt;- (fl_A + fl_alpha*fl_A)/(fl_A)^2 return(fl_out) } Apply the function over the dataframe, note five different ways below, the third way allows for parameters to be strings. # variable names svr_fl_A &lt;- &#39;fl_A&#39; svr_fl_alpha &lt;- &#39;fl_alpha&#39; # Evaluate tb_nN_by_nQ_A_alpha_mutate_rows &lt;- tb_nN_by_nQ_A_alpha %&gt;% mutate(fl_out_m1 = ffi_nonlinear(fl_A=.$fl_A, fl_alpha=.$fl_alpha), fl_out_m2 = ffi_nonlinear(fl_A=`$`(., &#39;fl_A&#39;), fl_alpha=`$`(., &#39;fl_alpha&#39;)), fl_out_m3 = ffi_nonlinear(fl_A=.[[svr_fl_A]], fl_alpha=.[[svr_fl_alpha]]), fl_out_m4 = ffi_nonlinear(fl_A=fl_A, fl_alpha=fl_alpha), fl_out_m5 = ffi_nonlinear(fl_A, fl_alpha)) # print kable(tb_nN_by_nQ_A_alpha_mutate_rows) %&gt;% kable_styling_fc() fl_A fl_alpha fl_N fl_out_m1 fl_out_m2 fl_out_m3 fl_out_m4 fl_out_m5 -2 0.1 0.0666667 -0.55 -0.55 -0.55 -0.55 -0.55 -1 0.3 0.1333333 -1.30 -1.30 -1.30 -1.30 -1.30 0 0.5 0.2000000 NaN NaN NaN NaN NaN 1 0.7 0.2666667 1.70 1.70 1.70 1.70 1.70 2 0.9 0.3333333 0.95 0.95 0.95 0.95 0.95 3.1.1.3 Testing Function with Scalar and Arrays Test non-linear Equation. # Test Parameters fl_N_agg = 100 fl_rho = -1 fl_N_q = ar_nN_N_choice[4]*fl_N_agg ar_A_alpha = mt_nN_by_nQ_A_alpha[4,] # Apply Function ar_p1_s1 = exp((ar_A_alpha[1] - ar_nN_A)*fl_rho) ar_p1_s2 = (ar_A_alpha[2]/ar_nN_alpha) ar_p1_s3 = (1/(ar_nN_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N_q^((ar_A_alpha[2]*fl_rho-1)/(ar_nN_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) print(fl_overall) ## [1] -598.2559 Implement the non-linear problems evaluation using apply over all \\(N\\) individuals. # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha # # Test Parameters # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } # Parameters fl_rho = -1 # Evaluate Function print(ffi_nonlin_dplyrdo(mt_nN_by_nQ_A_alpha[1,1], mt_nN_by_nQ_A_alpha[1,2], mt_nN_by_nQ_A_alpha[1,3]*fl_N_agg, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) ## [1] 81.86645 for (i in seq(1,dim(mt_nN_by_nQ_A_alpha)[1])){ fl_eval = ffi_nonlin_dplyrdo(mt_nN_by_nQ_A_alpha[i,1], mt_nN_by_nQ_A_alpha[i,2], mt_nN_by_nQ_A_alpha[i,3]*fl_N_agg, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho) print(fl_eval) } ## [1] 81.86645 ## [1] 54.48885 ## [1] -65.5619 ## [1] -598.2559 ## [1] -3154.072 3.1.1.4 Evaluate Nonlinear Function using dplyr mutate # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # Test Parameters # ar_A = ar_nN_A # ar_alpha = ar_nN_alpha # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = (fl_N*fl_N_agg)^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_nN_by_nQ_A_alpha = tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha fl_N dplyr_eval -2 0.1 0.0666667 81.86645 -1 0.3 0.1333333 54.48885 0 0.5 0.2000000 -65.56190 1 0.7 0.2666667 -598.25595 2 0.9 0.3333333 -3154.07226 3.1.2 Evaluate Choices Across States Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). See the ff_opti_bisect_pmap_multi function from Fans REconTools Package, which provides a resuable function based on the algorithm worked out here. We want evaluate linear function \\(0=f(z_{ij}, x_i, y_i, \\textbf{X}, \\textbf{Y}, c, d)\\). There are \\(i\\) functions that have \\(i\\) specific \\(x\\) and \\(y\\). For each \\(i\\) function, we evaluate along a grid of feasible values for \\(z\\), over \\(j\\in J\\) grid points, potentially looking for the \\(j\\) that is closest to the root. \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are arrays common across the \\(i\\) equations, and \\(c\\) and \\(d\\) are constants. The evaluation strategy is the following, given min and max for \\(z\\) that are specific for each \\(j\\), and given common number of grid points, generate a matrix of \\(z_{ij}\\). Suppose there the number of \\(i\\) is \\(I\\), and the number of grid points for \\(j\\) is \\(J\\). Generate a \\(J \\cdot I\\) by \\(3\\) matrix where the columns are \\(z,x,y\\) as tibble Follow this Mutate to evaluate the \\(f(\\cdot)\\) function. Add two categorical columns for grid levels and wich \\(i\\), \\(i\\) and \\(j\\) index. Plot Mutate output evaluated column categorized by \\(i\\) as color and \\(j\\) as x-axis. 3.1.2.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] Now we need to expand this by the number of choice grid. Each row, representing one equation, is expanded by the number of choice grids. We are graphically searching, or rather brute force searching, which means if we have 100 individuals, we want to plot out the nonlinear equation for each of these lines, and show graphically where each line crosses zero. We achieve this, by evaluating the equation for each of the 100 individuals along a grid of feasible choices. In this problem here, the feasible choices are shared across individuals. # Parameters fl_rho = 0.20 svr_id_var = &#39;INDI_ID&#39; # it_child_count = N, the number of children it_N_child_cnt = 4 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # Choice Grid for nutritional feasible choices for each fl_N_agg = 100 fl_N_min = 0 it_N_choice_cnt_ttest = 3 it_N_choice_cnt_dense = 100 ar_N_choices_ttest = seq(fl_N_min, fl_N_agg, length.out = it_N_choice_cnt_ttest) ar_N_choices_dense = seq(fl_N_min, fl_N_agg, length.out = it_N_choice_cnt_dense) # Mesh Expand tb_states_choices &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rowid_to_column(var=svr_id_var) tb_states_choices_ttest &lt;- tb_states_choices %&gt;% expand_grid(choices = ar_N_choices_ttest) tb_states_choices_dense &lt;- tb_states_choices %&gt;% expand_grid(choices = ar_N_choices_dense) # display summary(tb_states_choices_dense) ## INDI_ID ar_nN_A ar_nN_alpha choices ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 0 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 25 ## Median :2.50 Median : 0 Median :0.5 Median : 50 ## Mean :2.50 Mean : 0 Mean :0.5 Mean : 50 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.: 75 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :100 kable(tb_states_choices_ttest) %&gt;% kable_styling_fc() INDI_ID ar_nN_A ar_nN_alpha choices 1 -2.0000000 0.1000000 0 1 -2.0000000 0.1000000 50 1 -2.0000000 0.1000000 100 2 -0.6666667 0.3666667 0 2 -0.6666667 0.3666667 50 2 -0.6666667 0.3666667 100 3 0.6666667 0.6333333 0 3 0.6666667 0.6333333 50 3 0.6666667 0.6333333 100 4 2.0000000 0.9000000 0 4 2.0000000 0.9000000 50 4 2.0000000 0.9000000 100 3.1.2.2 Apply Same Function all Rows, Some Inputs Row-specific, other Shared There are two types of inputs, row-specific inputs, and inputs that should be applied for each row. The Function just requires all of these inputs, it does not know what is row-specific and what is common for all row. Dplyr recognizes which parameter inputs already existing in the piped dataframe/tibble, given rowwise, those will be row-specific inputs. Additional function parameters that do not exist in dataframe as variable names, but that are pre-defined scalars or arrays will be applied to all rows. (param?) string variable name of input where functions are evaluated, these are already contained in the dataframe, existing variable names, row specific, rowwise computation over these, each rowwise calculation using different rows: fl_A, fl_alpha, fl_N (param?) scalar and array values that are applied to every rowwise calculation, all rowwise calculations using the same scalars and arrays:ar_A, ar_alpha, fl_N_agg, fl_rho (param?) string output variable name The function looks within group, finds min/max etc that are relevant. 3.1.2.2.1 3 Points and Denser Dataframs and Define Function # Convert Matrix to Tibble ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_states_choices &lt;- tb_states_choices %&gt;% rename_all(~c(ar_st_col_names)) ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;, &#39;fl_N&#39;) tb_states_choices_ttest &lt;- tb_states_choices_ttest %&gt;% rename_all(~c(ar_st_col_names)) tb_states_choices_dense &lt;- tb_states_choices_dense %&gt;% rename_all(~c(ar_st_col_names)) # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # scalar value that are row-specific, in dataframe already: *fl_A*, *fl_alpha*, *fl_N* # array and scalars not in dataframe, common all rows: *ar_A*, *ar_alpha*, *fl_N_agg*, *fl_rho* # Test Parameters # ar_A = ar_nN_A # ar_alpha = ar_nN_alpha # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } 3.1.2.2.2 Evaluate at Three Choice Points and Show Table In the example below, just show results evaluating over three choice points and show table. # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_states_choices_ttest_eval = tb_states_choices_ttest %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show kable(tb_states_choices_ttest_eval) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha fl_N dplyr_eval 1 -2.0000000 0.1000000 0 100.00000 1 -2.0000000 0.1000000 50 -5666.95576 1 -2.0000000 0.1000000 100 -12880.28392 2 -0.6666667 0.3666667 0 100.00000 2 -0.6666667 0.3666667 50 -595.73454 2 -0.6666667 0.3666667 100 -1394.70698 3 0.6666667 0.6333333 0 100.00000 3 0.6666667 0.6333333 50 -106.51058 3 0.6666667 0.6333333 100 -323.94216 4 2.0000000 0.9000000 0 100.00000 4 2.0000000 0.9000000 50 22.55577 4 2.0000000 0.9000000 100 -51.97161 3.1.2.2.3 Evaluate at Many Choice Points and Show Graphically Same as above, but now we evaluate the function over the individuals at many choice points so that we can graph things out. # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_states_choices_dense_eval = tb_states_choices_dense %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Labeling st_title &lt;- paste0(&#39;Evaluate Non-Linear Functions to Search for Roots&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/function/mutatef/htmlpdfr/fs_func_choice_states.html&#39;) st_caption &lt;- paste0(&#39;Evaluating the function, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;x values&#39; st_y_label &lt;- &#39;f(x)&#39; # Show dim(tb_states_choices_dense_eval) ## [1] 400 5 summary(tb_states_choices_dense_eval) ## INDI_ID fl_A fl_alpha fl_N dplyr_eval ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 0 Min. :-12880.28 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 25 1st Qu.: -1167.29 ## Median :2.50 Median : 0 Median :0.5 Median : 50 Median : -202.42 ## Mean :2.50 Mean : 0 Mean :0.5 Mean : 50 Mean : -1645.65 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.: 75 3rd Qu.: 0.96 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :100 Max. : 100.00 lineplot &lt;- tb_states_choices_dense_eval %&gt;% ggplot(aes(x=fl_N, y=dplyr_eval)) + geom_line() + facet_wrap( . ~ INDI_ID, scales = &quot;free&quot;) + geom_hline(yintercept=0, linetype=&quot;dashed&quot;, color = &quot;red&quot;, size=1) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) print(lineplot) 3.2 Dataframe Do Anything 3.2.1 (Mx1 by N) to (MxQ by N+1) Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Case One: There is a dataframe with \\(M\\) rows, based on these \\(m\\) specific information, generate dataframes for each \\(m\\). Stack these indivdiual dataframes together and merge original \\(m\\) specific information in as well. The number of rows for each \\(m\\) is \\(Q_m\\), each \\(m\\) could have different number of expansion rows. Generate a panel with \\(M\\) individuals, each individual is observed for different spans of times (uncount). Before expanding, generate individual specific normal distribution standard deviation. All individuals share the same mean, but have increasing standard deviations. 3.2.1.1 Generate Dataframe with M Rows. This is the first step, generate \\(M\\) rows of data, to be expanded. Each row contains the number of normal draws to make and the mean and the standard deviation for normal daraws that are \\(m\\) specific. # Parameter Setups it_M &lt;- 3 it_Q_max &lt;- 5 fl_rnorm_mu &lt;- 1000 ar_rnorm_sd &lt;- seq(0.01, 200, length.out=it_M) ar_it_q &lt;- sample.int(it_Q_max, it_M, replace=TRUE) # N by Q varying parameters mt_data = cbind(ar_it_q, ar_rnorm_sd) tb_M &lt;- as_tibble(mt_data) %&gt;% rowid_to_column(var = &quot;ID&quot;) %&gt;% rename(sd = ar_rnorm_sd, Q = ar_it_q) %&gt;% mutate(mean = fl_rnorm_mu) # display kable(tb_M) %&gt;% kable_styling_fc() ID Q sd mean 1 1 0.010 1000 2 3 100.005 1000 3 4 200.000 1000 3.2.1.2 Random Normal Draw Expansion The steps are: do anything use .$ sign to refer to variable names, or [[name]] unnest left_join expanded and original Note these all give the same results Use dot dollar to get variables # Generate $Q_m$ individual specific incomes, expanded different number of times for each m tb_income &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.$Q, mean=.$mean, sd=.$sd)) %&gt;% unnest(c(income)) # Merge back with tb_M tb_income_full_dd &lt;- tb_income %&gt;% left_join(tb_M) # display kable(tb_income) %&gt;% kable_styling_fc() ID income 1 999.9803 2 1070.1391 2 952.7185 2 893.2123 3 956.4050 3 794.7991 3 854.2218 3 874.9921 kable(tb_income_full_dd) %&gt;% kable_styling_fc() ID income Q sd mean 1 999.9803 1 0.010 1000 2 1070.1391 3 100.005 1000 2 952.7185 3 100.005 1000 2 893.2123 3 100.005 1000 3 956.4050 4 200.000 1000 3 794.7991 4 200.000 1000 3 854.2218 4 200.000 1000 3 874.9921 4 200.000 1000 3.2.2 (MxP by N) to (Mx1 by 1) Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). There is a Panel with \\(M\\) individuals and each individual has \\(Q\\) records/rows. A function generate an individual specific outcome given the \\(Q\\) individual specific inputs, along with shared parameters and arrays across the \\(M\\) individuals. For example, suppose we have a dataframe of individual wage information from different countries, each row is an individual from one country. We want to generate country specific gini based on the individual data for each country in the dataframe. But additionally, perhaps the gini formula requires not just individual income but some additional parameters or shared dataframes as inputs. Given the within \\(m\\) income observations, we can compute gini statistics that are individual specific based on the observed distribution of incomes. For this, we will use the ff_dist_gini_vector_pos.html function from REconTools. To make this more interesting, we will generate large dataframe with more \\(M\\) and more \\(Q\\) each \\(m\\). 3.2.2.1 Income Rows for Individuals in Many Groups There are up to ten thousand income observation per person. And there are ten people. # Parameter Setups it_M &lt;- 10 it_Q_max &lt;- 10000 fl_rnorm_mu &lt;- 1 ar_rnorm_sd &lt;- seq(0.01, 0.2, length.out=it_M) ar_it_q &lt;- sample.int(it_Q_max, it_M, replace=TRUE) # N by Q varying parameters mt_data = cbind(ar_it_q, ar_rnorm_sd) tb_M &lt;- as_tibble(mt_data) %&gt;% rowid_to_column(var = &quot;ID&quot;) %&gt;% rename(sd = ar_rnorm_sd, Q = ar_it_q) %&gt;% mutate(mean = fl_rnorm_mu) 3.2.2.2 Compute Group Specific Gini There is only one input for the gini function ar_pos. Note that the gini are not very large even with large SD, because these are normal distributions. By Construction, most peple are in the middle. So with almost zero standard deviation, we have perfect equality, as standard deviation increases, inequality increases, but still pretty equal overall, there is no fat upper tail. Note that there are three ways of referring to variable names with dot, which are all shown below: We can explicitly refer to names We can use the dollar dot structure to use string variable names in do anything. We can use dot bracket, this is the only option that works with string variable names First: Generate individual group all incomes: # A. Normal Draw Expansion, Explicitly Name set.seed(&#39;123&#39;) tb_income_norm_dot_dollar &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.$Q, mean=.$mean, sd=.$sd)) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # Normal Draw Expansion again, dot dollar differently with string variable name set.seed(&#39;123&#39;) tb_income_norm_dollar_dot &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(`$`(., &#39;Q&#39;), mean = `$`(., &#39;mean&#39;), sd = `$`(., &#39;sd&#39;))) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # Normal Draw Expansion again, dot double bracket set.seed(&#39;123&#39;) svr_mean &lt;- &#39;mean&#39; svr_sd &lt;- &#39;sd&#39; svr_Q &lt;- &#39;Q&#39; tb_income_norm_dot_bracket_db &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.[[svr_Q]], mean = .[[svr_mean]], sd = .[[svr_sd]])) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # display print(dim(tb_income_norm_dot_bracket_db)) ## [1] 59429 5 kable(head(tb_income_norm_dot_bracket_db, 20)) %&gt;% kable_styling_fc() ID income Q sd mean 1 0.9943952 3004 0.01 1 1 0.9976982 3004 0.01 1 1 1.0155871 3004 0.01 1 1 1.0007051 3004 0.01 1 1 1.0012929 3004 0.01 1 1 1.0171506 3004 0.01 1 1 1.0046092 3004 0.01 1 1 0.9873494 3004 0.01 1 1 0.9931315 3004 0.01 1 1 0.9955434 3004 0.01 1 1 1.0122408 3004 0.01 1 1 1.0035981 3004 0.01 1 1 1.0040077 3004 0.01 1 1 1.0011068 3004 0.01 1 1 0.9944416 3004 0.01 1 1 1.0178691 3004 0.01 1 1 1.0049785 3004 0.01 1 1 0.9803338 3004 0.01 1 1 1.0070136 3004 0.01 1 1 0.9952721 3004 0.01 1 Second, compute gini: # Gini by Group tb_gini_norm &lt;- tb_income_norm_dot_bracket_db %&gt;% group_by(ID) %&gt;% do(inc_gini_norm = ff_dist_gini_vector_pos(.$income)) %&gt;% unnest(c(inc_gini_norm)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # display kable(tb_gini_norm) %&gt;% kable_styling_fc() ID inc_gini_norm Q sd mean 1 0.0056006 3004 0.0100000 1 2 0.0174893 3207 0.0311111 1 3 0.0295527 7989 0.0522222 1 4 0.0412807 3995 0.0733333 1 5 0.0537107 8358 0.0944444 1 6 0.0650354 217 0.1155556 1 7 0.0766718 9506 0.1366667 1 8 0.0891009 8157 0.1577778 1 9 0.1014251 6216 0.1788889 1 10 0.1135054 8780 0.2000000 1 3.2.3 (MxP by N) to (MxQ by N+Z) Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). There is a dataframe composed of M mini-dataframes. Group by a variable that identifies each unique sub-dataframe, and use the sub-dataframes with P rows as inputs to a function. The function outputs Q by Z rows and columns of results, stack the results. The output file has MxQ rows and the Z columns of additional results should be appended. 3.2.3.1 Generate the MxP by N Dataframe M Grouping characteristics, P rows for each group, and N Variables. M are individuals P are dates A wage variable for individual wage at each date. And a savings varaible as well. # Define it_M &lt;- 3 it_P &lt;- 5 svr_m &lt;- &#39;group_m&#39; svr_mp &lt;- &#39;info_mp&#39; # dataframe set.seed(123) df_panel_skeleton &lt;- as_tibble(matrix(it_P, nrow=it_M, ncol=1)) %&gt;% rowid_to_column(var = svr_m) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_m)) %&gt;% mutate(!!sym(svr_mp) := row_number()) %&gt;% ungroup() %&gt;% rowwise() %&gt;% mutate(wage = rnorm(1, 100, 10), savings = rnorm(1, 200, 30)) %&gt;% ungroup() %&gt;% rowid_to_column(var = &quot;id_ji&quot;) # Print kable(df_panel_skeleton) %&gt;% kable_styling_fc() id_ji group_m info_mp wage savings 1 1 1 94.39524 253.6074 2 1 2 97.69823 214.9355 3 1 3 115.58708 141.0015 4 1 4 100.70508 221.0407 5 1 5 101.29288 185.8163 6 2 1 117.15065 167.9653 7 2 2 104.60916 193.4608 8 2 3 87.34939 169.2199 9 2 4 93.13147 178.1333 10 2 5 95.54338 181.2488 11 3 1 112.24082 149.3992 12 3 2 103.59814 225.1336 13 3 3 104.00771 204.6012 14 3 4 101.10683 165.8559 15 3 5 94.44159 237.6144 3.2.3.2 Subgroup Compute and Expand Use the M sub-dataframes, generate Q by Z result for each of the M groups. Stack all results together. Base on all the wages for each individual, generate individual specific mean and standard deviations. Do this for three things, the wage variable, the savings variable, and the sum of wage and savings: Z=2: 2 columns, mean and standard deviation Q=3: 3 rows, statistics based on wage, savings, and the sum of both First, here is the processing function that takes the dataframe as input, with a parameter for rounding: # define function ffi_subset_mean_sd &lt;- function(df_sub, it_round=1) { #&#39; A function that generates mean and sd for several variables #&#39; #&#39; @description #&#39; Assume there are two variables in df_sub wage and savings #&#39; #&#39; @param df_sub dataframe where each individual row is a different #&#39; data point, over which we compute mean and sd, Assum there are two #&#39; variables, savings and wage #&#39; @param it_round integer rounding for resulting dataframe #&#39; @return a dataframe where each row is aggregate for a different type #&#39; of variablea and each column is a different statistics fl_wage_mn = mean(df_sub$wage) fl_wage_sd = sd(df_sub$wage) fl_save_mn = mean(df_sub$savings) fl_save_sd = sd(df_sub$savings) fl_wgsv_mn = mean(df_sub$wage + df_sub$savings) fl_wgsv_sd = sd(df_sub$wage + df_sub$savings) ar_mn &lt;- c(fl_wage_mn, fl_save_mn, fl_wgsv_mn) ar_sd &lt;- c(fl_wage_sd, fl_save_sd, fl_wgsv_sd) ar_st_row_lab &lt;- c(&#39;wage&#39;, &#39;savings&#39;, &#39;wage_and_savings&#39;) mt_stats &lt;- cbind(ar_mn, ar_sd) mt_stats &lt;- round(mt_stats, it_round) ar_st_varnames &lt;- c(&#39;mean&#39;, &#39;sd&#39;, &#39;variables&#39;) df_combine &lt;- as_tibble(mt_stats) %&gt;% add_column(ar_st_row_lab) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% select(variables, &#39;mean&#39;, &#39;sd&#39;) %&gt;% rowid_to_column(var = &quot;id_q&quot;) return(df_combine) } # testing function ffi_subset_mean_sd(df_panel_skeleton %&gt;% filter(!!sym(svr_m)==1)) Second, call ffi_subset_mean_sd function for each of the groups indexed by j and stack results together with j index: group by call function unnest # run group stats and stack dataframes df_outputs &lt;- df_panel_skeleton %&gt;% group_by(!!sym(svr_m)) %&gt;% do(df_stats = ffi_subset_mean_sd(., it_round=2)) %&gt;% unnest() %&gt;% rowid_to_column(var = &quot;id_mq&quot;) # print kable(df_outputs) %&gt;% kable_styling_fc() id_mq group_m id_q variables mean sd 1 1 1 wage 101.94 8.11 2 1 2 savings 203.28 42.33 3 1 3 wage_and_savings 305.22 34.83 4 2 1 wage 99.56 11.63 5 2 2 savings 178.01 10.34 6 2 3 wage_and_savings 277.56 15.48 7 3 1 wage 103.08 6.39 8 3 2 savings 196.52 37.86 9 3 3 wage_and_savings 299.60 33.50 In the resulting file, we went from a matrix with MxP rows to a matrix with MxQ Rows. 3.3 Apply and pmap 3.3.1 Apply and Sapply Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). r apply matrix to function row by row r evaluate function on grid Apply a function to every row of a matrix or a data frame r apply r sapply sapply over matrix row by row function as parameters using formulas do We want evaluate linear function f(x_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. More specifically, we have a functions, this function takes inputs that are individual specific. We would like to evaluate this function concurrently across \\(N\\) individuals. The function is such that across the \\(N\\) individuals, some of the function parameter inputs are the same, but others are different. If we are looking at demand for a particular product, the prices of all products enter the demand equation for each product, but the products own price enters also in a different way. The objective is either to just evaluate this function across \\(N\\) individuals, or this is a part of a nonlinear solution system. What is the relationship between apply, lapply and vectorization? see Is the *apply family really not vectorized?. 3.3.1.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are # heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # display kable(mt_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 3.3.1.2 Using apply 3.3.1.2.1 Named Function First we use the apply function, we have to hard-code the arrays that are fixed for each of the \\(N\\) individuals. Then apply allows us to loop over the matrix that is \\(N\\) by \\(Q\\), each row one at a time, from \\(1\\) to \\(N\\). # Define Implicit Function ffi_linear_hardcode &lt;- function(ar_A_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha fl_out = sum(ar_A_alpha[1]*ar_nN_A + 1/(ar_A_alpha[2] + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row ar_func_apply = apply(mt_nN_by_nQ_A_alpha, 1, ffi_linear_hardcode) 3.3.1.2.2 Anonymous Function apply over matrix Apply with anonymous function generating a list of arrays of different lengths. In the example below, we want to drawn \\(N\\) sets of random uniform numbers, but for each set the number of draws we want to have is \\(Q_i\\). Furthermore, we want to rescale the random uniform draws so that they all become proportions that sum u pto one for each \\(i\\), but then we multply each rows values by the row specific aggregates. The anonymous function has hard coded parameters. Using an anonymous function here allows for parameters to be provided inside the function that are shared across each looped evaluation. This is perhaps more convenient than sapply with additional parameters. set.seed(1039) # Define the number of draws each row and total amount it_N &lt;- 4 fl_unif_min &lt;- 1 fl_unif_max &lt;- 2 mt_draw_define &lt;- cbind(sample(it_N, it_N, replace=TRUE), runif(it_N, min=1, max=10)) tb_draw_define &lt;- as_tibble(mt_draw_define) %&gt;% rowid_to_column(var = &quot;draw_group&quot;) print(tb_draw_define) # apply row by row, anonymous function has hard # coded min and max ls_ar_draws_shares_lvls = apply(tb_draw_define, 1, function(row) { it_draw &lt;- row[2] fl_sum &lt;- row[3] ar_unif &lt;- runif(it_draw, min=fl_unif_min, max=fl_unif_max) ar_share &lt;- ar_unif/sum(ar_unif) ar_levels &lt;- ar_share*fl_sum return(list(ar_share=ar_share, ar_levels=ar_levels)) }) # Show Results print(ls_ar_draws_shares_lvls) ## [[1]] ## [[1]]$ar_share ## [1] 0.2783638 0.2224140 0.2797840 0.2194381 ## ## [[1]]$ar_levels ## [1] 1.492414 1.192446 1.500028 1.176491 ## ## ## [[2]] ## [[2]]$ar_share ## [1] 0.5052919 0.4947081 ## ## [[2]]$ar_levels ## [1] 3.866528 3.785541 ## ## ## [[3]] ## [[3]]$ar_share ## [1] 1 ## ## [[3]]$ar_levels ## V2 ## 9.572211 ## ## ## [[4]] ## [[4]]$ar_share ## [1] 0.4211426 0.2909812 0.2878762 ## ## [[4]]$ar_levels ## [1] 4.051971 2.799640 2.769765 We will try to do the same thing as above, but now the output will be a stacked dataframe. Note that within each element of the apply row by row loop, we are generating two variables ar_share and ar_levels. We will not generate a dataframe with multiple columns, storing ar_share, ar_levels as well as information on min, max, number of draws and rescale total sum. set.seed(1039) # apply row by row, anonymous function has hard coded min and max ls_mt_draws_shares_lvls = apply(tb_draw_define, 1, function(row) { it_draw_group &lt;- row[1] it_draw &lt;- row[2] fl_sum &lt;- row[3] ar_unif &lt;- runif(it_draw, min=fl_unif_min, max=fl_unif_max) ar_share &lt;- ar_unif/sum(ar_unif) ar_levels &lt;- ar_share*fl_sum mt_all_res &lt;- cbind(it_draw_group, it_draw, fl_sum, ar_unif, ar_share, ar_levels) colnames(mt_all_res) &lt;- c(&#39;draw_group&#39;, &#39;draw_count&#39;, &#39;sum&#39;, &#39;unif_draw&#39;, &#39;share&#39;, &#39;rescale&#39;) rownames(mt_all_res) &lt;- NULL return(mt_all_res) }) mt_draws_shares_lvls_all &lt;- do.call(rbind, ls_mt_draws_shares_lvls) # Show Results kable(mt_draws_shares_lvls_all) %&gt;% kable_styling_fc() draw_group draw_count sum unif_draw share rescale 1 4 5.361378 1.125668 0.1988606 1.066167 1 4 5.361378 1.668536 0.2947638 1.580340 1 4 5.361378 1.419382 0.2507483 1.344356 1 4 5.361378 1.447001 0.2556274 1.370515 2 2 7.652069 1.484598 0.4605236 3.523959 2 2 7.652069 1.739119 0.5394764 4.128110 3 1 9.572211 1.952468 1.0000000 9.572211 4 3 9.621375 1.957931 0.3609352 3.472693 4 3 9.621375 1.926995 0.3552324 3.417824 4 3 9.621375 1.539678 0.2838324 2.730858 3.3.1.3 Using sapply 3.3.1.3.1 Named Function r convert matrix to list Convert a matrix to a list of vectors in R Sapply allows us to not have tohard code in the A and alpha arrays. But Sapply works over List or Vector, not Matrix. So we have to convert the \\(N\\) by \\(Q\\) matrix to a N element list Now update the function with sapply. ls_ar_nN_by_nQ_A_alpha = as.list(data.frame(t(mt_nN_by_nQ_A_alpha))) # Define Implicit Function ffi_linear_sapply &lt;- function(ar_A_alpha, ar_A, ar_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha fl_out = sum(ar_A_alpha[1]*ar_nN_A + 1/(ar_A_alpha[2] + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row ar_func_sapply = sapply(ls_ar_nN_by_nQ_A_alpha, ffi_linear_sapply, ar_A=ar_nN_A, ar_alpha=ar_nN_alpha) 3.3.1.3.2 Anonymous Function sapply anonymous function r anoymous function multiple lines Sapply with anonymous function generating a list of arrays of different lengths. In the example below, we want to drawn \\(N\\) sets of random uniform numbers, but for each set the number of draws we want to have is \\(Q_i\\). Furthermore, we want to rescale the random uniform draws so that they all become proportions that sum u pto one for each \\(i\\). it_N &lt;- 4 fl_unif_min &lt;- 1 fl_unif_max &lt;- 2 # Generate using runif without anonymous function set.seed(1039) ls_ar_draws = sapply(seq(it_N), runif, min=fl_unif_min, max=fl_unif_max) print(ls_ar_draws) ## [[1]] ## [1] 1.125668 ## ## [[2]] ## [1] 1.668536 1.419382 ## ## [[3]] ## [1] 1.447001 1.484598 1.739119 ## ## [[4]] ## [1] 1.952468 1.957931 1.926995 1.539678 # Generate Using Anonymous Function set.seed(1039) ls_ar_draws_shares = sapply(seq(it_N), function(n, min, max) { ar_unif &lt;- runif(n,min,max) ar_share &lt;- ar_unif/sum(ar_unif) return(ar_share) }, min=fl_unif_min, max=fl_unif_max) # Print Share print(ls_ar_draws_shares) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 0.5403432 0.4596568 ## ## [[3]] ## [1] 0.3098027 0.3178522 0.3723451 ## ## [[4]] ## [1] 0.2646671 0.2654076 0.2612141 0.2087113 # Sapply with anonymous function to check sums sapply(seq(it_N), function(x) {sum(ls_ar_draws[[x]])}) ## [1] 1.125668 3.087918 4.670717 7.377071 sapply(seq(it_N), function(x) {sum(ls_ar_draws_shares[[x]])}) ## [1] 1 1 1 1 3.3.1.4 Compare Results # Show overall Results mt_results &lt;- cbind(ar_func_apply, ar_func_sapply) colnames(mt_results) &lt;- c(&#39;eval_lin_apply&#39;, &#39;eval_lin_sapply&#39;) kable(mt_results) %&gt;% kable_styling_fc() eval_lin_apply eval_lin_sapply X1 2.346356 2.346356 X2 2.094273 2.094273 X3 1.895316 1.895316 X4 1.733708 1.733708 X5 1.599477 1.599477 3.3.2 Mutate Evaluate Functions Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Apply a function over rows of a matrix using mutate, rowwise, etc. 3.3.2.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are # heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # display kable(mt_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 # Convert Matrix to Tibble ar_st_col_names = c(&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_nN_by_nQ_A_alpha &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rename_all(~c(ar_st_col_names)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 3.3.2.2 mutate rowwise dplyr mutate own function dplyr all row function dplyr do function apply function each row dplyr applying a function to every row of a table using dplyr dplyr rowwise # Define Implicit Function ffi_linear_dplyrdo &lt;- function(fl_A, fl_alpha, ar_nN_A, ar_nN_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha print(paste0(&#39;cur row, fl_A=&#39;, fl_A, &#39;, fl_alpha=&#39;, fl_alpha)) fl_out = sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row of tibble # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_nN_by_nQ_A_alpha_show &lt;- tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_linear_dplyrdo(fl_A, fl_alpha, ar_nN_A, ar_nN_alpha)) ## [1] &quot;cur row, fl_A=-2, fl_alpha=0.1&quot; ## [1] &quot;cur row, fl_A=-1, fl_alpha=0.3&quot; ## [1] &quot;cur row, fl_A=0, fl_alpha=0.5&quot; ## [1] &quot;cur row, fl_A=1, fl_alpha=0.7&quot; ## [1] &quot;cur row, fl_A=2, fl_alpha=0.9&quot; # Show kable(tb_nN_by_nQ_A_alpha_show) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 same as before, still rowwise, but hard code some inputs: # Define function, fixed inputs are not parameters, but # defined earlier as a part of the function # ar_nN_A, ar_nN_alpha are fixed, not parameters ffi_linear_dplyrdo_func &lt;- function(fl_A, fl_alpha){ fl_out &lt;- sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row of tibble tbfunc_A_nN_by_nQ_A_alpha_rowwise = tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_linear_dplyrdo_func(fl_A, fl_alpha)) # Show kable(tbfunc_A_nN_by_nQ_A_alpha_rowwise) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.2.3 mutate with pmap Apparantly rowwise() is not a good idea, and pmap should be used, below is the pmap solution to the problem. Which does seem nicer. Crucially, dont have to define input parameter names, automatically I think they are matching up to the names in the function dplyr mutate pass function r function quosure string multiple r function multiple parameters as one string dplyr mutate anonymous function quosure style lambda pmap tibble rows dplyr pwalk # Define function, fixed inputs are not parameters, but defined # earlier as a part of the function Rorate fl_alpha and fl_A name # compared to before to make sure pmap tracks by names ffi_linear_dplyrdo_func &lt;- function(fl_alpha, fl_A){ fl_out &lt;- sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate a function row by row of dataframe, generate list, # then to vector tb_nN_by_nQ_A_alpha %&gt;% pmap(ffi_linear_dplyrdo_func) %&gt;% unlist() ## [1] 2.346356 2.094273 1.895316 1.733708 1.599477 # Same as above, but in line line and save output as new column # in dataframe note this ONLY works if the tibble only has variables # that are inputs for the function if tibble contains additional # variables, those should be droppd, or only the ones needed selected, # inside the pmap call below. tbfunc_A_nN_by_nQ_A_alpha_pmap &lt;- tb_nN_by_nQ_A_alpha %&gt;% mutate(dplyr_eval_pmap = unlist( pmap(tb_nN_by_nQ_A_alpha, ffi_linear_dplyrdo_func) ) ) # Show kable(tbfunc_A_nN_by_nQ_A_alpha_pmap) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval_pmap -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.2.4 rowwise and do Now, we have three types of parameters, for something like a bisection type calculation. We will supply the program with a function with some hard-coded value inside, and as parameters, we will have one parameter which is a row in the current matrix, and another parameter which is a sclar values. The three types of parameters are dealt with sparately: parameters that are fixed for all bisection iterations, but differ for each row these are hard-coded into the function parameters that are fixed for all bisection iterations, but are shared across rows these are the first parameter of the function, a list parameters that differ for each iteration, but differ acoss iterations second scalar value parameter for the function dplyr mutate function applow to each row dot notation note rowwise might be bad according to Hadley, should use pmap? ffi_linear_dplyrdo_fdot &lt;- function(ls_row, fl_param){ # Type 1 Param = ar_nN_A, ar_nN_alpha # Type 2 Param = ls_row$fl_A, ls_row$fl_alpha # Type 3 Param = fl_param fl_out &lt;- (sum(ls_row$fl_A*ar_nN_A + 1/(ls_row$fl_alpha + 1/ar_nN_alpha))) + fl_param return(fl_out) } cur_func &lt;- ffi_linear_dplyrdo_fdot fl_param &lt;- 0 dplyr_eval_flex &lt;- tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% do(dplyr_eval_flex = cur_func(., fl_param)) %&gt;% unnest(dplyr_eval_flex) tbfunc_B_nN_by_nQ_A_alpha &lt;- tb_nN_by_nQ_A_alpha %&gt;% add_column(dplyr_eval_flex) # Show kable(tbfunc_B_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval_flex -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.2.5 Compare Apply and Mutate Results # Show overall Results mt_results &lt;- cbind(tb_nN_by_nQ_A_alpha_show[&#39;dplyr_eval&#39;], tbfunc_A_nN_by_nQ_A_alpha_rowwise[&#39;dplyr_eval&#39;], tbfunc_A_nN_by_nQ_A_alpha_pmap[&#39;dplyr_eval_pmap&#39;], tbfunc_B_nN_by_nQ_A_alpha[&#39;dplyr_eval_flex&#39;], mt_nN_by_nQ_A_alpha) colnames(mt_results) &lt;- c(&#39;eval_dplyr_mutate&#39;, &#39;eval_dplyr_mutate_hcode&#39;, &#39;eval_dplyr_mutate_pmap&#39;, &#39;eval_dplyr_mutate_flex&#39;, &#39;A_child&#39;, &#39;alpha_child&#39;) kable(mt_results) %&gt;% kable_styling_fc_wide() eval_dplyr_mutate eval_dplyr_mutate_hcode eval_dplyr_mutate_pmap eval_dplyr_mutate_flex A_child alpha_child 2.346356 2.346356 2.346356 2.346356 -2 0.1 2.094273 2.094273 2.094273 2.094273 -1 0.3 1.895316 1.895316 1.895316 1.895316 0 0.5 1.733708 1.733708 1.733708 1.733708 1 0.7 1.599477 1.599477 1.599477 1.599477 2 0.9 "],["multi-dimensional-data-structures.html", "Chapter 4 Multi-dimensional Data Structures 4.1 Generate, Gather, Bind and Join 4.2 Wide and Long 4.3 Join and Compare", " Chapter 4 Multi-dimensional Data Structures 4.1 Generate, Gather, Bind and Join 4.1.1 Generate Panel Structure Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 4.1.1.1 Balanced Panel Skeleton There are \\(N\\) individuals, each could be observed \\(M\\) times. In the example below, there are 3 students, each observed over 4 dates. This just uses the uncount function from tidyr. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;student_id&#39; svr_date &lt;- &#39;class_day&#39; # dataframe df_panel_skeleton &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Print kable(df_panel_skeleton) %&gt;% kable_styling_fc() student_id class_day 1 1 1 2 1 3 1 4 1 5 2 1 2 2 2 3 2 4 2 5 3 1 3 2 3 3 3 4 3 5 4.1.1.2 Panel of Children with Height Growth Given \\(N\\) individuals, each with \\(G\\) observations. There is an initial height variable and height grows every year. There are growth variables, variables for cumulative growth and variables for height at each age for each child. Individuals are defined by gender (1 = female), race (1=asian), and birth height. Within individual yearly information includes height at each year of age. # Define it_N &lt;- 5 it_M &lt;- 3 svr_id &lt;- &#39;indi_id&#39; svr_gender &lt;- &#39;female&#39; svr_asian &lt;- &#39;asian&#39; svr_age &lt;- &#39;year_of_age&#39; # Define Height Related Variables svr_brthgt &lt;- &#39;birth_height&#39; svr_hgtgrow &lt;- &#39;hgt_growth&#39; svr_hgtgrow_cumu &lt;- &#39;hgt_growcumu&#39; svr_height &lt;- &#39;height&#39; # panel dataframe following set.seed(123) df_panel_indiage &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% mutate(!!sym(svr_gender) := rbinom(n(), 1, 0.5), !!sym(svr_asian) := rbinom(n(), 1, 0.5), !!sym(svr_brthgt) := rnorm(n(), mean=60,sd=3)) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_gender), !!sym(svr_asian), !!sym(svr_brthgt)) %&gt;% mutate(!!sym(svr_age) := row_number(), !!sym(svr_hgtgrow) := runif(n(), min=5, max=15), !!sym(svr_hgtgrow_cumu) := cumsum(!!sym(svr_hgtgrow)), !!sym(svr_height) := !!sym(svr_brthgt) + !!sym(svr_hgtgrow_cumu)) %&gt;% ungroup() # Add Height Index kable(df_panel_indiage) %&gt;% kable_styling_fc() female asian birth_height year_of_age hgt_growth hgt_growcumu height 0 0 65.14520 1 13.895393 13.895393 79.04059 0 0 65.14520 2 11.928034 25.823427 90.96862 0 0 65.14520 3 11.405068 37.228495 102.37369 1 1 61.38275 1 11.907053 11.907053 73.28980 1 1 61.38275 2 12.954674 24.861727 86.24448 1 1 61.38275 3 5.246137 30.107864 91.49061 0 1 56.20482 1 14.942698 14.942698 71.14751 0 1 56.20482 2 11.557058 26.499756 82.70457 0 1 56.20482 3 12.085305 38.585060 94.78988 1 1 57.93944 1 6.471137 6.471137 64.41058 1 1 57.93944 2 14.630242 21.101379 79.04082 1 1 57.93944 3 14.022991 35.124369 93.06381 1 0 58.66301 1 10.440660 10.440660 69.10367 1 0 58.66301 2 10.941420 21.382081 80.04509 1 0 58.66301 3 7.891597 29.273678 87.93669 4.1.1.3 Create Group IDs Given the dataframe just created, generate group IDs for each Gender and Race Groups. Given that both are binary, there can only be 4 unique groups. # group id svr_group_id &lt;- &#39;female_asian_id&#39; # Define ls_svr_group_vars &lt;- c(&#39;female&#39;, &#39;asian&#39;) # panel dataframe following df_panel_indiage_id &lt;- df_panel_indiage %&gt;% arrange(!!!syms(ls_svr_group_vars)) %&gt;% group_by(!!!syms(ls_svr_group_vars)) %&gt;% mutate(!!sym(svr_group_id) := (row_number()==1)*1) %&gt;% ungroup() %&gt;% mutate(!!sym(svr_group_id) := cumsum(!!sym(svr_group_id))) %&gt;% select(one_of(svr_group_id, ls_svr_group_vars), everything()) # Add Height Index kable(df_panel_indiage_id) %&gt;% kable_styling_fc_wide() female_asian_id female asian birth_height year_of_age hgt_growth hgt_growcumu height 1 0 0 65.14520 1 13.895393 13.895393 79.04059 1 0 0 65.14520 2 11.928034 25.823427 90.96862 1 0 0 65.14520 3 11.405068 37.228495 102.37369 2 0 1 56.20482 1 14.942698 14.942698 71.14751 2 0 1 56.20482 2 11.557058 26.499756 82.70457 2 0 1 56.20482 3 12.085305 38.585060 94.78988 3 1 0 58.66301 1 10.440660 10.440660 69.10367 3 1 0 58.66301 2 10.941420 21.382081 80.04509 3 1 0 58.66301 3 7.891597 29.273678 87.93669 4 1 1 61.38275 1 11.907053 11.907053 73.28980 4 1 1 61.38275 2 12.954674 24.861727 86.24448 4 1 1 61.38275 3 5.246137 30.107864 91.49061 4 1 1 57.93944 1 6.471137 6.471137 64.41058 4 1 1 57.93944 2 14.630242 21.101379 79.04082 4 1 1 57.93944 3 14.022991 35.124369 93.06381 4.1.2 Join Datasets Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 4.1.2.1 Join Panel with Multiple Keys We have two datasets, one for student enrollment, panel over time, but some students do not show up on some dates. The other is a skeleton panel with all student ID and all dates. Often we need to join dataframes together, and we need to join by the student ID and the panel time Key at the same time. When students show up, there is a quiz score for that day, so the joined panel should have as data column quiz score Student count is \\(N\\), total dates are \\(M\\). First we generate two panels below, then we join by both keys using left_join. First, define dataframes: # Define it_N &lt;- 4 it_M &lt;- 3 svr_id &lt;- &#39;sid&#39; svr_date &lt;- &#39;classday&#39; svr_attend &lt;- &#39;date_in_class&#39; # Panel Skeleton df_panel_balanced_skeleton &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Print kable(df_panel_balanced_skeleton) %&gt;% kable_styling_fc() sid classday 1 1 1 2 1 3 2 1 2 2 2 3 3 1 3 2 3 3 4 1 4 2 4 3 # Smaller Panel of Random Days in School set.seed(456) df_panel_attend &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), !!sym(svr_date)) %&gt;% rename(!!sym(svr_attend) := !!sym(svr_date)) %&gt;% mutate(dayquizscore = rnorm(n(),mean=80,sd=10)) # Print kable(df_panel_attend) %&gt;% kable_styling_fc() sid date_in_class dayquizscore 1 1 89.88726 2 1 96.53929 2 2 65.59195 2 3 99.47356 4 2 97.36936 Second, now join dataframes: # Join with explicit names df_quiz_joined_multikey &lt;- df_panel_balanced_skeleton %&gt;% left_join(df_panel_attend, by=(c(&#39;sid&#39;=&#39;sid&#39;, &#39;classday&#39;=&#39;date_in_class&#39;))) # Join with setname strings df_quiz_joined_multikey_setnames &lt;- df_panel_balanced_skeleton %&gt;% left_join(df_panel_attend, by=setNames(c(&#39;sid&#39;, &#39;date_in_class&#39;), c(&#39;sid&#39;, &#39;classday&#39;))) # Print kable(df_quiz_joined_multikey) %&gt;% kable_styling_fc() sid classday dayquizscore 1 1 89.88726 1 2 NA 1 3 NA 2 1 96.53929 2 2 65.59195 2 3 99.47356 3 1 NA 3 2 NA 3 3 NA 4 1 NA 4 2 97.36936 4 3 NA kable(df_quiz_joined_multikey_setnames) %&gt;% kable_styling_fc() sid classday dayquizscore 1 1 89.88726 1 2 NA 1 3 NA 2 1 96.53929 2 2 65.59195 2 3 99.47356 3 1 NA 3 2 NA 3 3 NA 4 1 NA 4 2 97.36936 4 3 NA 4.1.2.2 Stack Panel Frames Together There are multiple panel dataframe, each for different subsets of dates. All variable names and units of observations are identical. Use DPLYR bind_rows. # Define it_N &lt;- 2 # Number of individuals it_M &lt;- 3 # Number of Months svr_id &lt;- &#39;sid&#39; svr_date &lt;- &#39;date&#39; # Panel First Half of Year df_panel_m1tom3 &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Panel Second Half of Year df_panel_m4tom6 &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number() + 3) %&gt;% ungroup() # Bind Rows df_panel_m1tm6 &lt;- bind_rows(df_panel_m1tom3, df_panel_m4tom6) %&gt;% arrange(!!!syms(c(svr_id, svr_date))) # Print kable(df_panel_m1tom3) %&gt;% kable_styling_fc() sid date 1 1 1 2 1 3 2 1 2 2 2 3 kable(df_panel_m4tom6) %&gt;% kable_styling_fc() sid date 1 4 1 5 1 6 2 4 2 5 2 6 kable(df_panel_m1tm6) %&gt;% kable_styling_fc() sid date 1 1 1 2 1 3 1 4 1 5 1 6 2 1 2 2 2 3 2 4 2 5 2 6 4.1.3 Gather Files Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 4.1.3.1 Stack CSV Files Together Extract and Select Variables There are multiple csv files, each was simulated with a different combination of parameters, each file has the same columns and perhaps even the same number of rows. We want to combine the files together, and provide correct attributes to rows from each table stacked, based on each underlying csv files file name. This is necessary, for example, when running computational exercises across EC2 instances in batch array and files are saved to different S3 folders. Need to gather parallel computational results together in a single file after syncing files locally with S3. In the csv folder under this section, there are four subfolder, each containing 3 files with identical file structures. We want to find the relevant csv files from these directories, and stack the results together. File search search string, search in all subfolders, the search string contains file prefix that is common across files that need to be gathered. Extract path folder hierarchy, each layer of folder is a different variable Stack files together, with variables for file name and folder name Extract from file name the component that is not in the search string, keep as separate variable Follow specific rules about how file suffix is constructed to obtain additional variables. Keep only a subset of columns of interest. First, search and find all files with certain prefix. # can search in multiple paths, second path here has no relevant contents spt_roots &lt;- c(&#39;C:/Users/fan/R4Econ/panel/basic/_file/csv&#39;, &#39;C:/Users/fan/R4Econ/panel/basic/_file/tex&#39;) # can skip file names with certain strings spn_skip &lt;- c(&#39;A3420&#39;) # prefix search patther, st_search_str &lt;- &#39;solu_19E1NEp99r99x_ITG_PE_cev_*&#39; # Search and get all Path ls_sfls &lt;- list.files(path=spt_roots, recursive=T, pattern=st_search_str, full.names=T) # Skip path if contains words in skip list if(!missing(spn_skip)) { ls_sfls &lt;- ls_sfls[!grepl(paste(spn_skip, collapse = &quot;|&quot;), ls_sfls)] } Second, show all the files found, show their full path, the file name and the two folder names above the file name. # Loop and print found files it_folders_names_to_keep = 2 for (spt_file in ls_sfls) { ls_srt_folders_name_keep &lt;- tail(strsplit(spt_file, &quot;/&quot;)[[1]], n=it_folders_names_to_keep+1) snm_file_name &lt;- tail(ls_srt_folders_name_keep, 1) ls_srt_folders_keep &lt;- head(ls_srt_folders_name_keep, it_folders_names_to_keep) print(paste0(&#39;path:&#39;, spt_file)) print(snm_file_name) print(ls_srt_folders_keep) } ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev-2000/solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0.csv&quot; ## [1] &quot;csv&quot; &quot;cev-2000&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev-2000/solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840.csv&quot; ## [1] &quot;csv&quot; &quot;cev-2000&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev-947/solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0.csv&quot; ## [1] &quot;csv&quot; &quot;cev-947&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev-947/solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840.csv&quot; ## [1] &quot;csv&quot; &quot;cev-947&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev2000/solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0.csv&quot; ## [1] &quot;csv&quot; &quot;cev2000&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev2000/solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840.csv&quot; ## [1] &quot;csv&quot; &quot;cev2000&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev947/solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A0.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A0.csv&quot; ## [1] &quot;csv&quot; &quot;cev947&quot; ## [1] &quot;path:C:/Users/fan/R4Econ/panel/basic/_file/csv/cev947/solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A6840.csv&quot; ## [1] &quot;solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A6840.csv&quot; ## [1] &quot;csv&quot; &quot;cev947&quot; Third, create a dataframe with the folder and file names: # String matrix empty mt_st_paths_names &lt;- matrix(data=NA, nrow=length(ls_sfls), ncol=4) # Loop and print found files it_folders_names_to_keep = 2 it_file_counter = 0 for (spt_file in ls_sfls) { # row counter it_file_counter = it_file_counter + 1 # get file paths ls_srt_folders_name_keep &lt;- tail(strsplit(spt_file, &quot;/&quot;)[[1]], n=it_folders_names_to_keep+1) snm_file_name &lt;- tail(ls_srt_folders_name_keep, 1) ls_srt_folders_keep &lt;- head(ls_srt_folders_name_keep, it_folders_names_to_keep) # store # tools::file_path_sans_ext to drop suffix mt_st_paths_names[it_file_counter,1] = tools::file_path_sans_ext(snm_file_name) mt_st_paths_names[it_file_counter,2] = ls_srt_folders_keep[1] mt_st_paths_names[it_file_counter,3] = ls_srt_folders_keep[2] mt_st_paths_names[it_file_counter,4] = spt_file } # Column Names ar_st_varnames &lt;- c(&#39;fileid&#39;,&#39;name&#39;,&#39;folder1&#39;,&#39;folder2&#39;, &#39;fullpath&#39;) # Combine to tibble, add name col1, col2, etc. tb_csv_info &lt;- as_tibble(mt_st_paths_names) %&gt;% rowid_to_column(var = &quot;id&quot;) %&gt;% rename_all(~c(ar_st_varnames)) # Display kable(tb_csv_info[,1:4]) %&gt;% kable_styling_fc() fileid name folder1 folder2 1 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0 csv cev-2000 2 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840 csv cev-2000 3 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0 csv cev-947 4 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840 csv cev-947 5 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0 csv cev2000 6 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840 csv cev2000 7 solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A0 csv cev947 8 solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A6840 csv cev947 Fourth, create a dataframe by expanding each row with the datafile loaded in, use apply with anonymous function. # Generate a list of dataframes ls_df_loaded_files = apply(tb_csv_info, 1, function(row) { # Loading file spn_full_path &lt;- row[5] mt_csv = read.csv(file = spn_full_path) # dataframe it_fileid &lt;- row[1] snm_filename &lt;- row[2] srt_folder_level2 &lt;- row[3] srt_folder_level1 &lt;- row[4] tb_combine = as_tibble(mt_csv) %&gt;% na.omit %&gt;% rowid_to_column(var = &quot;statesid&quot;) %&gt;% mutate(fileid = it_fileid, filename = snm_filename, folder_lvl1 = srt_folder_level1, folder_lvl2 = srt_folder_level2) %&gt;% select(fileid, filename, folder_lvl1, folder_lvl2, statesid, everything()) # return return(tb_combine) }) # Stack dataframes together df_all_files = do.call(bind_rows, ls_df_loaded_files) # show stacked table kable(df_all_files[seq(1,601,50),1:6]) %&gt;% kable_styling_fc_wide() fileid filename folder_lvl1 folder_lvl2 statesid EjV 1 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0 cev-2000 csv 1 -28.8586860 1 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0 cev-2000 csv 51 -0.2106603 2 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840 cev-2000 csv 3 -28.8586860 2 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840 cev-2000 csv 53 -0.0642997 3 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0 cev-947 csv 5 -5.8826609 3 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0 cev-947 csv 55 0.0353187 4 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840 cev-947 csv 7 -2.7046907 4 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840 cev-947 csv 57 0.1094474 5 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0 cev2000 csv 9 -2.9782236 5 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0 cev2000 csv 59 0.3389275 6 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840 cev2000 csv 11 -1.7229647 6 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840 cev2000 csv 61 -14.6880377 7 solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A0 cev947 csv 13 -1.7623279 Fifth, get additional information from the file name and file folder. Extract those as separate variables. The file names is dash connected, with various information. First, split just the final element of the string file name out, which is A###. Then, also extract the number next to N as a separate numeric column. Additional folder_lvl1 separate out the numeric number from the initial word cev. Split solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A### to solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000 and A###: # separate last eleemtnafter underscore df_all_files_finalA &lt;- df_all_files %&gt;% separate(filename, into = c(&quot;filename_main&quot;, &quot;prod_type_st&quot;), sep=&quot;_(?=[^_]+$)&quot;, remove = FALSE) %&gt;% select(fileid, filename, filename_main, prod_type_st, folder_lvl1, folder_lvl2, statesid, everything()) # show stacked table kable(df_all_files_finalA[seq(1,601,50),1:10]) %&gt;% kable_styling_fc_wide() fileid filename filename_main prod_type_st folder_lvl1 folder_lvl2 statesid EjV k_tt b_tt 1 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000 A0 cev-2000 csv 1 -28.8586860 0.000000 0.000000 1 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A0 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000 A0 cev-2000 csv 51 -0.2106603 0.000000 78.005300 2 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000 A6840 cev-2000 csv 3 -28.8586860 0.000000 0.000000 2 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000_A6840 solu_19E1NEp99r99x_ITG_PE_cev_c0_cev-2000 A6840 cev-2000 csv 53 -0.0642997 0.000000 84.215368 3 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947 A0 cev-947 csv 5 -5.8826609 0.000000 3.869909 3 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A0 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947 A0 cev-947 csv 55 0.0353187 0.000000 90.611739 4 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947 A6840 cev-947 csv 7 -2.7046907 0.000000 7.855916 4 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947_A6840 solu_19E1NEp99r99x_ITG_PE_cev_c5_cev-947 A6840 cev-947 csv 57 0.1094474 0.000000 90.611739 5 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000 A0 cev2000 csv 9 -2.9782236 0.000000 7.855916 5 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A0 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000 A0 cev2000 csv 59 0.3389275 0.000000 97.200000 6 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000 A6840 cev2000 csv 11 -1.7229647 0.000000 11.961502 6 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000_A6840 solu_19E1NEp99r99x_ITG_PE_cev_c19_cev2000 A6840 cev2000 csv 61 -14.6880377 1.990694 -1.879215 7 solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947_A0 solu_19E1NEp99r99x_ITG_PE_cev_c14_cev947 A0 cev947 csv 13 -1.7623279 0.000000 16.190257 Split A### to A and A###. Additionally, also split cev#### to cev and ####, allow for positive and negative numbers. See regular expression 101 helper # string and number separation df_all_files_finalB &lt;- df_all_files_finalA %&gt;% separate(prod_type_st, into = c(&quot;prod_type_st_prefix&quot;, &quot;prod_type_lvl&quot;), sep=&quot;(?&lt;=[A-Za-z])(?=[-0-9])&quot;, # positive or negative numbers remove=FALSE) %&gt;% separate(folder_lvl1, into = c(&quot;cev_prefix&quot;, &quot;cev_lvl&quot;), sep=&quot;(?&lt;=[A-Za-z])(?=[-0-9])&quot;, # positive or negative numbers remove=FALSE) %&gt;% mutate(cev_st = folder_lvl1, prod_type_lvl = as.numeric(prod_type_lvl), cev_lvl = as.numeric(cev_lvl)/10000) %&gt;% select(fileid, prod_type_st, prod_type_lvl, cev_st, cev_lvl, statesid, EjV, filename, folder_lvl1, folder_lvl2) # Ordering, sort by cev_lvl, then prod_type_lvl, then stateid df_all_files_finalB &lt;- df_all_files_finalB %&gt;% arrange(cev_lvl, prod_type_lvl, statesid) # show stacked table kable(df_all_files_finalB[seq(1,49*16,49),1:7]) %&gt;% kable_styling_fc_wide() fileid prod_type_st prod_type_lvl cev_st cev_lvl statesid EjV 1 A0 0 cev-2000 -0.2000 1 -28.8586860 1 A0 0 cev-2000 -0.2000 50 -0.2106603 2 A6840 6840 cev-2000 -0.2000 1 -28.8586860 2 A6840 6840 cev-2000 -0.2000 50 -0.1311749 3 A0 0 cev-947 -0.0947 1 -28.0399281 3 A0 0 cev-947 -0.0947 50 -0.0911499 4 A6840 6840 cev-947 -0.0947 1 -28.0399281 4 A6840 6840 cev-947 -0.0947 50 -0.0134719 7 A0 0 cev947 0.0947 1 -26.8243673 7 A0 0 cev947 0.0947 50 0.0857474 8 A6840 6840 cev947 0.0947 1 -26.8243673 8 A6840 6840 cev947 0.0947 50 0.1608382 5 A0 0 cev2000 0.2000 1 -26.2512036 5 A0 0 cev2000 0.2000 50 0.1694524 6 A6840 6840 cev2000 0.2000 1 -26.2512036 6 A6840 6840 cev2000 0.2000 50 0.2432677 4.2 Wide and Long 4.2.1 Long to Wide Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Using the pivot_wider function in tidyr to reshape panel or other data structures 4.2.1.1 Panel Long Attendance Roster to Wide There are \\(N\\) students in class, but only a subset of them attend class each day. If student \\(id_i\\) is in class on day \\(Q\\), the teacher records on a sheet the date and the student ID. So if the student has been in class 10 times, the teacher has ten rows of recorded data for the student with two columns: column one is the student ID, and column two is the date on which the student was in class. Suppose there were 50 students, who on average attended exactly 10 classes each during the semester, this means we have \\(10 \\cdot 50\\) rows of data, with differing numbers of rows for each student. This is shown as df_panel_attend_date generated below. Now we want to generate a new dataframe, where each row is a date, and each column is a student. The values in the new dataframe shows, at the \\(Q^{th}\\) day, how many classes student \\(i\\) has attended so far. The following results is also in a REconTools Function. This is shown as df_attend_cumu_by_day generated below. First, generate the raw data structure, df_panel_attend_date: # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;student_id&#39; # from : support/rand/fs_rand_draws.Rmd set.seed(222) df_panel_attend_date &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(date = row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), date) %&gt;% rename(date_in_class = date) # Print kable(df_panel_attend_date) %&gt;% kable_styling_fc() student_id date_in_class 1 2 1 4 2 1 2 2 2 5 3 2 3 3 3 5 Second, generate wider data structure, df_attend_cumu_by_day: # Define svr_id &lt;- &#39;student_id&#39; svr_date &lt;- &#39;date_in_class&#39; st_idcol_prefix &lt;- &#39;sid_&#39; # Generate cumulative enrollment counts by date df_panel_attend_date_addone &lt;- df_panel_attend_date %&gt;% mutate(attended = 1) kable(df_panel_attend_date_addone) %&gt;% kable_styling_fc() student_id date_in_class attended 1 2 1 1 4 1 2 1 1 2 2 1 2 5 1 3 2 1 3 3 1 3 5 1 # Pivot Wide df_panel_attend_date_wider &lt;- df_panel_attend_date_addone %&gt;% pivot_wider(names_from = svr_id, values_from = attended) kable(df_panel_attend_date_wider) %&gt;% kable_styling_fc() date_in_class 1 2 3 2 1 1 1 4 1 NA NA 1 NA 1 NA 5 NA 1 1 3 NA NA 1 # Sort and rename # rename see: https://fanwangecon.github.io/R4Econ/amto/tibble/fs_tib_basics.html ar_unique_ids &lt;- sort(unique(df_panel_attend_date %&gt;% pull(!!sym(svr_id)))) df_panel_attend_date_wider_sort &lt;- df_panel_attend_date_wider %&gt;% arrange(!!sym(svr_date)) %&gt;% rename_at(vars(num_range(&#39;&#39;,ar_unique_ids)) , list(~paste0(st_idcol_prefix, . , &#39;&#39;)) ) kable(df_panel_attend_date_wider_sort) %&gt;% kable_styling_fc() date_in_class sid_1 sid_2 sid_3 1 NA 1 NA 2 1 1 1 3 NA NA 1 4 1 NA NA 5 NA 1 1 # replace NA and cumusum again # see: R4Econ/support/function/fs_func_multivar for renaming and replacing df_attend_cumu_by_day &lt;- df_panel_attend_date_wider_sort %&gt;% mutate_at(vars(contains(st_idcol_prefix)), list(~replace_na(., 0))) %&gt;% mutate_at(vars(contains(st_idcol_prefix)), list(~cumsum(.))) kable(df_attend_cumu_by_day) %&gt;% kable_styling_fc() date_in_class sid_1 sid_2 sid_3 1 0 1 0 2 1 2 1 3 1 2 2 4 2 2 2 5 2 3 3 The structure above is also a function in Fans REconTools Package, here the function is tested: # Parameters df &lt;- df_panel_attend_date svr_id_i &lt;- &#39;student_id&#39; svr_id_t &lt;- &#39;date_in_class&#39; st_idcol_prefix &lt;- &#39;sid_&#39; # Invoke Function ls_df_rosterwide &lt;- ff_panel_expand_longrosterwide(df, svr_id_t, svr_id_i, st_idcol_prefix) df_roster_wide_func &lt;- ls_df_rosterwide$df_roster_wide df_roster_wide_cumu_func &lt;- ls_df_rosterwide$df_roster_wide_cumu # Print print(df_roster_wide_func) print(df_roster_wide_cumu_func) 4.2.2 Wide to Long Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Using the pivot_wider function in tidyr to reshape panel or other data structures 4.2.2.1 Generated Matrix by States to Long Table A matrix of ev given states, rows are states and cols are shocks. Convert to Long table with shock and state values and ev. Generated Matrix by States to Long Table where state values are stored as variables, with correct value labels for states: Generate a matrix Convert matrix to tibble Tibble make longer, and store column and row id var names # Generate A Matrix set.seed(123) ar_a &lt;- c(1.1,5.1) ar_z &lt;- seq(-2.5, 2.53, length.out=11) mt_ev = matrix(rnorm(ar_a*ar_z), nrow=length(ar_a), ncol=length(ar_z)) # Name Matrix rownames(mt_ev) &lt;- paste0(&#39;ai&#39;, seq(1:length(ar_a))) colnames(mt_ev) &lt;- paste0(&#39;zi&#39;, seq(1:length(ar_z))) # to tibble tb_ev &lt;- as_tibble(mt_ev) %&gt;% rowid_to_column(var = &quot;ai&quot;) # longer tb_ev_long &lt;- tb_ev %&gt;% pivot_longer(cols = starts_with(&#39;zi&#39;), names_to = c(&#39;zi&#39;), names_pattern = paste0(&quot;zi(.*)&quot;), values_to = &quot;ev&quot;) %&gt;% mutate(zi = as.numeric(zi)) # Merge with a and z values tb_ev_long &lt;- tb_ev_long %&gt;% left_join(as_tibble(ar_a) %&gt;% rowid_to_column(var = &quot;ai&quot;) %&gt;% rename(a = value) , by = &#39;ai&#39;) %&gt;% left_join(as_tibble(ar_z) %&gt;% rowid_to_column(var = &quot;zi&quot;) %&gt;% rename(z = value), by = &#39;zi&#39;) %&gt;% select(a,ai,z,zi,ev) # Display kable(tb_ev) %&gt;% kable_styling_fc_wide() ai zi1 zi2 zi3 zi4 zi5 zi6 zi7 zi8 zi9 zi10 zi11 1 -0.5604756 1.5587083 0.1292877 0.4609162 -0.6868529 1.2240818 -0.2301775 0.0705084 1.7150650 -1.2650612 -0.445662 2 -0.2301775 0.0705084 1.7150650 -1.2650612 -0.4456620 -0.5604756 1.5587083 0.1292877 0.4609162 -0.6868529 1.224082 kable(tb_ev_long) %&gt;% kable_styling_fc() a ai z zi ev 1.1 1 -2.500 1 -0.5604756 1.1 1 -1.997 2 1.5587083 1.1 1 -1.494 3 0.1292877 1.1 1 -0.991 4 0.4609162 1.1 1 -0.488 5 -0.6868529 1.1 1 0.015 6 1.2240818 1.1 1 0.518 7 -0.2301775 1.1 1 1.021 8 0.0705084 1.1 1 1.524 9 1.7150650 1.1 1 2.027 10 -1.2650612 1.1 1 2.530 11 -0.4456620 5.1 2 -2.500 1 -0.2301775 5.1 2 -1.997 2 0.0705084 5.1 2 -1.494 3 1.7150650 5.1 2 -0.991 4 -1.2650612 5.1 2 -0.488 5 -0.4456620 5.1 2 0.015 6 -0.5604756 5.1 2 0.518 7 1.5587083 5.1 2 1.021 8 0.1292877 5.1 2 1.524 9 0.4609162 5.1 2 2.027 10 -0.6868529 5.1 2 2.530 11 1.2240818 4.3 Join and Compare 4.3.1 Find Closest Neighbor on Grid Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Using the pivot_wider function in tidyr to reshape panel or other data structures 4.3.1.1 Closest Neighbor on Grid There is a dataframe that provides \\(V(coh, a, cev)\\) levels. There is another dataframe with \\(\\hat{V}(coh, a)\\), for each \\(coh, a\\), find the \\(cev\\) that such that the difference between \\(\\hat{V}(coh, a)\\) and \\(V(coh, a, cev)\\) is minimized. \\(V\\) and \\(\\hat{V}\\) information are stored in a dataframe in the csv folder in the current directory. In fact, we have one \\(V\\) surface, but multiple \\(\\hat{V}\\) files, so we want to do the find closest neighbor exercise for each one of the several \\(\\hat{V}\\) files. The structure is as follows: (1) Load in the \\(V\\) file, where \\(coh, a, cev\\) are all variable attributes. (2) Merge with one \\(\\hat{V}\\) file. (3) Take the difference between the \\(V\\) and \\(\\hat{V}\\) columns, and take the absolute value of the difference. (4) Group by \\(coh, a\\), and sort to get the smallest absolute difference among the \\(cev\\) possibilities, and slice out the row for the smallest. (5) Now We have \\(V(coh, a, cev^{\\star}(coh, a))\\). (6) Do this for each of the several \\(\\hat{V}\\) files. (7) Stack the results from 1 through 6 together, generate a column that identifies which simulation/exercise/counterfactual each of the \\(\\hat{V}\\) file comes from. (8) Visualize by plotting as subplot different \\(a\\), \\(coh\\) is x-axis, different \\(\\hat{V}\\) outcome are different lines, and \\(cev^{\\star}\\left(coh, a, \\hat{V}\\right)\\) is the y-axis outcome. First, load the CEV file. # folder spt_root &lt;- c(&#39;C:/Users/fan/R4Econ/panel/join/_file/csv&#39;) # cev surface file, the V file snm_cev_surface &lt;- &#39;e_19E1NEp99r99_ITG_PE_cev_subsettest.csv&#39; mt_cev_surface &lt;- read.csv(file = file.path(spt_root, snm_cev_surface)) tb_cev_surface &lt;- as_tibble(mt_cev_surface) %&gt;% rename(EjVcev = EjV) Second, loop over the V hat files, join V with V hat: ls_tb_cev_surfhat = vector(mode = &quot;list&quot;, length = 4) for (it_simu_counter in c(1,2,3,4)) { # conditionally change file names if (it_simu_counter == 1) { st_counter &lt;- &#39;19E1NEp99r99&#39; } else if (it_simu_counter == 2) { st_counter &lt;- &#39;19E1NEp02r99&#39; } else if (it_simu_counter == 3) { st_counter &lt;- &#39;19E1NEp02per02ger99&#39; } else if (it_simu_counter == 4) { st_counter &lt;- &#39;19E1NEp02r02&#39; } snm_v_hat &lt;- paste0(&#39;e_&#39;, st_counter, &#39;_ITG_PE_subsettest.csv&#39;) # Overall path to files mt_v_hat &lt;- read.csv(file = file.path(spt_root, snm_v_hat)) tb_v_hat &lt;- as_tibble(mt_v_hat) %&gt;% select(prod_type_lvl, statesid, EjV) # Merge file using key tb_cev_surfhat &lt;- tb_cev_surface %&gt;% left_join(tb_v_hat, by=(c(&#39;prod_type_lvl&#39;=&#39;prod_type_lvl&#39;, &#39;statesid&#39;=&#39;statesid&#39;))) %&gt;% arrange(statesid, prod_type_lvl, cev_lvl) %&gt;% mutate(counter_policy = st_counter) # Store to list ls_tb_cev_surfhat[[it_simu_counter]] &lt;- tb_cev_surfhat } # Display kable(ls_tb_cev_surfhat[[1]][seq(1, 40, 5),]) %&gt;% kable_styling_fc_wide() X cev_st cev_lvl prod_type_st prod_type_lvl statesid cash_tt EjVcev EjV counter_policy 1 cev-2000 -0.2000 A0 0 526 32.84747 -1.0479929 -0.7957419 19E1NEp99r99 1501 cev-947 -0.0947 A0 0 526 32.84747 -0.9079859 -0.7957419 19E1NEp99r99 3001 cev105 0.0105 A0 0 526 32.84747 -0.7880156 -0.7957419 19E1NEp99r99 4501 cev1157 0.1157 A0 0 526 32.84747 -0.6803586 -0.7957419 19E1NEp99r99 51 cev-2000 -0.2000 A2504 2504 526 32.90371 -1.0002921 -0.7504785 19E1NEp99r99 1551 cev-947 -0.0947 A2504 2504 526 32.90371 -0.8613743 -0.7504785 19E1NEp99r99 3051 cev105 0.0105 A2504 2504 526 32.90371 -0.7423281 -0.7504785 19E1NEp99r99 4551 cev1157 0.1157 A2504 2504 526 32.90371 -0.6354620 -0.7504785 19E1NEp99r99 Third, sort each file, and keep only the best match rows that minimize the absolute distance between EjV and EjVcev. ls_tb_cev_matched = vector(mode = &quot;list&quot;, length = 4) for (it_simu_counter in c(1,2,3,4)) { # Load merged file tb_cev_surfhat &lt;- ls_tb_cev_surfhat[[it_simu_counter]] # Difference Column tb_cev_surfhat &lt;- tb_cev_surfhat %&gt;% mutate(EjVcev_gap = abs(EjVcev - EjV)) # Group by, Arrange and Slice, get lowest gap tb_cev_matched &lt;- tb_cev_surfhat %&gt;% arrange(statesid, prod_type_lvl, EjVcev_gap) %&gt;% group_by(statesid, prod_type_lvl) %&gt;% slice_head(n=1) # Store to list ls_tb_cev_matched[[it_simu_counter]] &lt;- tb_cev_matched } # Display kable(ls_tb_cev_matched[[2]][seq(1, 30, 1),]) %&gt;% kable_styling_fc_wide() X cev_st cev_lvl prod_type_st prod_type_lvl statesid cash_tt EjVcev EjV counter_policy EjVcev_gap 3001 cev105 0.0105 A0 0 526 32.847471 -0.7880156 -0.7928034 19E1NEp02r99 0.0047878 3051 cev105 0.0105 A2504 2504 526 32.903714 -0.7423281 -0.7480617 19E1NEp02r99 0.0057336 3101 cev105 0.0105 A4145 4145 526 32.948970 -0.7082006 -0.7145418 19E1NEp02r99 0.0063412 3151 cev105 0.0105 A5633 5633 526 32.996952 -0.6753576 -0.6818996 19E1NEp02r99 0.0065420 3201 cev105 0.0105 A7274 7274 526 33.058832 -0.6368297 -0.6431710 19E1NEp02r99 0.0063413 3251 cev105 0.0105 A9779 9779 526 33.175241 -0.5711706 -0.5774648 19E1NEp02r99 0.0062942 3002 cev105 0.0105 A0 0 555 53.346587 -0.2985944 -0.3041922 19E1NEp02r99 0.0055978 3052 cev105 0.0105 A2504 2504 555 53.815772 -0.2617572 -0.2680026 19E1NEp02r99 0.0062454 3102 cev105 0.0105 A4145 4145 555 54.193302 -0.2340822 -0.2406142 19E1NEp02r99 0.0065320 3152 cev105 0.0105 A5633 5633 555 54.593579 -0.2067964 -0.2134634 19E1NEp02r99 0.0066670 3202 cev105 0.0105 A7274 7274 555 55.109790 -0.1740126 -0.1806320 19E1NEp02r99 0.0066194 3252 cev105 0.0105 A9779 9779 555 56.080888 -0.1169470 -0.1236111 19E1NEp02r99 0.0066641 3603 cev526 0.0526 A0 0 905 1.533025 -5.2530406 -5.2486887 19E1NEp02r99 0.0043519 3353 cev315 0.0315 A2504 2504 905 1.714498 -4.5517474 -4.5408560 19E1NEp02r99 0.0108913 3403 cev315 0.0315 A4145 4145 905 1.860521 -4.1039608 -4.1072736 19E1NEp02r99 0.0033128 3453 cev315 0.0315 A5633 5633 905 2.015341 -3.7465733 -3.7611842 19E1NEp02r99 0.0146109 3503 cev315 0.0315 A7274 7274 905 2.215003 -3.4101025 -3.4235413 19E1NEp02r99 0.0134388 3553 cev315 0.0315 A9779 9779 905 2.590608 -2.9413469 -2.9535570 19E1NEp02r99 0.0122101 3004 cev105 0.0105 A0 0 953 20.125381 -1.3249909 -1.3290865 19E1NEp02r99 0.0040957 3054 cev105 0.0105 A2504 2504 953 20.306854 -1.2476021 -1.2531860 19E1NEp02r99 0.0055839 3104 cev105 0.0105 A4145 4145 953 20.452876 -1.1916003 -1.1975215 19E1NEp02r99 0.0059211 3154 cev105 0.0105 A5633 5633 953 20.607697 -1.1383665 -1.1444048 19E1NEp02r99 0.0060383 3204 cev105 0.0105 A7274 7274 953 20.807359 -1.0766095 -1.0823344 19E1NEp02r99 0.0057250 3254 cev105 0.0105 A9779 9779 953 21.182964 -0.9729832 -0.9781408 19E1NEp02r99 0.0051576 3005 cev105 0.0105 A0 0 1017 63.774766 -0.1284542 -0.1342653 19E1NEp02r99 0.0058110 3055 cev105 0.0105 A2504 2504 1017 64.298911 -0.0967695 -0.1031112 19E1NEp02r99 0.0063417 3105 cev105 0.0105 A4145 4145 1017 64.720664 -0.0728485 -0.0793940 19E1NEp02r99 0.0065454 3155 cev105 0.0105 A5633 5633 1017 65.167829 -0.0490898 -0.0557238 19E1NEp02r99 0.0066341 3205 cev105 0.0105 A7274 7274 1017 65.744507 -0.0203378 -0.0269149 19E1NEp02r99 0.0065772 3255 cev105 0.0105 A9779 9779 1017 66.829359 0.0299397 0.0233507 19E1NEp02r99 0.0065890 Fourth, row_bind results together. # Single dataframe with all results tb_cev_matched_all_counter &lt;- do.call(bind_rows, ls_tb_cev_matched) # check size print(dim(tb_cev_matched_all_counter)) ## [1] 1200 11 Fifth, visualize results # select four from the productivity types ar_prod_type_lvl_unique &lt;- unique(tb_cev_matched_all_counter %&gt;% pull(prod_type_lvl)) ar_prod_type_lvl_selected &lt;- ar_prod_type_lvl_unique[round(seq(1, length(ar_prod_type_lvl_unique), length.out=4))] # graph lineplot &lt;- tb_cev_matched_all_counter %&gt;% filter(prod_type_lvl %in% ar_prod_type_lvl_selected) %&gt;% group_by(prod_type_st, cash_tt) %&gt;% ggplot(aes(x=cash_tt, y=cev_lvl, colour=counter_policy, linetype=counter_policy, shape=counter_policy)) + facet_wrap( ~ prod_type_st) + geom_line() + geom_point() + labs(title = &#39;Visualizing the positions of matched values&#39;, x = &#39;Resource Levels&#39;, y = &#39;CEV&#39;, caption = paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/panel/join/htmlpdfr/fs_join_compare.html&#39;)) print(lineplot) "],["linear-regression.html", "Chapter 5 Linear Regression 5.1 OLS and IV 5.2 Decomposition", " Chapter 5 Linear Regression 5.1 OLS and IV Back to Fans R4Econ Homepage Table of Content 5.1.1 OLS and IV Regression Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). IV regression using AER package. Option to store all results in dataframe row for combining results from other estimations together. Produce Row Statistics. 5.1.1.1 Construct Program # IV regression function # The code below uses the AER library&#39;s regresison function # All results are stored in a single row as data_frame # This functoin could work with dplyr do # var.y is single outcome, vars.x, vars.c and vars.z are vectors of endogenous variables, controls and instruments. regf.iv &lt;- function(var.y, vars.x, vars.c, vars.z, df, transpose=TRUE) { # A. Set-Up Equation str.vars.x &lt;- paste(vars.x, collapse=&#39;+&#39;) str.vars.c &lt;- paste(vars.c, collapse=&#39;+&#39;) df &lt;- df %&gt;% select(one_of(var.y, vars.x, vars.c, vars.z)) %&gt;% drop_na() %&gt;% filter_all(all_vars(!is.infinite(.))) if (length(vars.z) &gt;= 1) { # library(AER) str.vars.z &lt;- paste(vars.z, collapse=&#39;+&#39;) equa.iv &lt;- paste(var.y, paste(paste(str.vars.x, str.vars.c, sep=&#39;+&#39;), paste(str.vars.z, str.vars.c, sep=&#39;+&#39;), sep=&#39;|&#39;), sep=&#39;~&#39;) # print(equa.iv) # B. IV Regression ivreg.summ &lt;- summary(ivreg(as.formula(equa.iv), data=df), vcov = sandwich, df = Inf, diagnostics = TRUE) # C. Statistics from IV Regression # ivreg.summ$coef # ivreg.summ$diagnostics # D. Combine Regression Results into a Matrix df.results &lt;- suppressWarnings(suppressMessages( as_tibble(ivreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(as_tibble(ivreg.summ$diagnostics, rownames=&#39;rownames&#39;)) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.z=str.vars.z, vars.c=str.vars.c)))) } else { # OLS regression equa.ols &lt;- paste(var.y, paste(paste(vars.x, collapse=&#39;+&#39;), paste(vars.c, collapse=&#39;+&#39;), sep=&#39;+&#39;), sep=&#39;~&#39;) lmreg.summ &lt;- summary(lm(as.formula(equa.ols), data=df)) lm.diagnostics &lt;- as_tibble( list(df1=lmreg.summ$df[[1]], df2=lmreg.summ$df[[2]], df3=lmreg.summ$df[[3]], sigma=lmreg.summ$sigma, r.squared=lmreg.summ$r.squared, adj.r.squared=lmreg.summ$adj.r.squared)) %&gt;% gather(variable, value) %&gt;% rename(rownames = variable) %&gt;% rename(v = value) df.results &lt;- suppressWarnings(suppressMessages( as_tibble(lmreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(lm.diagnostics) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.c=str.vars.c)))) } # E. Flatten Matrix, All IV results as a single tibble # row to be combined with other IV results df.row.results &lt;- df.results %&gt;% gather(variable, value, -rownames) %&gt;% drop_na() %&gt;% unite(esti.val, rownames, variable) %&gt;% mutate(esti.val = gsub(&#39; &#39;, &#39;&#39;, esti.val)) if (transpose) { df.row.results &lt;- df.row.results %&gt;% spread(esti.val, value) } # F. Return return(data.frame(df.row.results)) } 5.1.1.2 Program Testing Load Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) 5.1.1.2.1 Example No Instrument, OLS # One Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- NULL vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 52.1186286658651 prot_Estimate 0.374472386357917 sexMale_Estimate 0.611043720578292 hgt0_Estimate 0.148513781160842 wgt0_Estimate 0.00150560230505631 (Intercept)_Std.Error 1.57770483608693 prot_Std.Error 0.00418121191133815 sexMale_Std.Error 0.118396259120659 hgt0_Std.Error 0.0393807494783186 wgt0_Std.Error 0.000187123663624397 (Intercept)_tvalue 33.0344608660332 prot_tvalue 89.5607288744356 sexMale_tvalue 5.16100529794248 hgt0_tvalue 3.77122790013449 wgt0_tvalue 8.04602836377991 (Intercept)_Pr(&gt;|t|) 9.92126150975783e-233 prot_Pr(&gt;|t|) 0 sexMale_Pr(&gt;|t|) 2.48105505495642e-07 hgt0_Pr(&gt;|t|) 0.000162939618371183 wgt0_Pr(&gt;|t|) 9.05257561534111e-16 df1_v 5 df2_v 18958 df3_v 5 sigma_v 8.06197784622979 r.squared_v 0.319078711001325 adj.r.squared_v 0.318935041565942 vars_var.y hgt vars_vars.x prot vars_vars.c sex+hgt0+wgt0 5.1.1.2.2 Example 1 Insturment # One Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- c(&#39;momEdu&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 43.4301969117558 prot_Estimate 0.130833343849446 sexMale_Estimate 0.868121847262411 hgt0_Estimate 0.412093881817148 wgt0_Estimate 0.000858630042617921 (Intercept)_Std.Error 1.82489550971182 prot_Std.Error 0.0192036220809189 sexMale_Std.Error 0.13373016700542 hgt0_Std.Error 0.0459431912927002 wgt0_Std.Error 0.00022691057702563 (Intercept)_zvalue 23.798730766023 prot_zvalue 6.81295139521853 sexMale_zvalue 6.49159323361366 hgt0_zvalue 8.96963990141069 wgt0_zvalue 3.7840018472164 (Intercept)_Pr(&gt;|z|) 3.4423766196876e-125 prot_Pr(&gt;|z|) 9.56164541643828e-12 sexMale_Pr(&gt;|z|) 8.49333228172763e-11 hgt0_Pr(&gt;|z|) 2.97485394526792e-19 wgt0_Pr(&gt;|z|) 0.000154326676608523 Weakinstruments_df1 1 Wu-Hausman_df1 1 Sargan_df1 0 Weakinstruments_df2 16394 Wu-Hausman_df2 16393 Weakinstruments_statistic 935.817456612075 Wu-Hausman_statistic 123.595856606729 Weakinstruments_p-value 6.39714929178024e-200 Wu-Hausman_p-value 1.30703637796748e-28 vars_var.y hgt vars_vars.x prot vars_vars.z momEdu vars_vars.c sex+hgt0+wgt0 5.1.1.2.3 Example Multiple Instrucments # Multiple Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 42.2437613555242 prot_Estimate 0.26699945194704 sexMale_Estimate 0.695548488812932 hgt0_Estimate 0.424954881263031 wgt0_Estimate 0.000486951420329484 (Intercept)_Std.Error 1.85356686789642 prot_Std.Error 0.0154939347964083 sexMale_Std.Error 0.133157977814374 hgt0_Std.Error 0.0463195803786233 wgt0_Std.Error 0.000224867994873235 (Intercept)_zvalue 22.7905246296649 prot_zvalue 17.2325142357597 sexMale_zvalue 5.22348341593581 hgt0_zvalue 9.17441129192849 wgt0_zvalue 2.16549901022595 (Intercept)_Pr(&gt;|z|) 5.69294074735747e-115 prot_Pr(&gt;|z|) 1.51424021931607e-66 sexMale_Pr(&gt;|z|) 1.75588197502565e-07 hgt0_Pr(&gt;|z|) 4.54048595587756e-20 wgt0_Pr(&gt;|z|) 0.030349491114332 Weakinstruments_df1 4 Wu-Hausman_df1 1 Sargan_df1 3 Weakinstruments_df2 14914 Wu-Hausman_df2 14916 Weakinstruments_statistic 274.147084958343 Wu-Hausman_statistic 17.7562545747101 Sargan_statistic 463.729664547249 Weakinstruments_p-value 8.61731956233366e-228 Wu-Hausman_p-value 2.52567249124181e-05 Sargan_p-value 3.45452874915475e-100 vars_var.y hgt vars_vars.x prot vars_vars.z momEdu+wealthIdx+p.A.prot+p.A.nProt vars_vars.c sex+hgt0+wgt0 5.1.1.2.4 Example Multiple Endogenous Variables # Multiple Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) %&gt;% kable() %&gt;% kable_styling_fc() esti.val value (Intercept)_Estimate 44.0243196254297 prot_Estimate -1.4025623247106 cal_Estimate 0.065104895750151 sexMale_Estimate 0.120832787571818 hgt0_Estimate 0.286525437984517 wgt0_Estimate 0.000850481389651033 (Intercept)_Std.Error 2.75354847244082 prot_Std.Error 0.198640060273635 cal_Std.Error 0.00758881298880996 sexMale_Std.Error 0.209984580636303 hgt0_Std.Error 0.0707828182888255 wgt0_Std.Error 0.00033711210444429 (Intercept)_zvalue 15.9882130516502 prot_zvalue -7.06082309267581 cal_zvalue 8.57906181719737 sexMale_zvalue 0.575436478267434 hgt0_zvalue 4.04795181812859 wgt0_zvalue 2.52284441418383 (Intercept)_Pr(&gt;|z|) 1.54396598126854e-57 prot_Pr(&gt;|z|) 1.65519210848649e-12 cal_Pr(&gt;|z|) 9.56500648203187e-18 sexMale_Pr(&gt;|z|) 0.564996139463599 hgt0_Pr(&gt;|z|) 5.16677787108928e-05 wgt0_Pr(&gt;|z|) 0.0116409892837831 Weakinstruments(prot)_df1 4 Weakinstruments(cal)_df1 4 Wu-Hausman_df1 2 Sargan_df1 2 Weakinstruments(prot)_df2 14914 Weakinstruments(cal)_df2 14914 Wu-Hausman_df2 14914 Weakinstruments(prot)_statistic 274.147084958343 Weakinstruments(cal)_statistic 315.036848606231 Wu-Hausman_statistic 94.7020085425169 Sargan_statistic 122.081979628898 Weakinstruments(prot)_p-value 8.61731956233366e-228 Weakinstruments(cal)_p-value 1.18918641220866e-260 Wu-Hausman_p-value 1.35024050408262e-41 Sargan_p-value 3.09196773720398e-27 vars_var.y hgt vars_vars.x prot+cal vars_vars.z momEdu+wealthIdx+p.A.prot+p.A.nProt vars_vars.c sex+hgt0+wgt0 5.1.1.2.5 Examples Line by Line The examples are just to test the code with different types of variables. # Selecting Variables var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # A. create Equation str.vars.x &lt;- paste(vars.x, collapse=&#39;+&#39;) str.vars.c &lt;- paste(vars.c, collapse=&#39;+&#39;) str.vars.z &lt;- paste(vars.z, collapse=&#39;+&#39;) print(str.vars.x) ## [1] &quot;prot+cal&quot; print(str.vars.c) ## [1] &quot;sex+hgt0+wgt0&quot; print(str.vars.z) ## [1] &quot;momEdu+wealthIdx+p.A.prot+p.A.nProt&quot; equa.iv &lt;- paste(var.y, paste(paste(str.vars.x, str.vars.c, sep=&#39;+&#39;), paste(str.vars.z, str.vars.c, sep=&#39;+&#39;), sep=&#39;|&#39;), sep=&#39;~&#39;) print(equa.iv) ## [1] &quot;hgt~prot+cal+sex+hgt0+wgt0|momEdu+wealthIdx+p.A.prot+p.A.nProt+sex+hgt0+wgt0&quot; # B. regression res.ivreg &lt;- ivreg(as.formula(equa.iv), data=df) coef(res.ivreg) ## (Intercept) prot cal sexMale hgt0 wgt0 ## 44.0243196254 -1.4025623247 0.0651048958 0.1208327876 0.2865254380 0.0008504814 # C. Regression Summary ivreg.summ &lt;- summary(res.ivreg, vcov = sandwich, df = Inf, diagnostics = TRUE) ivreg.summ$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 44.0243196254 2.7535484724 15.9882131 1.543966e-57 ## prot -1.4025623247 0.1986400603 -7.0608231 1.655192e-12 ## cal 0.0651048958 0.0075888130 8.5790618 9.565006e-18 ## sexMale 0.1208327876 0.2099845806 0.5754365 5.649961e-01 ## hgt0 0.2865254380 0.0707828183 4.0479518 5.166778e-05 ## wgt0 0.0008504814 0.0003371121 2.5228444 1.164099e-02 ## attr(,&quot;df&quot;) ## [1] 0 ## attr(,&quot;nobs&quot;) ## [1] 14922 ivreg.summ$diagnostics ## df1 df2 statistic p-value ## Weak instruments (prot) 4 14914 274.14708 8.617320e-228 ## Weak instruments (cal) 4 14914 315.03685 1.189186e-260 ## Wu-Hausman 2 14914 94.70201 1.350241e-41 ## Sargan 2 NA 122.08198 3.091968e-27 # D. Combine Regression Results into a Matrix df.results &lt;- suppressMessages(as_tibble(ivreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(as_tibble(ivreg.summ$diagnostics, rownames=&#39;rownames&#39;)) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.z=str.vars.z, vars.c=str.vars.c))) # E. Flatten Matrix, All IV results as a single tibble row to be combined with other IV results df.row.results &lt;- df.results %&gt;% gather(variable, value, -rownames) %&gt;% drop_na() %&gt;% unite(esti.val, rownames, variable) %&gt;% mutate(esti.val = gsub(&#39; &#39;, &#39;&#39;, esti.val)) # F. Results as Single Colum # df.row.results # G. Results as Single Row # df.row.results # t(df.row.results %&gt;% spread(esti.val, value)) %&gt;% # kable() %&gt;% # kable_styling_fc_wide() 5.1.2 IV Loop over RHS Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Regression with a Variety of Outcome Variables and Right Hand Side Variables. There are M outcome variables, and there are N alternative right hand side variables. Regress each M outcome variable and each N alternative right hand side variable, with some common sets of controls and perhaps shared instruments. The output file is a M by N matrix of coefficients, with proper variable names and row names. The matrix stores coefficients for this key endogenous variable. Dependency: R4Econ/linreg/ivreg/ivregdfrow.R 5.1.2.1 Construct Program The program relies on double lapply. lapply is used for convenience, not speed. ff_reg_mbyn &lt;- function(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;, time = FALSE) { # regf.iv() function is from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R if (time) { start_time &lt;- Sys.time() } if (return_all) { df.reg.out.all &lt;- bind_rows(lapply(list.vars.x, function(x) ( bind_rows( lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) ))) } else { df.reg.out.all &lt;- (lapply(list.vars.x, function(x) ( bind_rows( lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) %&gt;% select(vars_var.y, starts_with(x)) %&gt;% select(vars_var.y, ends_with(stats_ends)) ))) %&gt;% reduce(full_join) } if (time) { end_time &lt;- Sys.time() print(paste0(&#39;Estimation for all ys and xs took (seconds):&#39;, end_time - start_time)) } return(df.reg.out.all) } 5.1.2.2 Prepare Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) # Source Dependency source(&#39;C:/Users/fan/R4Econ/linreg/ivreg/ivregdfrow.R&#39;) # Setting options(repr.matrix.max.rows=50, repr.matrix.max.cols=50) Parameters. var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) var.y3 &lt;- c(&#39;vil.id&#39;) list.vars.y &lt;- c(var.y1, var.y2, var.y3) var.x1 &lt;- c(&#39;prot&#39;) var.x2 &lt;- c(&#39;cal&#39;) var.x3 &lt;- c(&#39;wealthIdx&#39;) var.x4 &lt;- c(&#39;p.A.prot&#39;) var.x5 &lt;- c(&#39;p.A.nProt&#39;) list.vars.x &lt;- c(var.x1, var.x2, var.x3, var.x4, var.x5) vars.z &lt;- c(&#39;indi.id&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) 5.1.2.3 Program Testing 5.1.2.3.1 Test Program OLS Z-Stat vars.z &lt;- NULL suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_tvalue cal_tvalue wealthIdx_tvalue p.A.prot_tvalue p.A.nProt_tvalue hgt 18.8756010031786 23.4421863484661 13.508899618216 3.83682180045518 32.5448257554855 wgt 16.3591125056062 17.3686031309332 14.1390521528113 1.36958319982295 12.0961557911467 vil.id -14.9385580468907 -19.6150110809452 34.0972558327347 8.45943342783186 17.7801422421419 5.1.2.3.2 Test Program IV T-stat vars.z &lt;- c(&#39;indi.id&#39;) suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_zvalue cal_zvalue wealthIdx_zvalue p.A.prot_zvalue p.A.nProt_zvalue hgt 8.87674929300964 12.0739764947235 4.62589553677969 26.6373587567312 32.1162192385744 wgt 5.60385871756365 6.1225187008946 5.17869536991717 11.9295584469998 12.3509307017263 vil.id -9.22106223347162 -13.0586007975839 -51.5866689219593 -29.9627476577329 -38.3528894620707 5.1.2.3.3 Test Program OLS Coefficient vars.z &lt;- NULL suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;Estimate&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_Estimate cal_Estimate wealthIdx_Estimate p.A.prot_Estimate p.A.nProt_Estimate hgt 0.049431093806755 0.00243408846205622 0.21045655488185 3.86952250259526e-05 0.00542428867316449 wgt 16.5557424523585 0.699072500364623 106.678721085969 0.00521731297924587 0.779514232050632 vil.id -0.0758835879205584 -0.00395676177098486 0.451733304543324 0.000149388430455142 0.00526237555581024 5.1.2.3.4 Test Program IV coefficient vars.z &lt;- c(&#39;indi.id&#39;) suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;Estimate&#39;))) %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_Estimate cal_Estimate wealthIdx_Estimate p.A.prot_Estimate p.A.nProt_Estimate hgt 0.859205733632614 0.0238724384575419 0.144503490136948 0.00148073028434642 0.0141317656200726 wgt 98.9428234201406 2.71948246216953 69.1816142883022 0.221916473012486 2.11856940494335 vil.id -6.02451379136132 -0.168054407187466 -1.91414470908345 -0.00520794333267238 -0.0494468877742109 5.1.2.3.5 Test Program OLS Return All vars.z &lt;- NULL t(suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = TRUE, stats_ends = &#39;Estimate&#39;)))) %&gt;% kable() %&gt;% kable_styling_fc_wide() X.Intercept._Estimate 27.3528514188608 99.873884728925 31.4646660224049 27.9038445914729 219.626705179399 30.5103987898551 35.7840188807906 -2662.74787734003 29.2381039651127 23.9948407749744 -547.959546430028 22.3367814226238 24.4904444950827 -476.703973630552 22.7781908464511 X.Intercept._Prt.. 5.68247182214952e-231 0.75529705553815 6.78164655340399e-84 8.24252673989353e-242 0.493216914827181 1.62608789535248e-79 2.26726906489443e-145 7.13318862990131e-05 1.53578035267873e-124 2.11912344053336e-165 0.0941551350855875 3.04337266226599e-49 2.34941965806705e-181 0.143844033032183 9.58029450711211e-52 X.Intercept._Std.Error 0.831272666092284 320.450650378664 1.61328519718754 0.828072565159449 320.522532223672 1.60831193651104 1.38461348429899 670.301542938561 1.22602177264147 0.86658104216672 327.343126852912 1.5098937308759 0.843371070670838 326.132837036936 1.5004526558957 X.Intercept._tvalue 32.9047886867776 0.31166697465244 19.503474077155 33.6973421962119 0.685214557790078 18.9704485163756 25.8440491058106 -3.97246270039407 23.8479483950102 27.6890903532576 -1.6739607509042 14.7936116071335 29.0387533397398 -1.46168652614567 15.1808794212527 adj.r.squared_v 0.814249026159781 0.60716936506893 0.0373247512680971 0.81608922805658 0.607863678511207 0.0453498711076042 0.935014931990565 0.92193683733695 0.059543122812776 0.814690803458616 0.617300597776144 0.0261131074199838 0.824542352656376 0.620250730454724 0.0385437355117917 df1_v 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 df2_v 18957 18962 18999 18957 18962 18999 25092 25102 30013 18587 18591 18845 18587 18591 18845 df3_v 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 hgt0_Estimate 0.60391817340617 56.3852027199184 -0.296844389234445 0.589847843438394 52.9707041800704 -0.273219210757899 0.439374451256039 47.176969664749 -0.35908163982046 0.687269209411865 72.105560623359 -0.108789161111504 0.622395388389206 62.7336220289257 -0.157811627494693 hgt0_Prt.. 1.14533314566771e-183 1.52417506966835e-12 1.40290395213743e-13 7.79174951119325e-177 3.05720143843395e-11 8.49149153665126e-12 2.71000479249152e-36 0.00520266507060071 2.41020063623865e-31 1.31914432912869e-220 4.78613024244006e-19 0.0034801146146182 1.11511327164938e-190 8.38546282719268e-15 2.13723119924676e-05 hgt0_Std.Error 0.0206657538633713 7.96735224000553 0.0401060913799595 0.0205836398278421 7.96822145797115 0.0399777363511633 0.0348701896610764 16.8823489375743 0.0307984635553859 0.0213841849324282 8.07744906400683 0.0372288594891345 0.0208846437570215 8.07589192978212 0.0371223237183417 hgt0_tvalue 29.2231378249683 7.0770314931977 -7.40147890309685 28.6561486875877 6.64774497790599 -6.83428417151858 12.6002885423502 2.79445531182864 -11.659076407325 32.1391351404584 8.92677379355593 -2.92217281443323 29.8015803204665 7.76801157994423 -4.25112470577158 prot_Estimate 0.049431093806755 16.5557424523585 -0.0758835879205584 NA NA NA NA NA NA NA NA NA NA NA NA prot_Prt.. 9.54769322304645e-79 9.61203373222183e-60 3.56396093562335e-50 NA NA NA NA NA NA NA NA NA NA NA NA prot_Std.Error 0.00261878251179557 1.01201959743751 0.00507971302734622 NA NA NA NA NA NA NA NA NA NA NA NA prot_tvalue 18.8756010031786 16.3591125056062 -14.9385580468907 NA NA NA NA NA NA NA NA NA NA NA NA r.squared_v 0.814298005954592 0.607272921412825 0.0375780335372857 0.816137722617266 0.60796705182314 0.0456010419476623 0.93502787877066 0.921952383432195 0.0596997716363463 0.814740639193486 0.617403496088206 0.0263714328556815 0.824589538985803 0.620352835549783 0.0387987636986586 sexMale_Estimate 0.935177182449406 415.163616765357 -0.254089999175318 0.893484662055608 405.534891838028 -0.181389489610951 1.80682463132073 999.926876716707 -0.33436777751525 0.932686930233136 397.141948675354 -0.445232370681998 0.96466980500711 401.59056368102 -0.423829627017582 sexMale_Prt.. 2.36432111724607e-51 2.48252880290814e-67 0.0343768259467621 2.08765935335877e-47 2.51355675686752e-64 0.129768754080748 1.26527362032354e-66 2.64630894140004e-86 0.000311174554787706 7.90489020586094e-47 6.19449742677662e-59 7.93666802281971e-05 1.24556615236597e-52 1.18469030741261e-60 0.00015644693636154 sexMale_Std.Error 0.0618482294097262 23.8518341439675 0.120093045309631 0.0616078355613525 23.8567507583516 0.11972270545355 0.104475287357902 50.5879876531386 0.0927193334338799 0.0647209948973267 24.4473730956481 0.112797805327952 0.0629827627260302 24.3549086073387 0.112083516545945 sexMale_tvalue 15.1205166481668 17.4059409544552 -2.11577613441484 14.5027763743757 16.9987478993157 -1.51508010885476 17.2942776901016 19.7660931597596 -3.60623577771614 14.4108867873979 16.2447698213453 -3.94717228218682 15.316409812052 16.4891016491029 -3.78137339083082 sigma_v 4.21029844914315 1623.77111076428 8.18491760066961 4.18939119979502 1622.33549880859 8.15073036560541 8.18607049768594 3964.45339913597 7.93450742809862 4.35662621773428 1645.77655955938 7.6435668370875 4.23923961592693 1639.42085007515 7.59462918474114 svymthRound_Estimate 0.87166589100565 189.04290688382 -0.0154759587993917 0.851989049736817 185.318286001897 0.0201471237605442 0.432815253441723 189.877994795061 0.00215144302579706 0.91961467696139 205.597385664745 -0.0509574460702806 0.921894094780682 205.945143306004 -0.0557204455206461 svymthRound_Prt.. 0 0 0.0397984032097113 0 0 0.0117151185126433 0 0 0.000447277200167272 0 0 1.37139389802397e-18 0 0 7.79141497751766e-23 svymthRound_Std.Error 0.00387681209575621 1.4955473831309 0.00752730297891317 0.00411253488213795 1.59266949679221 0.00799217807522278 0.000728323735328998 0.352701518968252 0.000612792699568233 0.00331108017589107 1.25083486490652 0.00578476859618168 0.00317113547025635 1.22639878616071 0.00565696328562864 svymthRound_tvalue 224.840892330022 126.403823119306 -2.05597660181154 207.168832400006 116.357025971267 2.52085521254888 594.262183761197 538.353209678558 3.51088227277012 277.738571133786 164.368128386085 -8.80889965139067 290.714194782148 167.926734460268 -9.84988636256528 vars_var.y hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id vars_vars.c sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound vars_vars.x prot prot prot cal cal cal wealthIdx wealthIdx wealthIdx p.A.prot p.A.prot p.A.prot p.A.nProt p.A.nProt p.A.nProt wgt0_Estimate -0.000146104685986986 0.637023553461055 -0.000903390591533867 -0.000116898230009949 0.649394003614758 -0.000941137072743919 0.00122231975126219 1.32870822160235 -0.000845938526704796 -0.000489534836079617 0.580023505722658 -0.00156196911156061 3.23596154259101e-05 0.65551206304675 -0.00115432723977403 wgt0_Prt.. 0.136011583497549 2.96480083692757e-63 2.05763549729273e-06 0.230228828649018 7.43034302413852e-66 6.66901196231733e-07 1.22269348058816e-13 6.75367630221077e-62 4.32675510884621e-09 7.77000489086602e-07 7.42419220783427e-54 1.40362012201826e-19 0.740027016459552 4.09082062947785e-67 2.75472781728448e-11 wgt0_Std.Error 9.79994437486573e-05 0.0378027371614794 0.000190221503167431 9.74307633896921e-05 0.037739875283113 0.000189270503626621 0.000164767846917989 0.0798131859486402 0.000144040382619518 9.90410500454311e-05 0.0374185042114355 0.000172365145002826 9.75208524392668e-05 0.0377202854835204 0.000173241059789276 wgt0_tvalue -1.49087260496811 16.8512547316329 -4.74915073475531 -1.19980821193398 17.2071051836606 -4.97244448929308 7.41843614592224 16.6477281392748 -5.872926128913 -4.94274682926991 15.5009805428138 -9.0619777654873 0.331822524275644 17.3782370584956 -6.66312732777158 cal_Estimate NA NA NA 0.00243408846205622 0.699072500364623 -0.00395676177098486 NA NA NA NA NA NA NA NA NA cal_Prt.. NA NA NA 8.01672708877986e-120 4.71331900885298e-67 7.94646124029527e-85 NA NA NA NA NA NA NA NA NA cal_Std.Error NA NA NA 0.000103833679413418 0.0402492068645167 0.000201721108117477 NA NA NA NA NA NA NA NA NA cal_tvalue NA NA NA 23.4421863484661 17.3686031309332 -19.6150110809452 NA NA NA NA NA NA NA NA NA wealthIdx_Estimate NA NA NA NA NA NA 0.21045655488185 106.678721085969 0.451733304543324 NA NA NA NA NA NA wealthIdx_Prt.. NA NA NA NA NA NA 1.93494257274268e-41 3.2548345535026e-45 4.82890644822007e-250 NA NA NA NA NA NA wealthIdx_Std.Error NA NA NA NA NA NA 0.0155791042075745 7.54496977117083 0.0132483771350785 NA NA NA NA NA NA wealthIdx_tvalue NA NA NA NA NA NA 13.508899618216 14.1390521528113 34.0972558327347 NA NA NA NA NA NA p.A.prot_Estimate NA NA NA NA NA NA NA NA NA 3.86952250259526e-05 0.00521731297924587 0.000149388430455142 NA NA NA p.A.prot_Prt.. NA NA NA NA NA NA NA NA NA 0.000125048896903791 0.170833589209346 2.88060045451681e-17 NA NA NA p.A.prot_Std.Error NA NA NA NA NA NA NA NA NA 1.00852286184785e-05 0.00380941660201464 1.76593895713687e-05 NA NA NA p.A.prot_tvalue NA NA NA NA NA NA NA NA NA 3.83682180045518 1.36958319982295 8.45943342783186 NA NA NA p.A.nProt_Estimate NA NA NA NA NA NA NA NA NA NA NA NA 0.00542428867316449 0.779514232050632 0.00526237555581024 p.A.nProt_Prt.. NA NA NA NA NA NA NA NA NA NA NA NA 5.25341325077391e-226 1.47950939943836e-33 3.7685780281174e-70 p.A.nProt_Std.Error NA NA NA NA NA NA NA NA NA NA NA NA 0.000166671307872964 0.06444313759758 0.000295969260771016 p.A.nProt_tvalue NA NA NA NA NA NA NA NA NA NA NA NA 32.5448257554855 12.0961557911467 17.7801422421419 5.1.2.3.6 Test Program IV Return All vars.z &lt;- c(&#39;indi.id&#39;) t(suppressWarnings(suppressMessages( ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = TRUE, stats_ends = &#39;Estimate&#39;)))) %&gt;% kable() %&gt;% kable_styling_fc_wide() X.Intercept._Estimate 40.2173991882938 1408.1626637032 -64.490636067872 39.6732302990235 1325.54736576331 -59.8304089440729 35.5561817357046 -2791.221534909 21.8005242861645 24.3009261707644 -499.067024090554 21.4632286881661 25.299209739617 -352.278518334717 17.9359211844992 X.Intercept._Prz.. 3.69748206920405e-59 0.00217397545504963 0.000109756271656929 1.30030240177373e-103 0.00138952700443324 3.75547414421179e-07 2.01357089467444e-142 1.95034793045284e-05 1.17899313785408e-34 1.97968607369592e-84 0.155922992163314 1.84405333738942e-09 1.29388565624566e-157 0.287184942021997 1.13855583530306e-12 X.Intercept._Std.Error 2.47963650430699 459.377029874119 16.673099250727 1.83545587849039 414.645900526211 11.7754321198995 1.39936229104453 653.605248808641 1.77547715237629 1.2481331128579 351.723712333143 3.57067054655531 0.945826571474308 330.990098562619 2.52170174723203 X.Intercept._zvalue 16.2190704639323 3.06537456626657 -3.86794531107106 21.6149190857443 3.19681772828602 -5.08095230263053 25.4088465605032 -4.27050048939585 12.2786847788984 19.4698193008609 -1.41891776582254 6.01097984491234 26.748254386829 -1.0643173915611 7.11262590993832 hgt0_Estimate 0.403139725681418 35.5765914326678 1.20995060148712 0.357976348180876 31.0172706497394 1.5037447089682 0.460434521499963 59.1545587745268 0.412512139031067 0.515794899569023 46.2591615803265 0.520812513246773 0.510868687340428 45.5654716961559 0.534362107844268 hgt0_Prz.. 1.25009876641748e-13 0.000445802636381424 0.00097112649404847 2.82141265004339e-17 0.0013100303315764 3.70002169470828e-08 2.98739737280869e-37 0.000542570320022534 3.02226357947691e-20 8.57492956381676e-59 2.8561488738123e-07 1.10039023747789e-08 3.24936430168307e-102 6.3454545304127e-08 3.42500501176006e-17 hgt0_Std.Error 0.0543948312973965 10.1318250572006 0.366789440587685 0.0423453726223874 9.65135595900306 0.273179527952317 0.0361031059207763 17.1025823111635 0.0447499166716409 0.0319035514861838 9.01263684093548 0.0911390672920558 0.0237991645877977 8.42434865398195 0.063380058773461 hgt0_zvalue 7.41136089709158 3.51137048180512 3.29876072644971 8.45373003027063 3.21377335801252 5.50460248701607 12.7533216258548 3.45880859967647 9.21816552325528 16.1673191711084 5.13270005180026 5.71448149208973 21.4658243761363 5.40878275196011 8.4310762436216 prot_Estimate 0.859205733632614 98.9428234201406 -6.02451379136132 NA NA NA NA NA NA NA NA NA NA NA NA prot_Prz.. 6.88427338202428e-19 2.09631602352917e-08 2.94171378745816e-20 NA NA NA NA NA NA NA NA NA NA NA NA prot_Std.Error 0.0967928354481331 17.6561952052848 0.653342710289155 NA NA NA NA NA NA NA NA NA NA NA NA prot_zvalue 8.87674929300964 5.60385871756365 -9.22106223347162 NA NA NA NA NA NA NA NA NA NA NA NA Sargan_df1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 sexMale_Estimate 0.154043421788007 333.799680049259 5.41175429817609 0.106307556057668 330.452608866758 5.83118942788808 1.80283907885782 997.747599807148 -0.452827875182598 1.02741625216018 411.365911332896 -0.789122421167432 1.02009164592608 409.820707458838 -0.746032636368145 sexMale_Prz.. 0.38807812932888 5.06413216642981e-24 5.80077629932476e-06 0.423490075745117 2.52735690930834e-27 6.12283824664132e-12 1.1689328480129e-65 2.02347084785411e-89 0.000647195788038449 1.69796551008584e-27 2.05327249429949e-54 0.00428270841484855 1.70848440093529e-51 2.36314216739034e-62 6.57521045473888e-05 sexMale_Std.Error 0.178475271469781 33.0216035385405 1.19371921154418 0.132821186086547 30.5174257711927 0.847955715223327 0.105343525210948 49.7632792630648 0.132754263303719 0.0945646985181925 26.4822313532216 0.276250047248363 0.0675715533063635 24.5920104216267 0.18692145837209 sexMale_zvalue 0.86310792817082 10.1085242471545 4.53352366774387 0.800381017440976 10.8283251459136 6.87676174970095 17.113904962338 20.0498764266063 -3.41102322376347 10.8646912458831 15.5336574870174 -2.85655126226267 15.0964658352764 16.6647907361992 -3.99115565898846 svymthRound_Estimate 0.20990165085783 121.78985943172 4.84745570027424 0.322893837128574 135.494858749214 4.07024693316581 0.433164820953121 190.07735139541 0.0137438264666969 1.00582859923509 218.549980922774 -0.369567838754916 0.929266902426869 207.078222946319 -0.0985678389223824 svymthRound_Prz.. 0.00846239710392287 5.96047652813855e-17 2.07373887977152e-19 9.66146445882893e-11 4.48931446042076e-34 5.64723572160763e-36 0 0 1.57416908709431e-66 0 0 2.42696379701225e-102 0 0 1.84569897952709e-27 svymthRound_Std.Error 0.0797183179471441 14.5577085129475 0.538050140685815 0.0498896912188091 11.133488331472 0.325043349284718 0.00120472816008751 0.739269879490032 0.000797655931686456 0.00746867714609297 1.9315711781906 0.0172056989832505 0.00539330635998817 1.46167854745858 0.00907867488118012 svymthRound_zvalue 2.63304164291327 8.36600480930094 9.00930105527994 6.47215545416802 12.1700274626596 12.5221664806331 359.553993426746 257.11496798237 17.2302692435808 134.672925279848 113.146221785884 -21.4793853545086 172.300040161061 141.671520941705 -10.8570733297996 vars_var.y hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id hgt wgt vil.id vars_vars.c sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound sex+wgt0+hgt0+svymthRound vars_vars.x prot prot prot cal cal cal wealthIdx wealthIdx wealthIdx p.A.prot p.A.prot p.A.prot p.A.nProt p.A.nProt p.A.nProt vars_vars.z indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id indi.id Weakinstruments_df1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Weakinstruments_df2 18957 18962 18999 18957 18962 18999 25092 25102 30013 18587 18591 18845 18587 18591 18845 Weakinstruments_p.value 1.42153759923994e-19 4.45734829676713e-19 5.72345606957941e-20 1.77770827184424e-37 4.03760292920738e-37 5.47447735093002e-38 0 0 0 0 0 0 0 0 0 Weakinstruments_statistic 82.0931934821266 79.8251182827386 83.8989817367586 164.392129625299 162.747072038429 166.75260665498 7029.47383089383 7038.38467113128 12942.6315513372 1710.98122418591 1715.15052113399 1725.71954882902 5097.88462603711 5110.7741807338 5136.55662964887 wgt0_Estimate -0.00163274724538111 0.492582112313709 0.00999798623641602 -0.000658938519302931 0.601258436431587 0.00326074237566435 0.00112485055604169 1.27282038539707 -0.00512158791392237 0.000716628918444932 0.761704518610475 -0.00601345031606092 0.000922100117259348 0.792700893714085 -0.00668277875606482 wgt0_Prz.. 4.88365163639597e-08 2.33136555228405e-20 7.95432753711715e-07 0.00032843149807424 2.0921134733036e-48 0.00667886646012294 2.26123807446765e-11 6.67525280062144e-56 6.51923753120087e-127 2.43477572076212e-06 8.2201479288098e-69 5.19751747217521e-44 1.68237436753105e-15 4.81415543564975e-82 2.54848840100353e-105 wgt0_Std.Error 0.00029928487659495 0.0532753838702833 0.00202532507408065 0.000183457551985601 0.0411255751282477 0.00120214094164169 0.000168187467853553 0.08080475140115 0.000213715312589078 0.000152036990658929 0.0434474820359048 0.00043218241369976 0.00011580150512068 0.0413159097814445 0.000306609919182859 wgt0_zvalue -5.45549532591606 9.24596082710666 4.93648469787221 -3.59177647456371 14.6200614716414 2.71244598924594 6.68807593334564 15.7518012657231 -23.9645341827701 4.71351685756907 17.531614789115 -13.9141485757875 7.96276452796019 19.1863351892132 -21.7957030675165 Wu.Hausman_df1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Wu.Hausman_df2 18956 18961 18998 18956 18961 18998 25091 25101 30012 18586 18590 18844 18586 18590 18844 Wu.Hausman_p.value 1.53929570343279e-118 3.13415891402799e-08 0 2.88592507054107e-108 7.6495944085204e-07 0 0.0221987672063003 0.0099360023036833 0 1.80909125272768e-238 2.14946499922491e-35 0 3.15182965429765e-108 1.7681125741529e-17 0 Wu.Hausman_statistic 543.467268879953 30.6481856102772 5652.51924792859 494.955883488045 24.4605456760994 5583.56513052781 5.23078768861684 6.6473469952822 25949.7118056025 1119.87022468742 154.793296861581 4826.92242730041 494.903094649183 72.530787010352 7607.83405438193 cal_Estimate NA NA NA 0.0238724384575419 2.71948246216953 -0.168054407187466 NA NA NA NA NA NA NA NA NA cal_Prz.. NA NA NA 1.44956616452661e-33 9.21076021290446e-10 5.67614501764414e-39 NA NA NA NA NA NA NA NA NA cal_Std.Error NA NA NA 0.00197718112735887 0.444177077282291 0.0128692506794877 NA NA NA NA NA NA NA NA NA cal_zvalue NA NA NA 12.0739764947235 6.1225187008946 -13.0586007975839 NA NA NA NA NA NA NA NA NA wealthIdx_Estimate NA NA NA NA NA NA 0.144503490136948 69.1816142883022 -1.91414470908345 NA NA NA NA NA NA wealthIdx_Prz.. NA NA NA NA NA NA 3.72983264926432e-06 2.23442991281176e-07 0 NA NA NA NA NA NA wealthIdx_Std.Error NA NA NA NA NA NA 0.0312379492766376 13.358888551386 0.0371054140359243 NA NA NA NA NA NA wealthIdx_zvalue NA NA NA NA NA NA 4.62589553677969 5.17869536991717 -51.5866689219593 NA NA NA NA NA NA p.A.prot_Estimate NA NA NA NA NA NA NA NA NA 0.00148073028434642 0.221916473012486 -0.00520794333267238 NA NA NA p.A.prot_Prz.. NA NA NA NA NA NA NA NA NA 2.50759287066563e-156 8.30126393398654e-33 3.00201194005694e-197 NA NA NA p.A.prot_Std.Error NA NA NA NA NA NA NA NA NA 5.55884799941827e-05 0.0186022369560791 0.000173813943639721 NA NA NA p.A.prot_zvalue NA NA NA NA NA NA NA NA NA 26.6373587567312 11.9295584469998 -29.9627476577329 NA NA NA p.A.nProt_Estimate NA NA NA NA NA NA NA NA NA NA NA NA 0.0141317656200726 2.11856940494335 -0.0494468877742109 p.A.nProt_Prz.. NA NA NA NA NA NA NA NA NA NA NA NA 2.61782083774363e-226 4.81511329043196e-35 0 p.A.nProt_Std.Error NA NA NA NA NA NA NA NA NA NA NA NA 0.000440019589949091 0.17153115470458 0.00128926108222202 p.A.nProt_zvalue NA NA NA NA NA NA NA NA NA NA NA NA 32.1162192385744 12.3509307017263 -38.3528894620707 5.1.2.4 Program Line by Line Set Up Parameters vars.z &lt;- c(&#39;indi.id&#39;) vars.z &lt;- NULL vars.c &lt;- c(&#39;sex&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) 5.1.2.4.1 Lapply df.reg.out &lt;- as_tibble( bind_rows(lapply(list.vars.y, regf.iv, vars.x=var.x1, vars.c=vars.c, vars.z=vars.z, df=df))) 5.1.2.4.2 Nested Lapply Test lapply(list.vars.y, function(y) (mean(df[[var.x1]], na.rm=TRUE) + mean(df[[y]], na.rm=TRUE))) ## [[1]] ## [1] 98.3272 ## ## [[2]] ## [1] 13626.51 ## ## [[3]] ## [1] 26.11226 lapplytwice &lt;- lapply( list.vars.x, function(x) ( lapply(list.vars.y, function(y) (mean(df[[x]], na.rm=TRUE) + mean(df[[y]], na.rm=TRUE))))) # lapplytwice 5.1.2.4.3 Nested Lapply All df.reg.out.all &lt;- bind_rows( lapply(list.vars.x, function(x) ( bind_rows( lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) ))) # df.reg.out.all %&gt;% # kable() %&gt;% # kable_styling_fc_wide() 5.1.2.4.4 Nested Lapply Select df.reg.out.all &lt;- (lapply(list.vars.x, function(x) ( bind_rows(lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) %&gt;% select(vars_var.y, starts_with(x)) %&gt;% select(vars_var.y, ends_with(&#39;value&#39;)) ))) %&gt;% reduce(full_join) df.reg.out.all %&gt;% kable() %&gt;% kable_styling_fc_wide() vars_var.y prot_tvalue cal_tvalue wealthIdx_tvalue p.A.prot_tvalue p.A.nProt_tvalue hgt 18.8756010031786 23.4421863484661 13.508899618216 3.83682180045518 32.5448257554855 wgt 16.3591125056062 17.3686031309332 14.1390521528113 1.36958319982295 12.0961557911467 vil.id -14.9385580468907 -19.6150110809452 34.0972558327347 8.45943342783186 17.7801422421419 5.2 Decomposition 5.2.1 Decompose RHS Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). One runs a number of regressions. With different outcomes, and various right hand side variables. What is the remaining variation in the left hand side variable if right hand side variable one by one is set to the average of the observed values. Dependency: R4Econ/linreg/ivreg/ivregdfrow.R The code below does not work with categorical variables (except for dummies). Dummy variable inputs need to be converted to zero/one first. The examples are just to test the code with different types of variables. # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) # Source Dependency source(&#39;C:/Users/fan/R4Econ/linreg/ivreg/ivregdfrow.R&#39;) Data Cleaning. # Convert Variable for Sex which is categorical to Numeric df &lt;- df df$male &lt;- (as.numeric(factor(df$sex)) - 1) summary(factor(df$sex)) ## Female Male ## 16446 18619 summary(df$male) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 0.000 1.000 0.531 1.000 1.000 df.use &lt;- df %&gt;% filter(S.country == &#39;Guatemala&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) dim(df.use) ## [1] 2022 16 Setting Up Parameters. # Define Left Hand Side Variab les var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) vars.y &lt;- c(var.y1, var.y2) # Define Right Hand Side Variables vars.x &lt;- c(&#39;prot&#39;) vars.c &lt;- c(&#39;male&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) # vars.z &lt;- c(&#39;p.A.prot&#39;) vars.z &lt;- c(&#39;vil.id&#39;) # vars.z &lt;- NULL vars.xc &lt;- c(vars.x, vars.c) # Other variables to keep vars.other.keep &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;, &#39;svymthRound&#39;) # Decompose sequence vars.tomean.first &lt;- c(&#39;male&#39;, &#39;hgt0&#39;) var.tomean.first.name.suffix &lt;- &#39;_mh02m&#39; vars.tomean.second &lt;- c(vars.tomean.first, &#39;hgt0&#39;, &#39;wgt0&#39;) var.tomean.second.name.suffix &lt;- &#39;_mh0me2m&#39; vars.tomean.third &lt;- c(vars.tomean.second, &#39;prot&#39;) var.tomean.third.name.suffix &lt;- &#39;_mh0mep2m&#39; vars.tomean.fourth &lt;- c(vars.tomean.third, &#39;svymthRound&#39;) var.tomean.fourth.name.suffix &lt;- &#39;_mh0mepm2m&#39; list.vars.tomean = list( # vars.tomean.first, vars.tomean.second, vars.tomean.third, vars.tomean.fourth ) list.vars.tomean.name.suffix &lt;- list( # var.tomean.first.name.suffix, var.tomean.second.name.suffix, var.tomean.third.name.suffix, var.tomean.fourth.name.suffix ) 5.2.1.1 Obtain Regression Coefficients from somewhere # Regressions # regf.iv from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R df.reg.out &lt;- as_tibble( bind_rows(lapply(vars.y, regf.iv, vars.x=vars.x, vars.c=vars.c, vars.z=vars.z, df=df))) # Regressions # reg1 &lt;- regf.iv(var.y = var.y1, vars.x, vars.c, vars.z, df.use) # reg2 &lt;- regf.iv(var.y = var.y2, vars.x, vars.c, vars.z, df.use) # df.reg.out &lt;- as_tibble(bind_rows(reg1, reg2)) # df.reg.out # Select Variables str.esti.suffix &lt;- &#39;_Estimate&#39; arr.esti.name &lt;- paste0(vars.xc, str.esti.suffix) str.outcome.name &lt;- &#39;vars_var.y&#39; arr.columns2select &lt;- c(arr.esti.name, str.outcome.name) arr.columns2select ## [1] &quot;prot_Estimate&quot; &quot;male_Estimate&quot; &quot;wgt0_Estimate&quot; &quot;hgt0_Estimate&quot; &quot;svymthRound_Estimate&quot; ## [6] &quot;vars_var.y&quot; # Generate dataframe for coefficients df.coef &lt;- df.reg.out[,c(arr.columns2select)] %&gt;% mutate_at(vars(arr.esti.name), as.numeric) %&gt;% column_to_rownames(str.outcome.name) df.coef %&gt;% kable() %&gt;% kable_styling_fc() prot_Estimate male_Estimate wgt0_Estimate hgt0_Estimate svymthRound_Estimate hgt -0.2714772 1.244735 0.0004430 0.6834853 1.133919 wgt -59.0727542 489.852902 0.7696158 75.4867897 250.778883 str(df.coef) ## &#39;data.frame&#39;: 2 obs. of 5 variables: ## $ prot_Estimate : num -0.271 -59.073 ## $ male_Estimate : num 1.24 489.85 ## $ wgt0_Estimate : num 0.000443 0.769616 ## $ hgt0_Estimate : num 0.683 75.487 ## $ svymthRound_Estimate: num 1.13 250.78 5.2.1.2 Decomposition Step 1 # Decomposition Step 1: gather df.decompose_step1 &lt;- df.use %&gt;% filter(svymthRound %in% c(12, 18, 24)) %&gt;% select(one_of(c(vars.other.keep, vars.xc, vars.y))) %&gt;% drop_na() %&gt;% gather(variable, value, -one_of(c(vars.other.keep, vars.xc))) options(repr.matrix.max.rows=20, repr.matrix.max.cols=20) dim(df.decompose_step1) ## [1] 1382 10 head(df.decompose_step1, 10) %&gt;% kable() %&gt;% kable_styling_fc() S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value Guatemala 3 1352 18 13.3 1 2545.2 47.4 hgt 70.2 Guatemala 3 1352 24 46.3 1 2545.2 47.4 hgt 75.8 Guatemala 3 1354 12 1.0 1 3634.3 51.2 hgt 66.3 Guatemala 3 1354 18 9.8 1 3634.3 51.2 hgt 69.2 Guatemala 3 1354 24 15.4 1 3634.3 51.2 hgt 75.3 Guatemala 3 1356 12 8.6 1 3911.8 51.9 hgt 68.1 Guatemala 3 1356 18 17.8 1 3911.8 51.9 hgt 74.1 Guatemala 3 1356 24 30.5 1 3911.8 51.9 hgt 77.1 Guatemala 3 1357 12 1.0 1 3791.4 52.6 hgt 71.5 Guatemala 3 1357 18 12.7 1 3791.4 52.6 hgt 77.8 5.2.1.3 Decomposition Step 2 # Decomposition Step 2: mutate_at(vars, funs(mean = mean(.))) # the xc averaging could have taken place earlier, no difference in mean across variables df.decompose_step2 &lt;- df.decompose_step1 %&gt;% group_by(variable) %&gt;% mutate_at(vars(c(vars.xc, &#39;value&#39;)), funs(mean = mean(.))) %&gt;% ungroup() options(repr.matrix.max.rows=20, repr.matrix.max.cols=20) dim(df.decompose_step2) ## [1] 1382 16 head(df.decompose_step2,10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean Guatemala 3 1352 18 13.3 1 2545.2 47.4 hgt 70.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1352 24 46.3 1 2545.2 47.4 hgt 75.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1354 12 1.0 1 3634.3 51.2 hgt 66.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1354 18 9.8 1 3634.3 51.2 hgt 69.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1354 24 15.4 1 3634.3 51.2 hgt 75.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1356 12 8.6 1 3911.8 51.9 hgt 68.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1356 18 17.8 1 3911.8 51.9 hgt 74.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1356 24 30.5 1 3911.8 51.9 hgt 77.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1357 12 1.0 1 3791.4 52.6 hgt 71.5 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 Guatemala 3 1357 18 12.7 1 3791.4 52.6 hgt 77.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 5.2.1.4 Decomposition Step 3 Non-Loop ff_lr_decompose_valadj &lt;- function(df, df.coef, vars.tomean, str.esti.suffix) { new_value &lt;- (df$value + rowSums((df[paste0(vars.tomean, &#39;_mean&#39;)] - df[vars.tomean]) *df.coef[df$variable, paste0(vars.tomean, str.esti.suffix)])) return(new_value) } 5.2.1.5 Decomposition Step 3 With Loop df.decompose_step3 &lt;- df.decompose_step2 for (i in 1:length(list.vars.tomean)) { var.decomp.cur &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[i]])) vars.tomean &lt;- list.vars.tomean[[i]] var.decomp.cur df.decompose_step3 &lt;- df.decompose_step3 %&gt;% mutate((!!var.decomp.cur) := ff_lr_decompose_valadj(., df.coef, vars.tomean, str.esti.suffix)) } dim(df.decompose_step3) ## [1] 1382 19 head(df.decompose_step3, 10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean value_mh0me2m value_mh0mep2m value_mh0mepm2m Guatemala 3 1352 18 13.3 1 2545.2 47.4 hgt 70.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 73.19390 71.19903 71.68148 Guatemala 3 1352 24 46.3 1 2545.2 47.4 hgt 75.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 78.79390 85.75778 79.43671 Guatemala 3 1354 12 1.0 1 3634.3 51.2 hgt 66.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 63.61689 58.28285 65.56882 Guatemala 3 1354 18 9.8 1 3634.3 51.2 hgt 69.2 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 66.51689 63.57185 64.05430 Guatemala 3 1354 24 15.4 1 3634.3 51.2 hgt 75.3 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 72.61689 71.19213 64.87106 Guatemala 3 1356 12 8.6 1 3911.8 51.9 hgt 68.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 64.33707 61.06626 68.35222 Guatemala 3 1356 18 17.8 1 3911.8 51.9 hgt 74.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 70.33707 69.56385 70.04630 Guatemala 3 1356 24 30.5 1 3911.8 51.9 hgt 77.1 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 73.33707 76.01161 69.69055 Guatemala 3 1357 12 1.0 1 3791.4 52.6 hgt 71.5 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 66.83353 61.49949 68.78545 Guatemala 3 1357 18 12.7 1 3791.4 52.6 hgt 77.8 20.64819 0.5499276 3312.297 49.75137 18.42547 73.41216 73.13353 70.97578 71.45823 5.2.1.6 Decomposition Step 4 Variance df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;)) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% select(matches(&#39;value&#39;)) %&gt;% select(ends_with(&quot;_var&quot;)) %&gt;% mutate_if(is.numeric, funs( frac = (./value_var))) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% kable() %&gt;% kable_styling_fc_wide() value_var value_mean_var value_mh0me2m_var value_mh0mep2m_var value_mh0mepm2m_var value_var_frac value_mean_var_frac value_mh0me2m_var_frac value_mh0mep2m_var_frac value_mh0mepm2m_var_frac 21.864 NA 25.35 49.047 23.06 1 NA 1.159 2.243 1.055 2965693.245 NA 2949187.64 4192769.518 3147506.60 1 NA 0.994 1.414 1.061 5.2.1.7 Graphical Results Graphically, difficult to pick up exact differences in variance, a 50 percent reduction in variance visually does not look like 50 percent. Intuitively, we are kind of seeing standard deviation, not variance on the graph if we think abou the x-scale. head(df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;), -value_mean), 10) %&gt;% kable() %&gt;% kable_styling_fc() variable value value_mh0me2m value_mh0mep2m value_mh0mepm2m hgt 70.2 73.19390 71.19903 71.68148 hgt 75.8 78.79390 85.75778 79.43671 hgt 66.3 63.61689 58.28285 65.56882 hgt 69.2 66.51689 63.57185 64.05430 hgt 75.3 72.61689 71.19213 64.87106 hgt 68.1 64.33707 61.06626 68.35222 hgt 74.1 70.33707 69.56385 70.04630 hgt 77.1 73.33707 76.01161 69.69055 hgt 71.5 66.83353 61.49949 68.78545 hgt 77.8 73.13353 70.97578 71.45823 df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;), -value_mean) %&gt;% rename(outcome = variable) %&gt;% gather(variable, value, -outcome) %&gt;% ggplot(aes(x=value, color = variable, fill = variable)) + geom_line(stat = &quot;density&quot;) + facet_wrap(~ outcome, scales=&#39;free&#39;, nrow=2) 5.2.1.8 Additional Decomposition Testings head(df.decompose_step2[vars.tomean.first],3) head(df.decompose_step2[paste0(vars.tomean.first, &#39;_mean&#39;)], 3) head(df.coef[df.decompose_step2$variable, paste0(vars.tomean.first, str.esti.suffix)], 3) df.decompose.tomean.first &lt;- df.decompose_step2 %&gt;% mutate(pred_new = df.decompose_step2$value + rowSums((df.decompose_step2[paste0(vars.tomean.first, &#39;_mean&#39;)] - df.decompose_step2[vars.tomean.first]) *df.coef[df.decompose_step2$variable, paste0(vars.tomean.first, str.esti.suffix)])) %&gt;% select(variable, value, pred_new) head(df.decompose.tomean.first, 10) df.decompose.tomean.first %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, sd = sd)) %&gt;% kable() %&gt;% kable_styling_fc() variable value_mean pred_new_mean value_sd pred_new_sd hgt 73.41216 73.41216 4.675867 4.534947 wgt 8807.87656 8807.87656 1722.118824 1695.221845 Note the r-square from regression above matches up with the 1 - ratio below. This is the proper decomposition method that is equivalent to r2. df.decompose_step2 %&gt;% mutate(pred_new = df.decompose_step2$value + rowSums((df.decompose_step2[paste0(vars.tomean.second, &#39;_mean&#39;)] - df.decompose_step2[vars.tomean.second]) *df.coef[df.decompose_step2$variable, paste0(vars.tomean.second, str.esti.suffix)])) %&gt;% select(variable, value, pred_new) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% mutate(ratio = (pred_new_var/value_var)) %&gt;% kable() %&gt;% kable_styling_fc() variable value_mean pred_new_mean value_var pred_new_var ratio hgt 73.41216 73.41216 2.186374e+01 25.3504 1.1594724 wgt 8807.87656 8807.87656 2.965693e+06 2949187.6357 0.9944345 "],["nonlinear-and-other-regressions.html", "Chapter 6 Nonlinear and Other Regressions 6.1 Logit Regression 6.2 Quantile Regression", " Chapter 6 Nonlinear and Other Regressions 6.1 Logit Regression 6.1.1 Binary Logit Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Data Preparation df_mtcars &lt;- mtcars # X-variables to use on RHS ls_st_xs &lt;- c(&#39;mpg&#39;, &#39;qsec&#39;) ls_st_xs &lt;- c(&#39;mpg&#39;) ls_st_xs &lt;- c(&#39;qsec&#39;) ls_st_xs &lt;- c(&#39;wt&#39;) ls_st_xs &lt;- c(&#39;mpg&#39;, &#39;wt&#39;, &#39;vs&#39;) svr_binary &lt;- &#39;hpLowHigh&#39; svr_binary_lb0 &lt;- &#39;LowHP&#39; svr_binary_lb1 &lt;- &#39;HighHP&#39; svr_outcome &lt;- &#39;am&#39; sdt_name &lt;- &#39;mtcars&#39; # Discretize hp df_mtcars &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := cut(hp, breaks=c(-Inf, 210, Inf), labels=c(svr_binary_lb0, svr_binary_lb1))) 6.1.1.1 Logit Regresion and Prediction logit regression with glm, and predict using estimation data. Prediction and estimation with one variable. LOGIT REGRESSION R DATA ANALYSIS EXAMPLES Generalized Linear Models # Regress rs_logit &lt;- glm(as.formula(paste(svr_outcome, &quot;~&quot;, paste(ls_st_xs, collapse=&quot;+&quot;))) ,data = df_mtcars, family = &quot;binomial&quot;) summary(rs_logit) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~&quot;, paste(ls_st_xs, ## collapse = &quot;+&quot;))), family = &quot;binomial&quot;, data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.73603 -0.25477 -0.04891 0.13402 1.90321 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 22.69008 13.95112 1.626 0.1039 ## mpg -0.01786 0.33957 -0.053 0.9581 ## wt -6.73804 3.01400 -2.236 0.0254 * ## vs -4.44046 2.84247 -1.562 0.1182 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 13.092 on 28 degrees of freedom ## AIC: 21.092 ## ## Number of Fisher Scoring iterations: 7 # Predcit Using Regression Data df_mtcars$p_mpg &lt;- predict(rs_logit, newdata = df_mtcars, type = &quot;response&quot;) 6.1.1.1.1 Prediction with Observed Binary Input Logit regression with a continuous variable and a binary variable. Predict outcome with observed continuous variable as well as observed binary input variable. # Regress rs_logit_bi &lt;- glm(as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary,&quot;) + &quot;, paste(ls_st_xs, collapse=&quot;+&quot;))) , data = df_mtcars, family = &quot;binomial&quot;) summary(rs_logit_bi) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary, ## &quot;) + &quot;, paste(ls_st_xs, collapse = &quot;+&quot;))), family = &quot;binomial&quot;, ## data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.45771 -0.09563 -0.00875 0.00555 1.87612 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.8285 18.0390 0.212 0.8319 ## factor(hpLowHigh)HighHP 6.9907 5.5176 1.267 0.2052 ## mpg 0.8985 0.8906 1.009 0.3131 ## wt -6.7291 3.3166 -2.029 0.0425 * ## vs -5.9206 4.1908 -1.413 0.1577 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.2297 on 31 degrees of freedom ## Residual deviance: 8.9777 on 27 degrees of freedom ## AIC: 18.978 ## ## Number of Fisher Scoring iterations: 9 # Predcit Using Regresion Data df_mtcars$p_mpg_hp &lt;- predict(rs_logit_bi, newdata = df_mtcars, type = &quot;response&quot;) # Predicted Probabilities am on mgp with or without hp binary scatter &lt;- ggplot(df_mtcars, aes(x=p_mpg_hp, y=p_mpg)) + geom_point(size=1) + # geom_smooth(method=lm) + # Trend line geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Predicted Probabilities &#39;, svr_outcome, &#39; on &#39;, ls_st_xs, &#39; with or without hp binary&#39;), x = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; indicator, 1 is high&#39;), y = paste0(&#39;prediction with only &#39;, ls_st_xs), caption = &#39;mtcars; prediction based on observed data&#39;) + theme_bw() print(scatter) 6.1.1.1.2 Prediction with Binary set to 0 and 1 Now generate two predictions. One set where binary input is equal to 0, and another where the binary inputs are equal to 1. Ignore whether in data binary input is equal to 0 or 1. Use the same regression results as what was just derived. Note that given the example here, the probability changes a lot when we # Previous regression results summary(rs_logit_bi) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary, ## &quot;) + &quot;, paste(ls_st_xs, collapse = &quot;+&quot;))), family = &quot;binomial&quot;, ## data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.45771 -0.09563 -0.00875 0.00555 1.87612 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.8285 18.0390 0.212 0.8319 ## factor(hpLowHigh)HighHP 6.9907 5.5176 1.267 0.2052 ## mpg 0.8985 0.8906 1.009 0.3131 ## wt -6.7291 3.3166 -2.029 0.0425 * ## vs -5.9206 4.1908 -1.413 0.1577 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.2297 on 31 degrees of freedom ## Residual deviance: 8.9777 on 27 degrees of freedom ## AIC: 18.978 ## ## Number of Fisher Scoring iterations: 9 # Two different dataframes, mutate the binary regressor df_mtcars_bi0 &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := svr_binary_lb0) df_mtcars_bi1 &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := svr_binary_lb1) # Predcit Using Regresion Data df_mtcars$p_mpg_hp_bi0 &lt;- predict(rs_logit_bi, newdata = df_mtcars_bi0, type = &quot;response&quot;) df_mtcars$p_mpg_hp_bi1 &lt;- predict(rs_logit_bi, newdata = df_mtcars_bi1, type = &quot;response&quot;) # Predicted Probabilities and Binary Input scatter &lt;- ggplot(df_mtcars, aes(x=p_mpg_hp_bi0)) + geom_point(aes(y=p_mpg_hp), size=4, shape=4, color=&quot;red&quot;) + geom_point(aes(y=p_mpg_hp_bi1), size=2, shape=8) + # geom_smooth(method=lm) + # Trend line geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Predicted Probabilities and Binary Input&#39;, &#39;\\ncross(shape=4)/red is predict actual binary data&#39;, &#39;\\nstar(shape=8)/black is predict set binary = 1 for all&#39;), x = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; = 0 for all&#39;), y = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; = 1&#39;), caption = paste0(sdt_name)) + theme_bw() print(scatter) 6.1.1.1.3 Prediction with Binary set to 0 and 1 Difference What is the difference in probability between binary = 0 vs binary = 1. How does that relate to the probability of outcome of interest when binary = 0 for all. In the binary logit case, the relationship will be humpshaped by construction between \\(A_i\\) and \\(\\alpha_i\\). In the exponential wage cases, the relationship is convex upwards. # Generate Gap Variable df_mtcars &lt;- df_mtcars %&gt;% mutate(alpha_i = p_mpg_hp_bi1 - p_mpg_hp_bi0) %&gt;% mutate(A_i = p_mpg_hp_bi0) # Binary Marginal Effects and Prediction without Binary scatter &lt;- ggplot(df_mtcars, aes(x=A_i)) + geom_point(aes(y=alpha_i), size=4, shape=4, color=&quot;red&quot;) + geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Binary Marginal Effects and Prediction without Binary&#39;), x = &#39;P(binary=0) for all&#39;, y = &#39;P(binary=1) - P(binary=0) gap&#39;, caption = paste0(sdt_name)) + theme_bw() print(scatter) 6.1.1.1.4 X variables and A and alpha Given the x-variables included in the logit regression, how do they relate to A_i and alpha_i # Generate Gap Variable df_mtcars &lt;- df_mtcars %&gt;% mutate(alpha_i = p_mpg_hp_bi1 - p_mpg_hp_bi0) %&gt;% mutate(A_i = p_mpg_hp_bi0) # Binary Marginal Effects and Prediction without Binary ggplot.A.alpha.x &lt;- function(svr_x, df, svr_alpha = &#39;alpha_i&#39;, svr_A = &quot;A_i&quot;){ scatter &lt;- ggplot(df, aes(x=!!sym(svr_x))) + geom_point(aes(y=alpha_i), size=4, shape=4, color=&quot;red&quot;) + geom_point(aes(y=A_i), size=2, shape=8, color=&quot;blue&quot;) + geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;A (blue) and alpha (red) vs x variables=&#39;, svr_x), x = svr_x, y = &#39;Probabilities&#39;, caption = paste0(sdt_name)) + theme_bw() return(scatter) } # Plot over multiple lapply(ls_st_xs, ggplot.A.alpha.x, df = df_mtcars) ## [[1]] ## ## [[2]] ## ## [[3]] 6.2 Quantile Regression 6.2.1 Quantile Regression Basics Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 6.2.1.1 Estimate Mean and Conditional Quantile Coefficients using mtcars dataset Here, we conduct tests for using the quantreg package, using the built-in mtcars dataset. First, estimate the mean (OLS) regression: fit_mean &lt;- lm(mpg ~ disp + hp + factor(am) + factor(vs), data = mtcars) summary(fit_mean) ## ## Call: ## lm(formula = mpg ~ disp + hp + factor(am) + factor(vs), data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7981 -1.9532 0.0111 1.5665 5.6321 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.832119 2.890418 8.591 3.32e-09 *** ## disp -0.008304 0.010087 -0.823 0.41757 ## hp -0.037623 0.013846 -2.717 0.01135 * ## factor(am)1 4.419257 1.493243 2.960 0.00634 ** ## factor(vs)1 2.052472 1.627096 1.261 0.21794 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.812 on 27 degrees of freedom ## Multiple R-squared: 0.8104, Adjusted R-squared: 0.7823 ## F-statistic: 28.85 on 4 and 27 DF, p-value: 2.13e-09 Now estimate conditional quantile regressions (not that this remains linear) at various quantiles, standard error obtained via bootstrap. Note that there is a gradient in the quantile hp coefficients as well as disp. disp sign reverses, also the coefficient on factor am is different by quantiles: ls_fl_quantiles &lt;- c(0.25, 0.50, 0.75) fit_quantiles &lt;- rq(mpg ~ disp + hp + factor(am), tau = ls_fl_quantiles, data = mtcars) summary(fit_quantiles, se = &quot;boot&quot;) ## ## Call: rq(formula = mpg ~ disp + hp + factor(am), tau = ls_fl_quantiles, ## data = mtcars) ## ## tau: [1] 0.25 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.34665 1.38870 18.25209 0.00000 ## disp -0.02441 0.00798 -3.06028 0.00484 ## hp -0.01672 0.01409 -1.18621 0.24551 ## factor(am)1 1.39719 1.41689 0.98610 0.33253 ## ## Call: rq(formula = mpg ~ disp + hp + factor(am), tau = ls_fl_quantiles, ## data = mtcars) ## ## tau: [1] 0.5 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.49722 1.75659 15.65379 0.00000 ## disp -0.02253 0.01634 -1.37904 0.17880 ## hp -0.02713 0.02414 -1.12386 0.27062 ## factor(am)1 3.37328 2.25738 1.49434 0.14627 ## ## Call: rq(formula = mpg ~ disp + hp + factor(am), tau = ls_fl_quantiles, ## data = mtcars) ## ## tau: [1] 0.75 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.06384 1.68640 16.64124 0.00000 ## disp 0.00445 0.01457 0.30566 0.76212 ## hp -0.06662 0.01795 -3.71247 0.00090 ## factor(am)1 7.91402 2.36999 3.33926 0.00239 6.2.1.2 Test Conditional Quantile Coefficients if Different Use the rq.anova function frm the quantile regression packge to conduct WALD test. Remember WALD test says given unrestricted models estimates, test where null is that the coefficients satisfy some linear restrictions. To test, use the returned object from running rq with different numbers of quantiles, and set the option joint to true or false. When joint is true: equality of slopes should be done as joint tests on all slope parameters, when joint is false: separate tests on each of the slope parameters should be reported. A slope parameter refers to one of the RHS variables. Note that quantile tests are parallel line tests. Meaning that we should except to have different x-intercepts for each quantile, because they represents the levels of the conditional shocks distributions. However, if quantile coefficients for the slopes are all the same, then there are no quantile specific effects, mean effects would be sufficient. see: anova.rq() in quantreg package in R 6.2.1.2.1 Test Statistical Difference between 25th and 50th Conditional Quantiles Given the quantile estimates above, the difference between 0.25 and 0.50 quantiles exists, but are they sufficiently large to be statistically different? What is the p-value? Reviewing the results below, they are not statistically different. First, joint = TRUE. This is not testing if the coefficien on disp is the same as the coefficient on hp. This is testing jointly if the coefficients for different quantiles of disp, and different quantiles of hp are the same for each RHS variable. ls_fl_quantiles &lt;- c(0.25, 0.50) fit_quantiles &lt;- rq(mpg ~ disp + hp + factor(am), tau = ls_fl_quantiles, data = mtcars) anova(fit_quantiles, test = &quot;Wald&quot;, joint=TRUE) ## Quantile Regression Analysis of Deviance Table ## ## Model: mpg ~ disp + hp + factor(am) ## Joint Test of Equality of Slopes: tau in { 0.25 0.5 } ## ## Df Resid Df F value Pr(&gt;F) ## 1 3 61 0.7986 0.4994 Second, joint = False: anova(fit_quantiles, test = &quot;Wald&quot;, joint=FALSE) ## Quantile Regression Analysis of Deviance Table ## ## Model: mpg ~ disp + hp + factor(am) ## Tests of Equality of Distinct Slopes: tau in { 0.25 0.5 } ## ## Df Resid Df F value Pr(&gt;F) ## disp 1 63 0.0304 0.8621 ## hp 1 63 0.5397 0.4653 ## factor(am)1 1 63 1.0957 0.2992 6.2.1.2.2 Test Statistical Difference between 25th, 50th, and 75th Conditional Quantiles The 1st quartile and median do not seem to be statistically different, now include the 3rd quartile. As seen earlier, the quartiles jointly show a gradient. Now, we can see that idisp, hp and am are separately have statistically different First, joint = TRUE: ls_fl_quantiles &lt;- c(0.25, 0.50, 0.75) fit_quantiles &lt;- rq(mpg ~ disp + hp + factor(am), tau = ls_fl_quantiles, data = mtcars) anova(fit_quantiles, test = &quot;Wald&quot;, joint=TRUE) ## Quantile Regression Analysis of Deviance Table ## ## Model: mpg ~ disp + hp + factor(am) ## Joint Test of Equality of Slopes: tau in { 0.25 0.5 0.75 } ## ## Df Resid Df F value Pr(&gt;F) ## 1 6 90 3.957 0.001475 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Second, joint = False: anova(fit_quantiles, test = &quot;Wald&quot;, joint=FALSE) ## Quantile Regression Analysis of Deviance Table ## ## Model: mpg ~ disp + hp + factor(am) ## Tests of Equality of Distinct Slopes: tau in { 0.25 0.5 0.75 } ## ## Df Resid Df F value Pr(&gt;F) ## disp 2 94 9.2284 0.0002191 *** ## hp 2 94 6.5798 0.0021162 ** ## factor(am)1 2 94 3.6669 0.0292803 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["optimization.html", "Chapter 7 Optimization 7.1 Bisection", " Chapter 7 Optimization 7.1 Bisection 7.1.1 Bisection Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). See the ff_opti_bisect_pmap_multi function from Fans REconTools Package, which provides a resuable function based on the algorithm worked out here. The bisection specific code does not need to do much. list variables in file for grouping, each group is an individual for whom we want to calculate optimal choice for using bisection. string variable name of input where functions are evaluated, these are already contained in the dataframe, existing variable names, row specific, rowwise computation over these, each rowwise calculation using different rows. scalar and array values that are applied to every rowwise calculation, all rowwise calculations using the same scalars and arrays. string output variable name This is how I implement the bisection algorithm, when we know the bounding minimum and maximum to be below and above zero already. Evaluate \\(f^0_a = f(a^0)\\) and \\(f^0_b = f(b^0)\\), min and max points. Evaluate at \\(f^0_p = f(p^0)\\), where \\(p_0 = \\frac{a^0+b^0}{2}\\). if \\(f^i_a \\cdot f^i_p &lt; 0\\), then \\(b_{i+1} = p_i\\), else, \\(a_{i+1} = p_i\\) and \\(f^{i+1}_a = p_i\\). iteratre until convergence. Generate New columns of a and b as we iteratre, do not need to store p, p is temporary. Evaluate the function below which we have already tested, but now, in the dataframe before generating all permutations, tb_states_choices, now the fl_N element will be changing with each iteration, it will be row specific. fl_N are first min and max, then each subsequent ps. 7.1.1.1 Initialize Matrix Prepare Input Data: # Parameters fl_rho = 0.20 svr_id_var = &#39;INDI_ID&#39; # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = 4) ar_nN_alpha = seq(0.1, 0.9, length.out = 4) # Choice Grid for nutritional feasible choices for each fl_N_agg = 100 fl_N_min = 0 # Mesh Expand tb_states_choices &lt;- as_tibble(cbind(ar_nN_A, ar_nN_alpha)) %&gt;% rowid_to_column(var=svr_id_var) # Convert Matrix to Tibble ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_states_choices &lt;- tb_states_choices %&gt;% rename_all(~c(ar_st_col_names)) Prepare Function: # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } Initialize the matrix with \\(a_0\\) and \\(b_0\\), the initial min and max points: # common prefix to make reshaping easier st_bisec_prefix &lt;- &#39;bisec_&#39; svr_a_lst &lt;- paste0(st_bisec_prefix, &#39;a_0&#39;) svr_b_lst &lt;- paste0(st_bisec_prefix, &#39;b_0&#39;) svr_fa_lst &lt;- paste0(st_bisec_prefix, &#39;fa_0&#39;) svr_fb_lst &lt;- paste0(st_bisec_prefix, &#39;fb_0&#39;) # Add initial a and b tb_states_choices_bisec &lt;- tb_states_choices %&gt;% mutate(!!sym(svr_a_lst) := fl_N_min, !!sym(svr_b_lst) := fl_N_agg) # Evaluate function f(a_0) and f(b_0) tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% rowwise() %&gt;% mutate(!!sym(svr_fa_lst) := ffi_nonlin_dplyrdo(fl_A, fl_alpha, !!sym(svr_a_lst), ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho), !!sym(svr_fb_lst) := ffi_nonlin_dplyrdo(fl_A, fl_alpha, !!sym(svr_b_lst), ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Summarize dim(tb_states_choices_bisec) ## [1] 4 7 # summary(tb_states_choices_bisec) 7.1.1.2 Iterate and Solve for f(p), update f(a) and f(b) Implement the DPLYR based Concurrent bisection algorithm. # fl_tol = float tolerance criteria # it_tol = number of interations to allow at most fl_tol &lt;- 10^-2 it_tol &lt;- 100 # fl_p_dist2zr = distance to zero to initalize fl_p_dist2zr &lt;- 1000 it_cur &lt;- 0 while (it_cur &lt;= it_tol &amp;&amp; fl_p_dist2zr &gt;= fl_tol ) { it_cur &lt;- it_cur + 1 # New Variables svr_a_cur &lt;- paste0(st_bisec_prefix, &#39;a_&#39;, it_cur) svr_b_cur &lt;- paste0(st_bisec_prefix, &#39;b_&#39;, it_cur) svr_fa_cur &lt;- paste0(st_bisec_prefix, &#39;fa_&#39;, it_cur) svr_fb_cur &lt;- paste0(st_bisec_prefix, &#39;fb_&#39;, it_cur) # Evaluate function f(a_0) and f(b_0) # 1. generate p # 2. generate f_p # 3. generate f_p*f_a tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% rowwise() %&gt;% mutate(p = ((!!sym(svr_a_lst) + !!sym(svr_b_lst))/2)) %&gt;% mutate(f_p = ffi_nonlin_dplyrdo(fl_A, fl_alpha, p, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) %&gt;% mutate(f_p_t_f_a = f_p*!!sym(svr_fa_lst)) # fl_p_dist2zr = sum(abs(p)) fl_p_dist2zr &lt;- mean(abs(tb_states_choices_bisec %&gt;% pull(f_p))) # Update a and b tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% mutate(!!sym(svr_a_cur) := case_when(f_p_t_f_a &lt; 0 ~ !!sym(svr_a_lst), TRUE ~ p)) %&gt;% mutate(!!sym(svr_b_cur) := case_when(f_p_t_f_a &lt; 0 ~ p, TRUE ~ !!sym(svr_b_lst))) # Update f(a) and f(b) tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% mutate(!!sym(svr_fa_cur) := case_when(f_p_t_f_a &lt; 0 ~ !!sym(svr_fa_lst), TRUE ~ f_p)) %&gt;% mutate(!!sym(svr_fb_cur) := case_when(f_p_t_f_a &lt; 0 ~ f_p, TRUE ~ !!sym(svr_fb_lst))) # Save from last svr_a_lst &lt;- svr_a_cur svr_b_lst &lt;- svr_b_cur svr_fa_lst &lt;- svr_fa_cur svr_fb_lst &lt;- svr_fb_cur # Summar current round print(paste0(&#39;it_cur:&#39;, it_cur, &#39;, fl_p_dist2zr:&#39;, fl_p_dist2zr)) summary(tb_states_choices_bisec %&gt;% select(one_of(svr_a_cur, svr_b_cur, svr_fa_cur, svr_fb_cur))) } ## [1] &quot;it_cur:1, fl_p_dist2zr:1597.93916362849&quot; ## [1] &quot;it_cur:2, fl_p_dist2zr:676.06602535902&quot; ## [1] &quot;it_cur:3, fl_p_dist2zr:286.850590132782&quot; ## [1] &quot;it_cur:4, fl_p_dist2zr:117.225493866655&quot; ## [1] &quot;it_cur:5, fl_p_dist2zr:37.570593471664&quot; ## [1] &quot;it_cur:6, fl_p_dist2zr:4.60826664896022&quot; ## [1] &quot;it_cur:7, fl_p_dist2zr:14.4217689135683&quot; ## [1] &quot;it_cur:8, fl_p_dist2zr:8.38950830086659&quot; ## [1] &quot;it_cur:9, fl_p_dist2zr:3.93347761455868&quot; ## [1] &quot;it_cur:10, fl_p_dist2zr:1.88261338941038&quot; ## [1] &quot;it_cur:11, fl_p_dist2zr:0.744478952222305&quot; ## [1] &quot;it_cur:12, fl_p_dist2zr:0.187061801237917&quot; ## [1] &quot;it_cur:13, fl_p_dist2zr:0.117844913432613&quot; ## [1] &quot;it_cur:14, fl_p_dist2zr:0.0275365951418891&quot; ## [1] &quot;it_cur:15, fl_p_dist2zr:0.0515488156908255&quot; ## [1] &quot;it_cur:16, fl_p_dist2zr:0.0191152349149135&quot; ## [1] &quot;it_cur:17, fl_p_dist2zr:0.00385372194545752&quot; 7.1.1.3 Reshape Wide to long to Wide To view results easily, how iterations improved to help us find the roots, convert table from wide to long. Pivot twice. This allows us to easily graph out how bisection is working out iterationby iteration. Here, we will first show what the raw table looks like, the wide only table, and then show the long version, and finally the version that is medium wide. 7.1.1.3.1 Table OneVery Wide Show what the tb_states_choices_bisec looks like. Variables are formatted like: bisec_xx_yy, where yy is the iteration indicator, and xx is either a, b, fa, or fb. kable(head(t(tb_states_choices_bisec), 25)) %&gt;% kable_styling_fc() INDI_ID 1.000000e+00 2.0000000 3.0000000 4.0000000 fl_A -2.000000e+00 -0.6666667 0.6666667 2.0000000 fl_alpha 1.000000e-01 0.3666667 0.6333333 0.9000000 bisec_a_0 0.000000e+00 0.0000000 0.0000000 0.0000000 bisec_b_0 1.000000e+02 100.0000000 100.0000000 100.0000000 bisec_fa_0 1.000000e+02 100.0000000 100.0000000 100.0000000 bisec_fb_0 -1.288028e+04 -1394.7069782 -323.9421599 -51.9716069 p 1.544952e+00 8.5838318 24.8359680 65.0367737 f_p -7.637200e-03 -0.0052211 -0.0016162 -0.0009405 f_p_t_f_a -3.800000e-04 -0.0000237 -0.0000025 -0.0000002 bisec_a_1 0.000000e+00 0.0000000 0.0000000 50.0000000 bisec_b_1 5.000000e+01 50.0000000 50.0000000 100.0000000 bisec_fa_1 1.000000e+02 100.0000000 100.0000000 22.5557704 bisec_fb_1 -5.666956e+03 -595.7345364 -106.5105843 -51.9716069 bisec_a_2 0.000000e+00 0.0000000 0.0000000 50.0000000 bisec_b_2 2.500000e+01 25.0000000 25.0000000 75.0000000 bisec_fa_2 1.000000e+02 100.0000000 100.0000000 22.5557704 bisec_fb_2 -2.464562e+03 -224.1460032 -0.6857375 -14.8701831 bisec_a_3 0.000000e+00 0.0000000 12.5000000 62.5000000 bisec_b_3 1.250000e+01 12.5000000 25.0000000 75.0000000 bisec_fa_3 1.000000e+02 100.0000000 50.8640414 3.7940196 bisec_fb_3 -1.041574e+03 -51.1700464 -0.6857375 -14.8701831 bisec_a_4 0.000000e+00 6.2500000 18.7500000 62.5000000 bisec_b_4 6.250000e+00 12.5000000 25.0000000 68.7500000 bisec_fa_4 1.000000e+02 29.4271641 25.2510409 3.7940196 # str(tb_states_choices_bisec) 7.1.1.3.2 Table TwoVery Wide to Very Long We want to treat the iteration count information that is the suffix of variable names as a variable by itself. Additionally, we want to treat the a,b,fa,fb as a variable. Structuring the data very long like this allows for easy graphing and other types of analysis. Rather than dealing with many many variables, we have only 3 core variables that store bisection iteration information. Here we use the very nice pivot_longer function. Note that to achieve this, we put a common prefix in front of the variables we wanted to convert to long. THis is helpful, because we can easily identify which variables need to be reshaped. # New variables svr_bisect_iter &lt;- &#39;biseciter&#39; svr_abfafb_long_name &lt;- &#39;varname&#39; svr_number_col &lt;- &#39;value&#39; svr_id_bisect_iter &lt;- paste0(svr_id_var, &#39;_bisect_ier&#39;) # Pivot wide to very long tb_states_choices_bisec_long &lt;- tb_states_choices_bisec %&gt;% pivot_longer( cols = starts_with(st_bisec_prefix), names_to = c(svr_abfafb_long_name, svr_bisect_iter), names_pattern = paste0(st_bisec_prefix, &quot;(.*)_(.*)&quot;), values_to = svr_number_col ) # Print # summary(tb_states_choices_bisec_long) kable(head(tb_states_choices_bisec_long %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 15)) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha varname biseciter value 1 -2 0.1 a 0 0.000 1 -2 0.1 b 0 100.000 1 -2 0.1 fa 0 100.000 1 -2 0.1 fb 0 -12880.284 1 -2 0.1 a 1 0.000 1 -2 0.1 b 1 50.000 1 -2 0.1 fa 1 100.000 1 -2 0.1 fb 1 -5666.956 1 -2 0.1 a 2 0.000 1 -2 0.1 b 2 25.000 1 -2 0.1 fa 2 100.000 1 -2 0.1 fb 2 -2464.562 1 -2 0.1 a 3 0.000 1 -2 0.1 b 3 12.500 1 -2 0.1 fa 3 100.000 kable(tail(tb_states_choices_bisec_long %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 15)) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha varname biseciter value 4 2 0.9 b 14 65.0390625 4 2 0.9 fa 14 0.0047633 4 2 0.9 fb 14 -0.0043628 4 2 0.9 a 15 65.0360107 4 2 0.9 b 15 65.0390625 4 2 0.9 fa 15 0.0002003 4 2 0.9 fb 15 -0.0043628 4 2 0.9 a 16 65.0360107 4 2 0.9 b 16 65.0375366 4 2 0.9 fa 16 0.0002003 4 2 0.9 fb 16 -0.0020812 4 2 0.9 a 17 65.0360107 4 2 0.9 b 17 65.0367737 4 2 0.9 fa 17 0.0002003 4 2 0.9 fb 17 -0.0009405 7.1.1.3.3 Table TwoVery Very Long to Wider Again But the previous results are too long, with the a, b, fa, and fb all in one column as different categories, they are really not different categories, they are in fact different types of variables. So we want to spread those four categories of this variable into four columns, each one representing the a, b, fa, and fb values. The rows would then be uniquly identified by the iteration counter and individual ID. # Pivot wide to very long to a little wide tb_states_choices_bisec_wider &lt;- tb_states_choices_bisec_long %&gt;% pivot_wider( names_from = !!sym(svr_abfafb_long_name), values_from = svr_number_col ) # Print # summary(tb_states_choices_bisec_wider) kable(head(tb_states_choices_bisec_wider %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 10)) %&gt;% kable_styling_fc_wide() INDI_ID fl_A fl_alpha biseciter a b fa fb 1 -2 0.1 0 0.000000 100.0000 100.00000 -12880.283918 1 -2 0.1 1 0.000000 50.0000 100.00000 -5666.955763 1 -2 0.1 2 0.000000 25.0000 100.00000 -2464.562178 1 -2 0.1 3 0.000000 12.5000 100.00000 -1041.574253 1 -2 0.1 4 0.000000 6.2500 100.00000 -408.674764 1 -2 0.1 5 0.000000 3.1250 100.00000 -126.904283 1 -2 0.1 6 0.000000 1.5625 100.00000 -1.328965 1 -2 0.1 7 0.781250 1.5625 54.69612 -1.328965 1 -2 0.1 8 1.171875 1.5625 27.46061 -1.328965 1 -2 0.1 9 1.367188 1.5625 13.23495 -1.328965 kable(head(tb_states_choices_bisec_wider %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 10)) %&gt;% kable_styling_fc_wide() INDI_ID fl_A fl_alpha biseciter a b fa fb 1 -2 0.1 0 0.000000 100.0000 100.00000 -12880.283918 1 -2 0.1 1 0.000000 50.0000 100.00000 -5666.955763 1 -2 0.1 2 0.000000 25.0000 100.00000 -2464.562178 1 -2 0.1 3 0.000000 12.5000 100.00000 -1041.574253 1 -2 0.1 4 0.000000 6.2500 100.00000 -408.674764 1 -2 0.1 5 0.000000 3.1250 100.00000 -126.904283 1 -2 0.1 6 0.000000 1.5625 100.00000 -1.328965 1 -2 0.1 7 0.781250 1.5625 54.69612 -1.328965 1 -2 0.1 8 1.171875 1.5625 27.46061 -1.328965 1 -2 0.1 9 1.367188 1.5625 13.23495 -1.328965 7.1.1.4 Graph Bisection Iteration Results Actually we want to graph based on the long results, not the wider. Wider easier to view in table. # Graph results lineplot &lt;- tb_states_choices_bisec_long %&gt;% mutate(!!sym(svr_bisect_iter) := as.numeric(!!sym(svr_bisect_iter))) %&gt;% filter(!!sym(svr_abfafb_long_name) %in% c(&#39;a&#39;, &#39;b&#39;)) %&gt;% ggplot(aes(x=!!sym(svr_bisect_iter), y=!!sym(svr_number_col), colour=!!sym(svr_abfafb_long_name), linetype=!!sym(svr_abfafb_long_name), shape=!!sym(svr_abfafb_long_name))) + facet_wrap( ~ INDI_ID) + geom_line() + geom_point() + labs(title = &#39;Bisection Iteration over individuals Until Convergence&#39;, x = &#39;Bisection Iteration&#39;, y = &#39;a (left side point) and b (right side point) values&#39;, caption = &#39;DPLYR concurrent bisection nonlinear multple individuals&#39;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) print(lineplot) "],["mathmatics-and-statistics.html", "Chapter 8 Mathmatics and Statistics 8.1 Distributions 8.2 Analytical Solutions 8.3 Inequality Models", " Chapter 8 Mathmatics and Statistics 8.1 Distributions 8.1.1 Integrate Over Normal Guassian Process Shock Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Some Common parameters fl_eps_mean = 10 fl_eps_sd = 50 fl_cdf_min = 0.000001 fl_cdf_max = 0.999999 ar_it_draws &lt;- seq(1, 1000) 8.1.1.1 Randomly Sample and Integrate (Monte Carlo Integration) Compare randomly drawn normal shock mean and known mean. How does simulated mean change with draws. Actual integral equals to \\(10\\), as sample size increases, the sample mean approaches the integration results, but this is expensive, even with ten thousand draws, not very exact. # Simulate Draws set.seed(123) ar_fl_means &lt;- sapply(ar_it_draws, function(x) return(mean(rnorm(x[1], mean=fl_eps_mean, sd=fl_eps_sd)))) ar_fl_sd &lt;- sapply(ar_it_draws, function(x) return(sd(rnorm(x[1], mean=fl_eps_mean, sd=fl_eps_sd)))) mt_sample_means &lt;- cbind(ar_it_draws, ar_fl_means, ar_fl_sd) colnames(mt_sample_means) &lt;- c(&#39;draw_count&#39;, &#39;mean&#39;, &#39;sd&#39;) tb_sample_means &lt;- as_tibble(mt_sample_means) # Graph # x-labels x.labels &lt;- c(&#39;n=1&#39;, &#39;n=10&#39;, &#39;n=100&#39;, &#39;n=1000&#39;) x.breaks &lt;- c(1, 10, 100, 1000) # Shared Subtitle st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/math/integration/htmlpdfr/fs_integrate_normal.html&#39;) # Shared Labels slb_title_shr = paste0(&#39;as Sample Size Increases\\n&#39;, &#39;True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd) slb_xtitle = paste0(&#39;Sample Size&#39;) # Graph Results--Draw plt_mean &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=mean)) + geom_line(size=0.75) + labs(title = paste0(&#39;Sample Mean &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Sample Mean&#39;, caption = &#39;Mean of Sample Integrates to True Mean&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_mean) plt_sd &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=sd)) + geom_line(size=0.75) + labs(title = paste0(&#39;Sample Standard Deviation &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Sample Standard Deviation&#39;, caption = &#39;Standard Deviation of Sample Integrates to True SD&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_sd) 8.1.1.2 Integration By Symmetric Uneven Rectangle Draw on even grid from close to 0 to close to 1. Get the corresponding x points to these quantile levels. Distance between x points are not equi-distance but increasing and symmetric away from the mean. Under this approach, each rectangle aims to approximate the same area. Resulting integration is rectangle based, but rectangle width differ. The rectangles have wider width as they move away from the mean, and thinner width close to the mean. This is much more stable than the random draw method, but note that it converges somewhat slowly to true values as well. mt_fl_means &lt;- sapply(ar_it_draws, function(x) { fl_prob_break = (fl_cdf_max - fl_cdf_min)/(x[1]) ar_eps_bounds &lt;- qnorm(seq(fl_cdf_min, fl_cdf_max, by=(fl_cdf_max - fl_cdf_min)/(x[1])), mean = fl_eps_mean, sd = fl_eps_sd) ar_eps_val &lt;- (tail(ar_eps_bounds, -1) + head(ar_eps_bounds, -1))/2 ar_eps_prb &lt;- rep(fl_prob_break/(fl_cdf_max - fl_cdf_min), x[1]) ar_eps_fev &lt;- dnorm(ar_eps_val, mean = fl_eps_mean, sd = fl_eps_sd) fl_cdf_total_approx &lt;- sum(ar_eps_fev*diff(ar_eps_bounds)) fl_mean_approx &lt;- sum(ar_eps_val*(ar_eps_fev*diff(ar_eps_bounds))) fl_sd_approx &lt;- sqrt(sum((ar_eps_val-fl_mean_approx)^2*(ar_eps_fev*diff(ar_eps_bounds)))) return(list(cdf=fl_cdf_total_approx, mean=fl_mean_approx, sd=fl_sd_approx)) }) mt_sample_means &lt;- cbind(ar_it_draws, as_tibble(t(mt_fl_means)) %&gt;% unnest()) colnames(mt_sample_means) &lt;- c(&#39;draw_count&#39;, &#39;cdf&#39;, &#39;mean&#39;, &#39;sd&#39;) tb_sample_means &lt;- as_tibble(mt_sample_means) # Graph # x-labels x.labels &lt;- c(&#39;n=1&#39;, &#39;n=10&#39;, &#39;n=100&#39;, &#39;n=1000&#39;) x.breaks &lt;- c(1, 10, 100, 1000) # Shared Labels slb_title_shr = paste0(&#39;as Uneven Rectangle Count Increases\\n&#39;, &#39;True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd) slb_xtitle = paste0(&#39;Number of Quantile Bins for Uneven Rectangles Approximation&#39;) # Graph Results--Draw plt_mean &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=mean)) + geom_line(size=0.75) + labs(title = paste0(&#39;Average &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Approximated Mean&#39;, caption = &#39;Integral Approximation as Uneven Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_mean) plt_sd &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=sd)) + geom_line(size=0.75) + labs(title = paste0(&#39;Standard Deviation &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Approximated Standard Deviation&#39;, caption = &#39;Integral Approximation as Uneven Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_sd) plt_cdf &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=cdf)) + geom_line(size=0.75) + labs(title = paste0(&#39;Aggregate Probability &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Sum of Uneven Rectangles&#39;, caption = &#39;Sum of Approx. Probability as Uneven Rectangle Count Increases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_cdf) 8.1.1.3 Integration By Constant Width Rectangle (Trapezoidal rule) This is implementing even width recentagle, even along x-axix. Rectangle width are the same, height is \\(f(x)\\). This is even width, but uneven area. Note that this method approximates the true answer much better and more quickly than the prior methods. mt_fl_means &lt;- sapply(ar_it_draws, function(x) { fl_eps_min &lt;- qnorm(fl_cdf_min, mean = fl_eps_mean, sd = fl_eps_sd) fl_eps_max &lt;- qnorm(fl_cdf_max, mean = fl_eps_mean, sd = fl_eps_sd) fl_gap &lt;- (fl_eps_max-fl_eps_min)/(x[1]) ar_eps_bounds &lt;- seq(fl_eps_min, fl_eps_max, by=fl_gap) ar_eps_val &lt;- (tail(ar_eps_bounds, -1) + head(ar_eps_bounds, -1))/2 ar_eps_prb &lt;- dnorm(ar_eps_val, mean = fl_eps_mean, sd = fl_eps_sd)*fl_gap fl_cdf_total_approx &lt;- sum(ar_eps_prb) fl_mean_approx &lt;- sum(ar_eps_val*ar_eps_prb) fl_sd_approx &lt;- sqrt(sum((ar_eps_val-fl_mean_approx)^2*ar_eps_prb)) return(list(cdf=fl_cdf_total_approx, mean=fl_mean_approx, sd=fl_sd_approx)) }) mt_sample_means &lt;- cbind(ar_it_draws, as_tibble(t(mt_fl_means)) %&gt;% unnest()) colnames(mt_sample_means) &lt;- c(&#39;draw_count&#39;, &#39;cdf&#39;, &#39;mean&#39;, &#39;sd&#39;) tb_sample_means &lt;- as_tibble(mt_sample_means) # Graph # x-labels x.labels &lt;- c(&#39;n=1&#39;, &#39;n=10&#39;, &#39;n=100&#39;, &#39;n=1000&#39;) x.breaks &lt;- c(1, 10, 100, 1000) # Shared Labels slb_title_shr = paste0(&#39;as Even Rectangle Count Increases\\n&#39;, &#39;True Mean=&#39;, fl_eps_mean,&#39;, sd=&#39;,fl_eps_sd) slb_xtitle = paste0(&#39;Number Equi-distance Rectangles Bins&#39;) # Graph Results--Draw plt_mean &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=mean)) + geom_line(size=0.75) + labs(title = paste0(&#39;Average &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Integrated Mean&#39;, caption = &#39;Integral Approximation as Even Rectangle width decreases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_mean) plt_sd &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=sd)) + geom_line(size=0.75) + labs(title = paste0(&#39;Standard Deviation &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Standard Deviation&#39;, caption = &#39;Integral Approximation as Even Rectangle width decreases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_sd) plt_cdf &lt;- tb_sample_means %&gt;% ggplot(aes(x=draw_count, y=cdf)) + geom_line(size=0.75) + labs(title = paste0(&#39;Aggregate Probability &#39;, slb_title_shr), subtitle = st_subtitle, x = slb_xtitle, y = &#39;Sum of Equi-Dist Rectangles&#39;, caption = &#39;Sum of Approx. Probability as Equi-Dist Rectangle width decreases&#39;) + scale_x_continuous(trans=&#39;log10&#39;, labels = x.labels, breaks = x.breaks) + theme_bw() print(plt_cdf) 8.2 Analytical Solutions 8.2.1 Linear Scalar f(x)=0 Solutions Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 8.2.1.1 Ratio Here are some common ratios. 8.2.1.1.1 Unif Draw Min and Max Ratio We want to draw numbers such that we have some mean \\(b\\), and that the possible maximum and minimum value drawn are at most \\(a\\) times apart. Given \\(b\\) and \\(a\\), solve for \\(x\\). \\[ f(x) = \\frac{b+x}{b-x} - a = 0 \\] \\[ b \\cdot a - x \\cdot a = b + x \\\\ b \\cdot a - b = x + x \\cdot a \\\\ b \\left(a - 1\\right) = x \\left( a+ 1\\right) \\\\ x = \\frac{b\\left(a-1\\right)}{a+1}\\\\ \\] Uniformly draw b &lt;- 100 a &lt;- 2 x &lt;- (b*(a-1))/(a+1) ar_unif_draws &lt;- runif(100, min=b-x, max=b+x) fl_max_min_ratio &lt;- max(ar_unif_draws)/min(ar_unif_draws) cat(&#39;fl_max_min_ratio =&#39;, fl_max_min_ratio, &#39;is close to a =&#39;, a, &#39;\\n&#39;) ## fl_max_min_ratio = 1.965882 is close to a = 2 8.3 Inequality Models 8.3.1 Gini Discrete Sample Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). This works out how the ff_dist_gini_vector_pos function works from Fans REconTools Package. 8.3.1.1 Gini Formula for Discrete Sample There is an vector values (all positive). This could be height information for N individuals. It could also be income information for N individuals. Calculate the GINI coefficient treating the given vector as population. This is not an estimation exercise where we want to estimate population gini based on a sample. The given array is the population. The population is discrete, and only has these N individuals in the length n vector. Note that when the sample size is small, there is a limit to inequality using the formula defined below given each \\(N\\). So for small \\(N\\), can not really compare inequality across arrays with different \\(N\\), can only compare arrays with the same \\(N\\). The GINI formula used here is: \\[ GINI = 1 - \\frac{2}{N+1} \\cdot \\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right) \\cdot \\left( \\sum_{i=1}^N x_i \\right)^{-1} \\] Derive the formula in the steps below. Step 1 Area Formula \\[ \\Gamma = \\sum_{i=1}^N \\frac{1}{N} \\cdot \\left( \\sum_{j=1}^{i} \\left( \\frac{x_j}{\\sum_{\\widehat{j}=1}^N x_{\\widehat{j}} } \\right) \\right) \\] Step 2 Total Area Given Perfect equality With perfect equality \\(x_i=a\\) for all \\(i\\), so need to divide by that. \\[ \\Gamma^{\\text{equal}} = \\sum_{i=1}^N \\frac{1}{N} \\cdot \\left( \\sum_{j=1}^{i} \\left( \\frac{a}{\\sum_{\\widehat{j}=1}^N a } \\right) \\right) = \\frac{N+1}{N}\\cdot\\frac{1}{2} \\] As the number of elements of the vecotr increases: \\[ \\lim_{N \\rightarrow \\infty}\\Gamma^{\\text{equal}} = \\lim_{N \\rightarrow \\infty} \\frac{N+1}{N}\\cdot\\frac{1}{2} = \\frac{1}{2} \\] Step 3 Arriving at Finite Vector Gini Formula Given what we have from above, we obtain the gini formula, divide by total area below 45 degree line. \\[ GINI = 1 - \\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right) \\cdot \\left( N \\cdot \\sum_{i=1}^N x_i \\right)^{-1} \\cdot \\left( \\frac{N+1}{N}\\cdot\\frac{1}{2} \\right)^{-1} = 1 - \\frac{2}{N+1} \\cdot \\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right) \\cdot \\left( \\sum_{i=1}^N x_i \\right)^{-1} \\] Step 4 Maximum Inequality given N Suppose \\(x_i=0\\) for all \\(i&lt;N\\), then: \\[ GINI^{x_i = 0 \\text{ except } i=N} = 1 - \\frac{2}{N+1} \\cdot X_N \\cdot \\left( X_N \\right)^{-1} = 1 - \\frac{2}{N+1} \\] \\[ \\lim_{N \\rightarrow \\infty} GINI^{x_i = 0 \\text{ except } i=N} = 1 - \\lim_{N \\rightarrow \\infty} \\frac{2}{N+1} = 1 \\] Note that for small N, for example if \\(N=10\\), even when one person holds all income, all others have 0 income, the formula will not produce gini is zero, but that gini is equal to \\(\\frac{2}{11}\\approx 0.1818\\). If \\(N=2\\), inequality is at most, \\(\\frac{2}{3}\\approx 0.667\\). \\[ MostUnequalGINI\\left(N\\right) = 1 - \\frac{2}{N+1} = \\frac{N-1}{N+1} \\] 8.3.1.2 Implement GINI Formula The GINI formula just derived is trivial to compute. scalar: \\(\\frac{2}{N+1}\\) cumsum: \\(\\sum_{j=1}^{i} x_j\\) sum of cumsum: \\(\\left(\\sum_{i=1}^N \\sum_{j=1}^{i} x_j\\right)\\) sum: \\(\\sum_{i=1}^N X_i\\) There are no package dependencies. Define the formula here: # Formula, directly implement the GINI formula Following Step 4 above fv_dist_gini_vector_pos_test &lt;- function(ar_pos) { # Check length and given warning it_n &lt;- length(ar_pos) if (it_n &lt;= 100) warning(&#39;Data vector has n=&#39;,it_n,&#39;, max-inequality/max-gini=&#39;,(it_n-1)/(it_n + 1)) # Sort ar_pos &lt;- sort(ar_pos) # formula implement fl_gini &lt;- 1 - ((2/(it_n+1)) * sum(cumsum(ar_pos))*(sum(ar_pos))^(-1)) return(fl_gini) } Generate a number of examples Arrays for testing # Example Arrays of data ar_equal_n1 = c(1) ar_ineql_n1 = c(100) ar_equal_n2 = c(1,1) ar_ineql_alittle_n2 = c(1,2) ar_ineql_somewht_n2 = c(1,2^3) ar_ineql_alotine_n2 = c(1,2^5) ar_ineql_veryvry_n2 = c(1,2^8) ar_ineql_mostmst_n2 = c(1,2^13) ar_equal_n10 = c(2,2,2,2,2,2, 2, 2, 2, 2) ar_ineql_some_n10 = c(1,2,3,5,8,13,21,34,55,89) ar_ineql_very_n10 = c(1,2^2,3^2,5^2,8^2,13^2,21^2,34^2,55^2,89^2) ar_ineql_extr_n10 = c(1,2^2,3^3,5^4,8^5,13^6,21^7,34^8,55^9,89^10) Now test the example arrays above using the function based no our formula: ## ## Small N=1 Hard-Code ## ar_equal_n1: 0 ## ar_ineql_n1: 0 ## ## Small N=2 Hard-Code, converge to 1/3, see formula above ## ar_ineql_alittle_n2: 0.1111111 ## ar_ineql_somewht_n2: 0.2592593 ## ar_ineql_alotine_n2: 0.3131313 ## ar_ineql_veryvry_n2: 0.3307393 ## ## Small N=10 Hard-Code, convege to 9/11=0.8181, see formula above ## ar_equal_n10: 0 ## ar_ineql_some_n10: 0.5395514 ## ar_ineql_very_n10: 0.7059554 ## ar_ineql_extr_n10: 0.8181549 8.3.2 Atkinson Inequality Index Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 8.3.2.1 Atkinson Inequality Measures Atkinson (JET, 1970) studies five standard inequality measures. Atkinson finds that given the same income data across countries, different inequality measure lead to different rankings of which country is more unequal. Atkinson develops an measure of inequality that changes depending on an inequality aversion parameter. \\[ \\text{Atkinson Inequality} = A\\left( \\left\\{Y_i\\right\\}_{i=1}^N, \\lambda \\right) = 1 - \\left( \\sum_{i=1}^N \\frac{1}{N} \\left( \\frac{Y_i}{\\sum_{j=1}^N \\left( \\frac{Y_j}{N} \\right) } \\right)^{\\lambda} \\right)^{\\frac{1}{\\lambda}} \\in \\left[0,1\\right] \\] \\(A\\left(\\left\\{Y_i\\right\\}_{i=1}^N,\\lambda\\right)\\) equals to zero is perfect equality. 1 is Perfect inequality. If \\(\\lambda=1\\), the inequality measure is always equal to 0 because the planner does not care abouot inequality anymore. 8.3.2.2 Atkinson Inequality Function Programming up the equation above, we have: # Formula ffi_atkinson_ineq &lt;- function(ar_data, fl_rho) { ar_data_demean &lt;- ar_data/mean(ar_data) it_len &lt;- length(ar_data_demean) fl_atkinson &lt;- 1 - sum(ar_data_demean^{fl_rho}*(1/it_len))^(1/fl_rho) return(fl_atkinson) } 8.3.2.3 Atkinson Inequality Examples Given a vectr of observables, compute the atkinson inequality measure given different inequality aversion. Preference vector and data vector: # Preference Vector ar_rho &lt;- c(1, 1 - (10^(c(seq(-2.2,2.2, length.out=60))))) ar_rho &lt;- unique(ar_rho) mt_rho &lt;- matrix(ar_rho, nrow=length(ar_rho), ncol=1) # Random Data Vector (not equal outcomes) set.seed(123) ar_data_rand &lt;- rnorm(15, mean=0,sd=1) ar_data_rand &lt;- ar_data_rand - min(ar_data_rand) + 1 # Uniform Data Vector (Equal) ar_data_unif &lt;- rep(1, length(ar_data_rand)) # One Rich (last person has income equal to the sum of all others*100) ar_data_onerich &lt;- rep(0.1, length(ar_data_rand)) ar_data_onerich[length(ar_data_onerich)] = sum(head(ar_data_onerich,-1))*10 Testing Atkinson with different data arrays: # ATK = 0.1180513 ffi_atkinson_ineq(ar_data_rand, -1) ## [1] 0.1180513 # ATK = 0 ffi_atkinson_ineq(ar_data_unif, -1) ## [1] 0 # ATK = 0.89 ffi_atkinson_ineq(ar_data_onerich, -1) ## [1] 0.8956933 8.3.2.3.1 Atkinson Inequality as Inequality Aversion Changes This is the vector of inequality aversion parameters: ar_rho ## [1] 1.00000000 0.99369043 0.99250837 0.99110487 0.98943842 0.98745978 0.98511046 0.98232100 ## [9] 0.97900896 0.97507643 0.97040717 0.96486316 0.95828051 0.95046465 0.94118454 0.93016586 ## [17] 0.91708291 0.90154895 0.88310482 0.86120530 0.83520306 0.80432947 0.76767192 0.72414684 ## [25] 0.67246762 0.61110666 0.53825013 0.45174443 0.34903248 0.22707813 0.08227648 -0.08965279 ## [33] -0.29379184 -0.53617495 -0.82396688 -1.16567469 -1.57139912 -2.05313328 -2.62511705 -3.30425810 ## [41] -4.11063160 -5.06807371 -6.20488608 -7.55467254 -9.15733231 -11.06023949 -13.31964342 -16.00233131 ## [49] -19.18760255 -22.96961271 -27.46015678 -32.79197376 -39.12267043 -46.63938010 -55.56429426 -66.16123043 ## [57] -78.74343059 -93.68282046 -111.42100351 -132.48231461 -157.48931925 How does Atkinson Inequality measure change with respect to a vector of random data as inequality aversion shifts: par(new=T) st_x_label &lt;- &#39;Lambda, left Rawlsian, right (1) is Utilitarian&#39; st_y_label &lt;- &#39;Atkinson Inequality, 0 = perfect equal&#39; ar_ylim = c(0,1) ffi_atkinson_ineq(ar_data_rand, -1) ## [1] 0.1180513 ar_atkinson &lt;- apply(mt_rho, 1, function(row){ffi_atkinson_ineq(ar_data_rand, row[1])}) plot(ar_rho, ar_atkinson, ylim = ar_ylim) title(main = &#39;A vector of Random data&#39;, xlab = st_x_label, ylab = st_y_label) grid() Now with the one person has the wealth of all others in the vector times 10: par(new=T) ffi_atkinson_ineq(ar_data_onerich, -1) ## [1] 0.8956933 ar_atkinson &lt;- apply(mt_rho, 1, function(row){ffi_atkinson_ineq(ar_data_onerich, row[1])}) plot(ar_rho, ar_atkinson, ylim = ar_ylim) title(main = &#39;1 person has the (income of all others summed up)*10&#39;, xlab = st_x_label, ylab = st_y_label) grid() The Uniform Results, since allocations are uniform, zero for all: par(new=T) ffi_atkinson_ineq(ar_data_unif, -1) ## [1] 0 ar_atkinson &lt;- apply(mt_rho, 1, function(row){ffi_atkinson_ineq(ar_data_unif, row[1])}) plot(ar_rho, ar_atkinson, ylim = ar_ylim) title(main = &#39;uniform distribution&#39;, xlab = st_x_label, ylab = st_y_label) grid() 8.3.2.4 Analyzing Equation Mechanics How does the Aktinson Family utility function work? THe Atkinson Family Utility has the following functional form. \\[ V^{\\text{social}} = \\left( \\alpha \\cdot A^{\\lambda} + \\beta \\cdot B^{\\lambda} \\right)^{\\frac{1}{\\lambda}} \\] Several key issues here: \\(V^{\\text{social}}\\) is the utility of some social planner \\(A\\) and \\(B\\) are allocations for Alex and Ben. \\(\\alpha\\) and \\(\\beta\\) are biases that a social planner has for Alex and Ben: \\(\\alpha+\\beta=1\\), \\(\\alpha&gt;0\\), and \\(\\beta&gt;0\\) \\(-\\infty &lt; \\lambda \\le 1\\) is a measure of inequality aversion \\(\\lambda=1\\) is when the planner cares about weighted total allocations (efficient, Utilitarian) \\(\\lambda=-\\infty\\) is when the planner cares about only the minimum between \\(A\\) and \\(B\\) allocations (equality, Rawlsian) What if only care about Alex? Clearly, if the planner only cares about Ben, \\(\\beta=1\\), then: \\[ V^{\\text{social}} = \\left( B^{\\lambda} \\right)^{\\frac{1}{\\lambda}} = B \\] Clearly, regardless of the value of \\(\\lambda\\), as \\(B\\) increases \\(V\\) increases. What Happens to V when A or B increases? What is the derivative of \\(V\\) with respect to \\(A\\) or \\(B\\)? \\[ \\frac{\\partial V}{\\partial A} = \\frac{1}{\\lambda} \\left( \\alpha A^{\\lambda} + \\beta B^{\\lambda} \\right)^{\\frac{1}{\\lambda}-1} \\cdot \\lambda \\alpha A^{\\lambda -1} \\] \\[ \\frac{\\partial V}{\\partial A} = \\left( \\alpha A^{\\lambda} + \\beta B^{\\lambda} \\right)^{\\frac{1-\\lambda}{\\lambda}} \\cdot \\alpha A^{\\lambda -1} &gt;0 \\] Note that \\(\\frac{\\partial V}{\\partial A}&gt;0\\). When \\(\\lambda &lt;0\\), \\(Z^{\\lambda}&gt;0\\). For example \\(10^{-2}=\\frac{1}{100}\\). And For example \\(0.1^{\\frac{3}{-2}}=\\frac{1}{0.1^{1.5}}\\). Still Positive. While the overall \\(V\\) increases with increasing \\(A\\), but if we did not have the outter power term, the situation is different. In particular, when \\(\\lambda &lt; 0\\): \\[ \\text{ if } \\lambda &lt;0 \\thinspace\\thinspace \\text{ then } \\thinspace\\thinspace \\frac{d \\left(\\alpha A^{\\lambda} + \\beta B^{\\lambda}\\right)}{dA}=\\alpha\\lambda A^{\\lambda -1}&lt;0 \\] Without the outter \\(\\frac{1}{\\lambda}\\) power, negative \\(\\lambda\\) would lead to decreasing weighted sum. But: \\[ \\text{ if } \\lambda &lt;0 \\thinspace\\thinspace \\text{ then } \\thinspace\\thinspace \\frac{dG^{\\frac{1}{\\lambda}}}{dG}=\\frac{1}{\\lambda}\\cdot G^{\\frac{1-\\lambda}{\\lambda}}&lt;0 \\] so when \\(G\\) is increasing and \\(\\lambda &lt;0\\), \\(V\\) would decrease. But when \\(G\\left(A,B\\right)\\) is decreasing, as is the case with increasing \\(A\\) when \\(\\lambda &lt;0\\), \\(V\\) will actually increase. This confirms that \\(\\frac{\\partial V}{\\partial A}&gt;0\\) for \\(\\lambda &lt;0\\). The result is symmetric for \\(\\lambda &gt;0\\). 8.3.2.5 Indifference Curve Graph Given \\(V^{\\ast}\\), we can show the combinations of \\(A\\) and \\(B\\) points that provide the same utility. We want to be able to potentially draw multiple indifference curves at the same time. Note that indifference curves are defined by \\(\\alpha\\), \\(\\lambda\\) only. Each indifference curve is a set of \\(A\\) and \\(B\\) coordinates. So to generate multiple indifference curves means to generate many sets of \\(A\\), \\(B\\) associated with different planner preferences, and then these could be graphed out. # A as x-axis, need bounds on A fl_A_min = 0.01 fl_A_max = 3 it_A_grid = 10000 # Define parameters # ar_lambda &lt;- 1 - (10^(c(seq(-2,2, length.out=3)))) ar_lambda &lt;- c(1, 0.6, 0.06, -6) ar_beta &lt;- seq(0.25, 0.75, length.out = 3) ar_beta &lt;- c(0.3, 0.5, 0.7) ar_v_star &lt;- seq(1, 2, length.out = 1) tb_pref &lt;- as_tibble(cbind(ar_lambda)) %&gt;% expand_grid(ar_beta) %&gt;% expand_grid(ar_v_star) %&gt;% rename_all(~c(&#39;lambda&#39;, &#39;beta&#39;, &#39;vstar&#39;)) %&gt;% rowid_to_column(var = &quot;indiff_id&quot;) # Generate indifference points with apply and anonymous function # tb_pref, whatever is selected from it, must be all numeric # if there are strings, would cause conversion error. ls_df_indiff &lt;- apply(tb_pref, 1, function(x){ indiff_id &lt;- x[1] lambda &lt;- x[2] beta &lt;- x[3] vstar &lt;- x[4] ar_fl_A_indiff &lt;- seq(fl_A_min, fl_A_max, length.out=it_A_grid) ar_fl_B_indiff &lt;- (((vstar^lambda) - (beta*ar_fl_A_indiff^(lambda)))/(1-beta))^(1/lambda) mt_A_B_indiff &lt;- cbind(indiff_id, lambda, beta, vstar, ar_fl_A_indiff, ar_fl_B_indiff) colnames(mt_A_B_indiff) &lt;- c(&#39;indiff_id&#39;, &#39;lambda&#39;, &#39;beta&#39;, &#39;vstar&#39;, &#39;indiff_A&#39;, &#39;indiff_B&#39;) tb_A_B_indiff &lt;- as_tibble(mt_A_B_indiff) %&gt;% rowid_to_column(var = &quot;A_grid_id&quot;) %&gt;% filter(indiff_B &gt;= 0 &amp; indiff_B &lt;= max(ar_fl_A_indiff)) return(tb_A_B_indiff) }) df_indiff &lt;- do.call(rbind, ls_df_indiff) %&gt;% drop_na() Note that many more A grid points are needed to fully plot out the leontief line. # Labeling st_title &lt;- paste0(&#39;Indifference Curves Aktinson Atkinson Utility (CES)&#39;) st_subtitle &lt;- paste0(&#39;Each Panel Different beta=A\\&#39;s Weight lambda=inequality aversion\\n&#39;, &#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/math/func_ineq/htmlpdfr/fs_atkinson_ces.html&#39;) st_caption &lt;- paste0(&#39;Indifference Curve 2 Individuals, &#39;, &#39;https://fanwangecon.github.io/R4Econ/&#39;) st_x_label &lt;- &#39;A&#39; st_y_label &lt;- &#39;B&#39; # Graphing plt_indiff &lt;- df_indiff %&gt;% mutate(lambda = as_factor(lambda), beta = as_factor(beta), vstar = as_factor(vstar)) %&gt;% ggplot(aes(x=indiff_A, y=indiff_B, colour=lambda)) + facet_wrap( ~ beta) + geom_line(size=1) + labs(title = st_title, subtitle = st_subtitle, x = st_x_label, y = st_y_label, caption = st_caption) + theme_bw() # show print(plt_indiff) 8.3.3 Location, Population, and Pollution Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 8.3.3.1 Simulate Population Distribution over Location and Demographics Use the binomial distribution to generate heterogenous demographic break-down by location. There are N demographic cells, and the binomial distribution provides the probability mass in each of the N cell. Different bernoulli win chance for each location. There is also probability distribution over population in each location. First, construct empty population share dataframe: # 7 different age groups and 12 different locationso it_N_pop_groups &lt;- 7 it_M_location &lt;- 12 # Matrix of demographics by location mt_pop_data_frac &lt;- matrix(data=NA, nrow=it_M_location, ncol=it_N_pop_groups) colnames(mt_pop_data_frac) &lt;- paste0(&#39;popgrp&#39;, seq(1,it_N_pop_groups)) rownames(mt_pop_data_frac) &lt;- paste0(&#39;location&#39;, seq(1,it_M_location)) # Display mt_pop_data_frac %&gt;% kable() %&gt;% kable_styling_fc() popgrp1 popgrp2 popgrp3 popgrp4 popgrp5 popgrp6 popgrp7 location1 NA NA NA NA NA NA NA location2 NA NA NA NA NA NA NA location3 NA NA NA NA NA NA NA location4 NA NA NA NA NA NA NA location5 NA NA NA NA NA NA NA location6 NA NA NA NA NA NA NA location7 NA NA NA NA NA NA NA location8 NA NA NA NA NA NA NA location9 NA NA NA NA NA NA NA location10 NA NA NA NA NA NA NA location11 NA NA NA NA NA NA NA location12 NA NA NA NA NA NA NA Second, generate conditional population distribution for each location, and then multiply by the share of population in each locality: # Share of population per location set.seed(123) ar_p_loc &lt;- dbinom(0:(3*it_M_location-1), 3*it_M_location-1, 0.5) it_start &lt;- length(ar_p_loc)/2-it_M_location/2 ar_p_loc &lt;- ar_p_loc[it_start:(it_start+it_M_location+1)] ar_p_loc &lt;- ar_p_loc/sum(ar_p_loc) # Different bernoulli &quot;win&quot; probability for each location set.seed(234) # ar_fl_unif_prob &lt;- sort(runif(it_M_location)*(0.25)+0.4) ar_fl_unif_prob &lt;- sort(runif(it_M_location)) # Generate population proportion by locality for (it_loc in 1:it_M_location ) { ar_p_pop_condi_loc &lt;- dbinom(0:(it_N_pop_groups-1), it_N_pop_groups-1, ar_fl_unif_prob[it_loc]) mt_pop_data_frac[it_loc,] &lt;- ar_p_pop_condi_loc*ar_p_loc[it_loc] } # Sum of cells, should equal to 1 print(paste0(&#39;pop frac sum = &#39;, sum(mt_pop_data_frac))) ## [1] &quot;pop frac sum = 0.962953679726938&quot; # Display round(mt_pop_data_frac*100, 2) %&gt;% kable(caption=&#39;Share of population in each location and demographic cell&#39;) %&gt;% kable_styling_fc() Table 8.1: Share of population in each location and demographic cell popgrp1 popgrp2 popgrp3 popgrp4 popgrp5 popgrp6 popgrp7 location1 1.09 0.13 0.01 0.00 0.00 0.00 0.00 location2 1.63 0.70 0.13 0.01 0.00 0.00 0.00 location3 0.59 1.40 1.39 0.74 0.22 0.03 0.00 location4 0.06 0.43 1.29 2.09 1.90 0.92 0.19 location5 0.07 0.55 1.73 2.89 2.71 1.36 0.28 location6 0.02 0.26 1.19 2.89 3.93 2.85 0.86 location7 0.01 0.10 0.66 2.23 4.26 4.33 1.83 location8 0.00 0.06 0.47 1.83 4.03 4.72 2.31 location9 0.00 0.03 0.27 1.26 3.28 4.55 2.63 location10 0.00 0.02 0.20 0.96 2.57 3.68 2.19 location11 0.00 0.00 0.00 0.04 0.40 2.05 4.38 location12 0.00 0.00 0.00 0.02 0.24 1.28 2.82 8.3.3.2 Simulate Enviromental Exposure Use log-normal distribution to describe average daily PM10 exposures distribution by locality: fl_meanlog &lt;- 3.4 fl_sdlog &lt;- 0.35 hist(rlnorm(1000, meanlog = fl_meanlog, sdlog = fl_sdlog)) First, draw pollution measure for each locality: # draw set.seed(123) ar_pollution_loc &lt;- rlnorm(it_M_location, meanlog = fl_meanlog, sdlog = fl_sdlog) # pollution dataframe # 5 by 3 matrix # Column Names ar_st_varnames &lt;- c(&#39;location&#39;,&#39;avgdailypm10&#39;) # Combine to tibble, add name col1, col2, etc. tb_loc_pollution &lt;- as_tibble(ar_pollution_loc) %&gt;% rowid_to_column(var = &quot;id&quot;) %&gt;% rename_all(~c(ar_st_varnames)) %&gt;% mutate(location = paste0(&#39;location&#39;, location)) # Display kable(tb_loc_pollution) %&gt;% kable_styling_fc() location avgdailypm10 location1 24.62676 location2 27.64481 location3 51.70466 location4 30.71275 location5 31.35114 location6 54.61304 location7 35.20967 location8 19.24456 location9 23.56121 location10 25.63653 location11 45.99021 location12 33.98553 Second, reshape population data: # Reshape population data, so each observation is location/demo df_pop_data_frac_long &lt;- as_tibble(mt_pop_data_frac, rownames=&#39;location&#39;) %&gt;% pivot_longer(cols = starts_with(&#39;popgrp&#39;), names_to = c(&#39;popgrp&#39;), names_pattern = paste0(&quot;popgrp(.*)&quot;), values_to = &quot;pop_frac&quot;) Third, join with pollution data: # Reshape population data, so each observation is location/demo df_pop_pollution_long &lt;- df_pop_data_frac_long %&gt;% left_join(tb_loc_pollution, by=&#39;location&#39;) # display df_pop_pollution_long[1:round(it_N_pop_groups*2.5),] %&gt;% kable() %&gt;% kable_styling_fc() location popgrp pop_frac avgdailypm10 location1 1 0.0109366 24.62676 location1 2 0.0013417 24.62676 location1 3 0.0000686 24.62676 location1 4 0.0000019 24.62676 location1 5 0.0000000 24.62676 location1 6 0.0000000 24.62676 location1 7 0.0000000 24.62676 location2 1 0.0163003 27.64481 location2 2 0.0070132 27.64481 location2 3 0.0012573 27.64481 location2 4 0.0001202 27.64481 location2 5 0.0000065 27.64481 location2 6 0.0000002 27.64481 location2 7 0.0000000 27.64481 location3 1 0.0058760 51.70466 location3 2 0.0140000 51.70466 location3 3 0.0138984 51.70466 location3 4 0.0073587 51.70466 8.3.3.3 Compute Demographic Group Specific Exposure Distributions What is the p10, median, p90 and mean pollution exposure for each demographic group? group by population group sort by pollution exposure within group generate population group specific conditional population weights generate population CDF for each population group (sorted by pollution) # Follow four steps above df_pop_pollution_by_popgrp_cdf &lt;- df_pop_pollution_long %&gt;% arrange(popgrp, avgdailypm10) %&gt;% group_by(popgrp) %&gt;% mutate(cdf_pop_condi_popgrp_sortpm10 = cumsum(pop_frac/sum(pop_frac))) # display df_pop_pollution_by_popgrp_cdf[1:round(it_N_pop_groups*5.5),] %&gt;% kable() %&gt;% kable_styling_fc() location popgrp pop_frac avgdailypm10 cdf_pop_condi_popgrp_sortpm10 location8 1 0.0000364 19.24456 0.0010453 location9 1 0.0000151 23.56121 0.0014804 location1 1 0.0109366 24.62676 0.3156484 location10 1 0.0000104 25.63653 0.3159471 location2 1 0.0163003 27.64481 0.7841942 location4 1 0.0005879 30.71275 0.8010816 location5 1 0.0007392 31.35114 0.8223166 location12 1 0.0000000 33.98553 0.8223168 location7 1 0.0000681 35.20967 0.8242718 location11 1 0.0000000 45.99021 0.8242721 location3 1 0.0058760 51.70466 0.9930669 location6 1 0.0002413 54.61304 1.0000000 location8 2 0.0006400 19.24456 0.0172871 location9 2 0.0003150 23.56121 0.0257947 location1 2 0.0013417 24.62676 0.0620374 location10 2 0.0002235 25.63653 0.0680736 location2 2 0.0070132 27.64481 0.2575157 location4 2 0.0042712 30.71275 0.3728918 location5 2 0.0055479 31.35114 0.5227547 location12 2 0.0000004 33.98553 0.5227662 location7 2 0.0010378 35.20967 0.5508009 location11 2 0.0000008 45.99021 0.5508213 location3 2 0.0140000 51.70466 0.9289930 location6 2 0.0026287 54.61304 1.0000000 location8 3 0.0046896 19.24456 0.0638166 location9 3 0.0027290 23.56121 0.1009539 location1 3 0.0000686 24.62676 0.1018872 location10 3 0.0020006 25.63653 0.1291118 location2 3 0.0012573 27.64481 0.1462207 location4 3 0.0129304 30.71275 0.3221799 location5 3 0.0173492 31.35114 0.5582709 location12 3 0.0000141 33.98553 0.5584625 location7 3 0.0065945 35.20967 0.6482016 location11 3 0.0000242 45.99021 0.6485305 location3 3 0.0138984 51.70466 0.8376617 location6 3 0.0119295 54.61304 1.0000000 location8 4 0.0183277 19.24456 0.1224562 location9 4 0.0126118 23.56121 0.2067219 Measure quantiles of pollution exposures for different population groups: Consider CDF larger than current quantile of interest. Slice group-specific CDF that is higher and closest to quantile of interest. Merge results for different quantiles together. # Generate pollution quantiles by population groups df_pop_pollution_distribution &lt;- df_pop_pollution_by_popgrp_cdf %&gt;% mutate(pm10_mean = weighted.mean(avgdailypm10, pop_frac)) %&gt;% filter(cdf_pop_condi_popgrp_sortpm10 &gt;= 0.10) %&gt;% slice(1) %&gt;% mutate(pm10_p10 = avgdailypm10) %&gt;% select(popgrp, pm10_mean, pm10_p10) %&gt;% left_join(df_pop_pollution_by_popgrp_cdf %&gt;% filter(cdf_pop_condi_popgrp_sortpm10 &gt;= 0.20) %&gt;% slice(1) %&gt;% mutate(pm10_p20 = avgdailypm10) %&gt;% select(popgrp, pm10_p20), by=&#39;popgrp&#39;) %&gt;% left_join(df_pop_pollution_by_popgrp_cdf %&gt;% filter(cdf_pop_condi_popgrp_sortpm10 &gt;= 0.50) %&gt;% slice(1) %&gt;% mutate(pm10_p50 = avgdailypm10) %&gt;% select(popgrp, pm10_p50), by=&#39;popgrp&#39;) %&gt;% left_join(df_pop_pollution_by_popgrp_cdf %&gt;% filter(cdf_pop_condi_popgrp_sortpm10 &gt;= 0.80) %&gt;% slice(1) %&gt;% mutate(pm10_p80 = avgdailypm10) %&gt;% select(popgrp, pm10_p80), by=&#39;popgrp&#39;) %&gt;% left_join(df_pop_pollution_by_popgrp_cdf %&gt;% filter(cdf_pop_condi_popgrp_sortpm10 &gt;= 0.90) %&gt;% slice(1) %&gt;% mutate(pm10_p90 = avgdailypm10) %&gt;% select(popgrp, pm10_p90), by=&#39;popgrp&#39;) # display df_pop_pollution_distribution %&gt;% kable() %&gt;% kable_styling_fc() popgrp pm10_mean pm10_p10 pm10_p20 pm10_p50 pm10_p80 pm10_p90 1 31.07894 24.62676 24.62676 27.64481 30.71275 51.70466 2 39.47897 27.64481 27.64481 31.35114 51.70466 51.70466 3 37.92901 23.56121 30.71275 31.35114 51.70466 54.61304 4 34.86470 19.24456 23.56121 31.35114 51.70466 54.61304 5 32.56731 19.24456 23.56121 30.71275 35.20967 54.61304 6 31.46626 19.24456 23.56121 25.63653 35.20967 54.61304 7 33.50541 19.24456 23.56121 33.98553 45.99021 45.99021 "],["tables-and-graphs.html", "Chapter 9 Tables and Graphs 9.1 R Base Plots 9.2 GGplot Line Related Plots 9.3 Write and Read Plots", " Chapter 9 Tables and Graphs 9.1 R Base Plots 9.1.1 Plot Curve, Line and Points Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Work with the R plot function. 9.1.1.1 One Point, One Line and Two Curves r curve on top of plot r plot specify pch lty both scatter and line r legend outside Jointly plot: 1 scatter plot 1 line plot 2 function curve plots ####################################################### # First, Some common Labels: ####################################################### # Labeling st_title &lt;- paste0(&#39;Scatter, Line and Curve Joint Ploting Example Using Base R\\n&#39;, &#39;plot() + curve(): x*sin(x), cos(x), sin(x)*cos(x), sin(x)+tan(x)+cos(x)&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/tabgraph/inout/htmlpdfr/fs_base_curve.html&#39;) st_x_label &lt;- &#39;x&#39; st_y_label &lt;- &#39;f(x)&#39; ####################################################### # Second, Generate the Graphs Functions and data points: ####################################################### # x only used for Point 1 and Line 1 x &lt;- seq(-1*pi, 1*pi, length.out=25) # Line (Point) 1: Generate X and Y y1 &lt;- x*sin(x) st_point_1_y_legend &lt;- &#39;x*sin(x)&#39; # Line 2: Line Plot y2 &lt;- cos(x) st_line_2_y_legend &lt;- &#39;cos(x)&#39; # Line 3: Function fc_sin_cos_diff &lt;- function(x) sin(x)*cos(x) st_line_3_y_legend &lt;- &#39;sin(x)*cos(x)&#39; # Line 4: Function fc_sin_cos_tan &lt;- function(x) sin(x) + cos(x) + tan(x) st_line_4_y_legend &lt;- &#39;sin(x) + tan(x) + cos(x)&#39; ####################################################### # Third, set: # - point shape and size: *pch* and *cex* # - line type and width: *lty* and *lwd* ####################################################### # http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r # http://www.sthda.com/english/wiki/line-types-in-r-lty # for colors, see: https://fanwangecon.github.io/M4Econ/graph/tools/fs_color.html st_point_1_blue &lt;- rgb(57/255,106/255,177/255) st_line_2_red &lt;- rgb(204/255, 37/255, 41/255,) st_line_3_black &lt;- &#39;black&#39; st_line_4_purple &lt;- &#39;orange&#39; # point type st_point_1_pch &lt;- 10 # point size st_point_1_cex &lt;- 2 # line type st_line_2_lty &lt;- &#39;dashed&#39; st_line_3_lty &lt;- &#39;dotted&#39; st_line_4_lty &lt;- &#39;dotdash&#39; # line width st_line_2_lwd &lt;- 3 st_line_3_lwd &lt;- 2.5 st_line_4_lwd &lt;- 3.5 ####################################################### # Fourth: Share xlim and ylim ####################################################### ar_xlim = c(min(x), max(x)) ar_ylim = c(-3.5, 3.5) ####################################################### # Fifth: the legend will be long, will place it to the right of figure, ####################################################### par(new=FALSE, mar=c(5, 4, 4, 10)) ####################################################### # Sixth, the four objects and do not print yet: ####################################################### # pdf(NULL) # Graph Scatter 1 plot(x, y1, type=&quot;p&quot;, col = st_point_1_blue, pch = st_point_1_pch, cex = st_point_1_cex, xlim = ar_xlim, ylim = ar_ylim, panel.first = grid(), ylab = &#39;&#39;, xlab = &#39;&#39;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ann=FALSE) pl_scatter_1 &lt;- recordPlot() # Graph Line 2 par(new=T) plot(x, y2, type=&quot;l&quot;, col = st_line_2_red, lwd = st_line_2_lwd, lty = st_line_2_lty, xlim = ar_xlim, ylim = ar_ylim, ylab = &#39;&#39;, xlab = &#39;&#39;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ann=FALSE) pl_12 &lt;- recordPlot() # Graph Curve 3 par(new=T) curve(fc_sin_cos_diff, col = st_line_3_black, lwd = st_line_3_lwd, lty = st_line_3_lty, from = ar_xlim[1], to = ar_xlim[2], ylim = ar_ylim, ylab = &#39;&#39;, xlab = &#39;&#39;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ann=FALSE) pl_123 &lt;- recordPlot() # Graph Curve 4 par(new=T) curve(fc_sin_cos_tan, col = st_line_4_purple, lwd = st_line_4_lwd, lty = st_line_4_lty, from = ar_xlim[1], to = ar_xlim[2], ylim = ar_ylim, ylab = &#39;&#39;, xlab = &#39;&#39;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ann=FALSE) pl_1234 &lt;- recordPlot() # invisible(dev.off()) ####################################################### # Seventh, Set Title and Legend and Plot Jointly ####################################################### # CEX sizing Contorl Titling and Legend Sizes fl_ces_fig_reg = 1 fl_ces_fig_small = 0.75 # R Legend title(main = st_title, sub = st_subtitle, xlab = st_x_label, ylab = st_y_label, cex.lab=fl_ces_fig_reg, cex.main=fl_ces_fig_reg, cex.sub=fl_ces_fig_small) axis(1, cex.axis=fl_ces_fig_reg) axis(2, cex.axis=fl_ces_fig_reg) grid() # Legend sizing CEX legend(&quot;topright&quot;, inset=c(-0.4,0), xpd=TRUE, c(st_point_1_y_legend, st_line_2_y_legend, st_line_3_y_legend, st_line_4_y_legend), col = c(st_point_1_blue, st_line_2_red, st_line_3_black, st_line_4_purple), pch = c(st_point_1_pch, NA, NA, NA), cex = fl_ces_fig_small, lty = c(NA, st_line_2_lty, st_line_3_lty, st_line_4_lty), lwd = c(NA, st_line_2_lwd, st_line_3_lwd,st_line_4_lwd), title = &#39;Legends&#39;, y.intersp=2) # record final plot pl_1234_final &lt;- recordPlot() We used recordplot() earlier. So now we can print just the first two constructed plots. ####################################################### # Eighth, Plot just the first two saved lines ####################################################### # mar: margin, bottom, left, top, right pl_12 # R Legend par(new=T) title(main = st_title, sub = st_subtitle, xlab = st_x_label, ylab = st_y_label, cex.lab = fl_ces_fig_reg, cex.main = fl_ces_fig_reg, cex.sub = fl_ces_fig_small) # Legend sizing CEX par(new=T) legend(&quot;topright&quot;, inset=c(-0.4,0), xpd=TRUE, c(st_point_1_y_legend, st_line_2_y_legend), col = c(st_point_1_blue, st_line_2_red), pch = c(st_point_1_pch, NA), cex = fl_ces_fig_small, lty = c(NA, st_line_2_lty), lwd = c(NA, st_line_2_lwd), title = &#39;Legends&#39;, y.intersp=2) 9.2 GGplot Line Related Plots 9.2.1 Continuous Outcome with Multiple Categorical Variables Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 9.2.1.1 Three Categories, One is Subplot The outcome is CEV, generated for results with different productivity types (subplot), generated for PE vs GE (linetype), and at different parameter specifications (lower and higher gamma). The graphs rely on this csv file cev_data.csv. # Libraries # library(tidyverse) # Load in CSV bl_save_img &lt;- FALSE spt_csv_root &lt;- c(&#39;C:/Users/fan/R4Econ/tabgraph/ggline/_file/&#39;) spt_img_root &lt;- c(&#39;G:/repos/R4Econ/tabgraph/ggline/_file/&#39;) spn_cev_data &lt;- paste0(spt_csv_root, &#39;cev_data.csv&#39;) spn_cev_graph &lt;- paste0(spt_img_root, &#39;cev_graph.png&#39;) spn_cev_graph_eps &lt;- paste0(spt_img_root, &#39;cev_graph.eps&#39;) df_cev_graph &lt;- as_tibble(read.csv(spn_cev_data)) %&gt;% select(-X) # Dataset subsetting ------ # Line Patterns and Colors ------ # ar_st_age_group_leg_labels &lt;- c(&quot;\\nGE\\n\\u03B3=0.42\\n&quot;, &quot;\\nGE\\n\\u03B3=0.56\\n&quot;, # &quot;\\nPE\\n\\u03B3=0.42\\n&quot;, &quot;\\nPE\\n\\u03B3=0.42\\n&quot;) ar_st_age_group_leg_labels &lt;- c(bquote(&quot;GE,&quot;~gamma == .(0.42)), bquote(&quot;GE,&quot;~gamma == .(0.56)), bquote(&quot;PE,&quot;~gamma == .(0.42)), bquote(&quot;PE,&quot;~gamma == .(0.56))) ar_st_colours &lt;- c(&quot;#85ccff&quot;, &quot;#026aa3&quot;, &quot;#85ccff&quot;, &quot;#026aa3&quot;) ar_st_linetypes &lt;- c(&quot;solid&quot;, &quot;solid&quot;, &quot;longdash&quot;, &quot;longdash&quot;) # Labels and Other Strings ------- st_title &lt;- &#39;&#39; st_x &lt;- &#39;Wealth&#39; st_y &lt;- &#39;Welfare Gain (% CEV)&#39; st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/tabgraph/ggline/htmlpdfr/fs_ggline_mgrp_ncts.html&#39;) # ar_st_age_group_leg_labels &lt;- c(&quot;C\\u2013Optimal&quot;, &quot;V\\u2013Optimal&quot;) prod_type_recode &lt;- c(&quot;Productivity Type\\n(-1 sd)&quot; = &quot;8993&quot;, &quot;Productivity Type\\n(mean)&quot; = &quot;10189&quot;, &quot;Productivity Type\\n(+1 sd)&quot; = &quot;12244&quot;) x.labels &lt;- c(&#39;0&#39;, &#39;200k&#39;, &#39;400k&#39;, &#39;600k&#39;, &#39;800k&#39;) x.breaks &lt;- c(0, 5, 10, 15, 20) x.min &lt;- 0 x.max &lt;- 20 # y.labels &lt;- c(&#39;-0.01&#39;, # &#39;\\u2191\\u2191\\nWelfare\\nGain\\n\\nCEV=0\\n\\nWelfare\\nLoss\\n\\u2193\\u2193&#39;, # &#39;+0.01&#39;, &#39;+0.02&#39;, &#39;+0.03&#39;, &#39;+0.04&#39;,&#39;+0.05&#39;) y.labels &lt;- c(&#39;-0.5 pp&#39;, &#39;CEV=0&#39;, &#39;+0.5 pp&#39;, &#39;+1.0 pp&#39;, &#39;+1.5 pp&#39;, &#39;+2.0 pp&#39;,&#39;+2.5 pp&#39;) y.breaks &lt;- c(-0.01, 0, 0.01, 0.02, 0.03, 0.04, 0.05) y.min &lt;- -0.011 y.max &lt;- 0.051 # data change ------- df_cev_graph &lt;- df_cev_graph %&gt;% filter(across(counter_policy, ~ grepl(&#39;70|42&#39;, .))) %&gt;% mutate(prod_type_lvl = as.factor(prod_type_lvl)) %&gt;% mutate(prod_type_lvl = fct_recode(prod_type_lvl, !!!prod_type_recode)) # graph ------ pl_cev &lt;- df_cev_graph %&gt;% group_by(prod_type_st, cash_tt) %&gt;% ggplot(aes(x=cash_tt, y=cev_lvl, colour=counter_policy, linetype=counter_policy, shape=counter_policy)) + facet_wrap( ~ prod_type_lvl, nrow=1) + geom_smooth(method=&quot;auto&quot;, se=FALSE, fullrange=FALSE, level=0.95) # labels pl_cev &lt;- pl_cev + labs(x = st_x, y = st_y, subtitle = st_subtitle) # set shapes and colors pl_cev &lt;- pl_cev + scale_colour_manual(values=ar_st_colours, labels=ar_st_age_group_leg_labels) + scale_shape_discrete(labels=ar_st_age_group_leg_labels) + scale_linetype_manual(values=ar_st_linetypes, labels=ar_st_age_group_leg_labels) + scale_x_continuous(labels = x.labels, breaks = x.breaks, limits = c(x.min, x.max)) + scale_y_continuous(labels = y.labels, breaks = y.breaks, limits = c(y.min, y.max)) # Horizontal line pl_cev &lt;- pl_cev + geom_hline(yintercept=0, linetype=&#39;solid&#39;, colour=&quot;black&quot;, size =1) # geom_hline(yintercept=0, linetype=&#39;dotted&#39;, colour=&quot;black&quot;, size=2) # theme pl_cev &lt;- pl_cev + theme_bw() + theme(text = element_text(size = 10), legend.title = element_blank(), legend.position = c(0.16, 0.65), legend.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;, linetype=&#39;solid&#39;), legend.key.width = unit(1.5, &quot;cm&quot;)) # Save Image Outputs ----- if (bl_save_img) { png(spn_cev_graph, width = 160, height = 105, units=&#39;mm&#39;, res = 150, pointsize=7) ggsave( spn_cev_graph_eps, plot = last_plot(), device = &#39;eps&#39;, path = NULL, scale = 1, width = 160, height =105, units = c(&quot;mm&quot;), dpi = 150, limitsize = TRUE ) } print(pl_cev) if (bl_save_img) { dev.off() } 9.3 Write and Read Plots 9.3.1 Import and Export Images Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). Work with the R plot function. 9.3.1.1 Export Images Different Formats with Plot() 9.3.1.1.1 Generate and Record A Plot Generate a graph and recordPlot() it. The generated graph does not have legends Yet. Crucially, there are no titles, legends, axis, labels in the figures. As we stack the figures together, do not add those. Only add at the end jointly for all figure elements together to control at one spot things. ####################################################### # First, Strings ####################################################### # Labeling st_title &lt;- paste0(&#39;Scatter, Line and Curve Joint Ploting Example Using Base R\\n&#39;, &#39;plot() + curve():sin(x)*cos(x), sin(x)+tan(x)+cos(x)&#39;) st_subtitle &lt;- paste0(&#39;https://fanwangecon.github.io/&#39;, &#39;R4Econ/tabgraph/inout/htmlpdfr/fs_base_curve.html&#39;) st_x_label &lt;- &#39;x&#39; st_y_label &lt;- &#39;f(x)&#39; ####################################################### # Second, functions ####################################################### fc_sin_cos_diff &lt;- function(x) sin(x)*cos(x) st_line_3_y_legend &lt;- &#39;sin(x)*cos(x)&#39; fc_sin_cos_tan &lt;- function(x) sin(x) + cos(x) + tan(x) st_line_4_y_legend &lt;- &#39;sin(x) + tan(x) + cos(x)&#39; ####################################################### # Third, patterns ####################################################### st_line_3_black &lt;- &#39;black&#39; st_line_4_purple &lt;- &#39;orange&#39; # line type st_line_3_lty &lt;- &#39;dotted&#39; st_line_4_lty &lt;- &#39;dotdash&#39; # line width st_line_3_lwd &lt;- 2.5 st_line_4_lwd &lt;- 3.5 ####################################################### # Fourth: Share xlim and ylim ####################################################### ar_xlim = c(-3, 3) ar_ylim = c(-3.5, 3.5) ####################################################### # Fifth: Even margins ####################################################### par(new=FALSE) ####################################################### # Sixth, the four objects and do not print yet: ####################################################### # Graph Curve 3 par(new=T) curve(fc_sin_cos_diff, col = st_line_3_black, lwd = st_line_3_lwd, lty = st_line_3_lty, from = ar_xlim[1], to = ar_xlim[2], ylim = ar_ylim, ylab = &#39;&#39;, xlab = &#39;&#39;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ann=FALSE) # Graph Curve 4 par(new=T) curve(fc_sin_cos_tan, col = st_line_4_purple, lwd = st_line_4_lwd, lty = st_line_4_lty, from = ar_xlim[1], to = ar_xlim[2], ylim = ar_ylim, ylab = &#39;&#39;, xlab = &#39;&#39;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ann=FALSE) pl_curves_save &lt;- recordPlot() 9.3.1.1.2 Generate Large Font and Small Font Versions of PLot Generate larger font version: # Replay pl_curves_save ####################################################### # Seventh, Set Title and Legend and Plot Jointly ####################################################### # CEX sizing Contorl Titling and Legend Sizes fl_ces_fig_reg = 0.75 fl_ces_fig_leg = 0.75 fl_ces_fig_small = 0.65 # R Legend title(main = st_title, sub = st_subtitle, xlab = st_x_label, ylab = st_y_label, cex.lab=fl_ces_fig_reg, cex.main=fl_ces_fig_reg, cex.sub=fl_ces_fig_small) axis(1, cex.axis=fl_ces_fig_reg) axis(2, cex.axis=fl_ces_fig_reg) grid() # Legend sizing CEX legend(&quot;topleft&quot;, bg=&quot;transparent&quot;, bty = &quot;n&quot;, c(st_line_3_y_legend, st_line_4_y_legend), col = c(st_line_3_black, st_line_4_purple), pch = c(NA, NA), cex = fl_ces_fig_leg, lty = c(st_line_3_lty, st_line_4_lty), lwd = c(st_line_3_lwd,st_line_4_lwd), y.intersp=2) # record final plot pl_curves_large &lt;- recordPlot() dev.off() Generate smaller font version: # Replay pl_curves_save ####################################################### # Seventh, Set Title and Legend and Plot Jointly ####################################################### # CEX sizing Contorl Titling and Legend Sizes fl_ces_fig_reg = 0.45 fl_ces_fig_leg = 0.45 fl_ces_fig_small = 0.25 # R Legend title(main = st_title, sub = st_subtitle, xlab = st_x_label, ylab = st_y_label, cex.lab=fl_ces_fig_reg, cex.main=fl_ces_fig_reg, cex.sub=fl_ces_fig_small) axis(1, cex.axis=fl_ces_fig_reg) axis(2, cex.axis=fl_ces_fig_reg) grid() # Legend sizing CEX legend(&quot;topleft&quot;, bg=&quot;transparent&quot;, bty = &quot;n&quot;, c(st_line_3_y_legend, st_line_4_y_legend), col = c(st_line_3_black, st_line_4_purple), pch = c(NA, NA), cex = fl_ces_fig_leg, lty = c(st_line_3_lty, st_line_4_lty), lwd = c(st_line_3_lwd,st_line_4_lwd), y.intersp=2) # record final plot pl_curves_small &lt;- recordPlot() dev.off() 9.3.1.1.3 Save Plot with Varying Resolutions and Heights Export recorded plot. A4 paper is 8.3 x 11.7, with 1 inch margins, the remaining area is 6.3 x 9.7. For figures that should take half of the page, the height should be 4.8 inch. One third of a page should be 3.2 inch. 6.3 inch is 160mm and 3 inch is 76 mm. In the example below, use # Store both in within folder directory and root image directory: # C:\\Users\\fan\\R4Econ\\tabgraph\\inout\\_img # C:\\Users\\fan\\R4Econ\\_img # need to store in both because bookdown and indi pdf path differ. # Wrap in try because will not work underbookdown, but images already created ls_spt_root &lt;- c(&#39;..//..//&#39;, &#39;&#39;) spt_prefix &lt;- &#39;_img/fs_img_io_2curve&#39; for (spt_root in ls_spt_root) { # Changing pointsize will not change font sizes inside, just rescale # PNG 72 try(png(paste0(spt_root, spt_prefix, &quot;_w135h76_res72.png&quot;), width = 135 , height = 76, units=&#39;mm&#39;, res = 72, pointsize=7)) print(pl_curves_large) dev.off() # PNG 300 try(png(paste0(spt_root, spt_prefix, &quot;_w135h76_res300.png&quot;), width = 135, height = 76, units=&#39;mm&#39;, res = 300, pointsize=7)) print(pl_curves_large) dev.off() # PNG 300, SMALL, POINT SIZE LOWER try(png(paste0(spt_root, spt_prefix, &quot;_w80h48_res300.png&quot;), width = 80, height = 48, units=&#39;mm&#39;, res = 300, pointsize=7)) print(pl_curves_small) dev.off() # PNG 300 try(png(paste0(spt_root, spt_prefix, &quot;_w160h100_res300.png&quot;), width = 160, height = 100, units=&#39;mm&#39;, res = 300)) print(pl_curves_large) dev.off() # EPS setEPS() try(postscript(paste0(spt_root, spt_prefix, &quot;_fs_2curve.eps&quot;))) print(pl_curves_large) dev.off() } ## Error in png(paste0(spt_root, spt_prefix, &quot;_w135h76_res72.png&quot;), width = 135, : ## unable to start png() device ## Error in png(paste0(spt_root, spt_prefix, &quot;_w135h76_res300.png&quot;), width = 135, : ## unable to start png() device ## Error in png(paste0(spt_root, spt_prefix, &quot;_w80h48_res300.png&quot;), width = 80, : ## unable to start png() device ## Error in png(paste0(spt_root, spt_prefix, &quot;_w160h100_res300.png&quot;), width = 160, : ## unable to start png() device ## Error in postscript(paste0(spt_root, spt_prefix, &quot;_fs_2curve.eps&quot;)) : ## cannot open file &#39;..//..//_img/fs_img_io_2curve_fs_2curve.eps&#39; 9.3.1.1.4 Low and High Resolution Figure The standard resolution often produces very low quality images. Resolution should be increased. See figure comparison. 9.3.1.1.5 Smaller and Larger Figures Smaller and larger figures with different font size comparison. Note that earlier, we generated the figure without legends, labels, etc first, recorded the figure. Then we associated the same underlying figure with differently sized titles, legends, axis, labels. "],["get-data.html", "Chapter 10 Get Data 10.1 Environmental Data", " Chapter 10 Get Data 10.1 Environmental Data 10.1.1 ECMWF ERA5 Data Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). This files uses R with the reticulate package to download ECMWF ERA5 data. See this file for instructions and tutorials for downloading the data. 10.1.1.1 Program to Download, Unzip, Convert to combined CSV, derived-utci-historical data The data downloaded from CDS climate could become very large in size. We want to process parts of the data one part at a time, summarize and aggregate over each part, and generate a file output file with aggregate statistics over the entire time period of interest. This code below accompalishes the following tasks: download data from derived-utci-historical as ZIP unzip convert nc files to csv files individual csv files are half year groups Parameter Control for the code below: spt_root: root folder where everything will be at spth_conda_env: the conda virtual environment python path, eccodes and cdsapi packages are installed in the conda virtual environment. In the example below, the first env is: wk_ecmwf st_nc_prefix: the downloaded individual nc files have dates and prefix before and after the date string in the nc file names. This is the string before that. st_nc_suffix: see (3), this is the suffix ar_years: array of years to download and aggregate over ar_months_g1: months to download in first half year ar_months_g2: months to download in second half year Note: area below corresponds to North, West, South, East. ################################################# # ------------ Parameters ################################################# # Where to store everything spt_root &lt;- &quot;C:/Users/fan/Downloads/_data/&quot; spth_conda_env &lt;- &quot;C:/ProgramData/Anaconda3/envs/wk_ecmwf/python.exe&quot; # nc name prefix st_nc_prefix &lt;- &quot;ECMWF_utci_&quot; st_nc_suffix &lt;- &quot;_v1.0_con.nc&quot; # Years list # ar_years &lt;- 2001:2019 ar_years &lt;- c(2005, 2015) # ar_months_g1 &lt;- c(&#39;01&#39;,&#39;02&#39;,&#39;03&#39;,&#39;04&#39;,&#39;05&#39;,&#39;06&#39;) ar_months_g1 &lt;- c(&#39;01&#39;, &#39;03&#39;) # ar_months_g2 &lt;- c(&#39;07&#39;,&#39;08&#39;,&#39;09&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;) ar_months_g2 &lt;- c(&#39;07&#39;, &#39;09&#39;) # Area # # China # fl_area_north &lt;- 53.31 # fl_area_west &lt;- 73 # fl_area_south &lt;- 4.15 # fl_area_east &lt;- 135 fl_area_north &lt;- 53 fl_area_west &lt;- 73 fl_area_south &lt;- 52 fl_area_east &lt;- 74 # folder to download any nc zips to nczippath &lt;- spt_root # we are changing the python api file with different requests stirngs and storing it here pyapipath &lt;- spt_root # output directory for AGGREGATE CSV with all DATES from this search csvpath &lt;- spt_root ################################################# # ------------ Packages ################################################# library(&quot;ncdf4&quot;) library(&quot;chron&quot;) library(&quot;lattice&quot;) library(&quot;RColorBrewer&quot;) library(&quot;stringr&quot;) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) Sys.setenv(RETICULATE_PYTHON = spth_conda_env) library(&quot;reticulate&quot;) ################################################# # ------------ Define Loops ################################################# for (it_yr in ar_years) { for (it_mth_group in c(1,2)) { if(it_mth_group == 1) { ar_months = ar_months_g1 } if(it_mth_group == 2) { ar_months = ar_months_g2 } ################################################# # ------------ Define Python API Call ################################################# # name of zip file nczipname &lt;- &quot;derived_utci_2010_2.zip&quot; unzipfolder &lt;- &quot;derived_utci_2010_2&quot; st_file &lt;- paste0(&quot;import cdsapi import urllib.request # download folder spt_root = &#39;&quot;, nczippath, &quot;&#39; spn_dl_test_grib = spt_root + &#39;&quot;, nczipname, &quot;&#39; # request c = cdsapi.Client() res = c.retrieve( &#39;derived-utci-historical&#39;, { &#39;format&#39;: &#39;zip&#39;, &#39;variable&#39;: &#39;Universal thermal climate index&#39;, &#39;product_type&#39;: &#39;Consolidated dataset&#39;, &#39;year&#39;: &#39;&quot;,it_yr, &quot;&#39;, &#39;month&#39;: [ &quot;, paste(&quot;&#39;&quot;, ar_months, &quot;&#39;&quot;, sep = &quot;&quot;, collapse = &quot;, &quot;), &quot; ], &#39;day&#39;: [ &#39;01&#39;,&#39;03&#39; ], &#39;area&#39; : [&quot;, fl_area_north ,&quot;, &quot;, fl_area_west ,&quot;, &quot;, fl_area_south ,&quot;, &quot;, fl_area_east ,&quot;], &#39;grid&#39; : [0.25, 0.25], }, spn_dl_test_grib) # show results print(&#39;print results&#39;) print(res) print(type(res))&quot;) # st_file = &quot;print(1+1)&quot; # Store Python Api File fl_test_tex &lt;- paste0(pyapipath, &quot;api.py&quot;) fileConn &lt;- file(fl_test_tex) writeLines(st_file, fileConn) close(fileConn) ################################################# # ------------ Run Python File ################################################# # Set Path setwd(pyapipath) # Run py file, api.py name just defined use_python(spth_conda_env) source_python(&#39;api.py&#39;) ################################################# # ------------ uNZIP ################################################# spn_zip &lt;- paste0(nczippath, nczipname) spn_unzip_folder &lt;- paste0(nczippath, unzipfolder) unzip(spn_zip, exdir=spn_unzip_folder) ################################################# # ------------ Find All files ################################################# # Get all files with nc suffix in folder ncpath &lt;- paste0(nczippath, unzipfolder) ls_sfls &lt;- list.files(path=ncpath, recursive=TRUE, pattern=&quot;.nc&quot;, full.names=T) ################################################# # ------------ Combine individual NC files to JOINT Dataframe ################################################# # List to gather dataframes ls_df &lt;- vector(mode = &quot;list&quot;, length = length(ls_sfls)) # Loop over files and convert nc to csv it_df_ctr &lt;- 0 for (spt_file in ls_sfls) { it_df_ctr &lt;- it_df_ctr + 1 # Get file name without Path snm_file_date &lt;- sub(paste0(&#39;\\\\&#39;,st_nc_suffix,&#39;$&#39;), &#39;&#39;, basename(spt_file)) snm_file_date &lt;- sub(st_nc_prefix, &#39;&#39;, basename(snm_file_date)) # Dates Start and End: list.files is auto sorted in ascending order if (it_df_ctr == 1) { snm_start_date &lt;- snm_file_date } else { # this will give the final date snm_end_date &lt;- snm_file_date } # Given this structure: ECMWF_utci_20100702_v1.0_con, sub out prefix and suffix print(spt_file) ncin &lt;- nc_open(spt_file) nchist &lt;- ncatt_get(ncin, 0, &quot;history&quot;) # not using this missing value flag at the moment missingval &lt;- str_match(nchist$value, &quot;setmisstoc,\\\\s*(.*?)\\\\s* &quot;)[,2] missingval &lt;- as.numeric(missingval) lon &lt;- ncvar_get(ncin, &quot;lon&quot;) lat &lt;- ncvar_get(ncin, &quot;lat&quot;) tim &lt;- ncvar_get(ncin, &quot;time&quot;) tunits &lt;- ncatt_get(ncin, &quot;time&quot;, &quot;units&quot;) nlon &lt;- dim(lon) nlat &lt;- dim(lat) ntim &lt;- dim(tim) # convert time -- split the time units string into fields # tustr &lt;- strsplit(tunits$value, &quot; &quot;) # tdstr &lt;- strsplit(unlist(tustr)[3], &quot;-&quot;) # tmonth &lt;- as.integer(unlist(tdstr)[2]) # tday &lt;- as.integer(unlist(tdstr)[3]) # tyear &lt;- as.integer(unlist(tdstr)[1]) # mytim &lt;- chron(tim, origin = c(tmonth, tday, tyear)) tmp_array &lt;- ncvar_get(ncin, &quot;utci&quot;) tmp_array &lt;- tmp_array - 273.15 lonlat &lt;- as.matrix(expand.grid(lon = lon, lat = lat, hours = tim)) temperature &lt;- as.vector(tmp_array) tmp_df &lt;- data.frame(cbind(lonlat, temperature)) # extract a rectangle eps &lt;- 1e-8 minlat &lt;- 22.25 - eps maxlat &lt;- 23.50 + eps minlon &lt;- 113.00 - eps maxlon &lt;- 114.50 + eps # subset data subset_df &lt;- tmp_df[tmp_df$lat &gt;= minlat &amp; tmp_df$lat &lt;= maxlat &amp; tmp_df$lon &gt;= minlon &amp; tmp_df$lon &lt;= maxlon, ] # add Date subset_df_date &lt;- as_tibble(subset_df) %&gt;% mutate(date = snm_file_date) # Add to list ls_df[[it_df_ctr]] &lt;- subset_df_date # Close NC nc_close(ncin) } # List of DF to one DF df_all_nc &lt;- do.call(rbind, ls_df) # Save File fname &lt;- paste0(paste0(st_nc_prefix, snm_start_date, &quot;_to_&quot;, snm_end_date, &quot;.csv&quot;)) csvfile &lt;- paste0(csvpath, fname) write.table(na.omit(df_all_nc), csvfile, row.names = FALSE, sep = &quot;,&quot;) # Delete folders unlink(spn_zip, recursive=TRUE, force=TRUE) unlink(spn_unzip_folder, recursive=TRUE, force=TRUE) # end loop months groups } # end loop year } "],["code-and-development.html", "Chapter 11 Code and Development 11.1 Files In and Out 11.2 Python with R 11.3 Command Line", " Chapter 11 Code and Development 11.1 Files In and Out 11.1.1 File Path Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 11.1.1.1 Compose a Path File Path might contain information related to the file, decompose the file path, keep the final N folder names, to be possibled stored as a variable in the datafile stored inside. # Compose together a path print(paste0(&#39;.Platform$file.sep=&#39;, .Platform$file.sep)) ## [1] &quot;.Platform$file.sep=/&quot; spn_file_path = file.path(&quot;C:&quot;, &quot;Users&quot;, &quot;fan&quot;, &quot;R4Econ&quot;, &quot;amto&quot;, &quot;tibble&quot;, &quot;fs_tib_basics.Rmd&quot;, fsep = .Platform$file.sep) # print print(spn_file_path) ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&quot; 11.1.1.2 Substring and File Name From path, get file name without suffix. r string split r list last element r get file name from path r get file path no name st_example &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; st_file_wth_suffix_s &lt;- tail(strsplit(st_example, &quot;/&quot;)[[1]],n=1) st_file_wno_suffix_s &lt;- tools::file_path_sans_ext(basename(st_example)) st_fullpath_nosufx_s &lt;- sub(&#39;\\\\.Rmd$&#39;, &#39;&#39;, st_example) st_fullpath_noname_s &lt;- dirname(st_example) print(paste0(&#39;st_file_wth_suffix_s:&#39;, st_file_wth_suffix_s)) ## [1] &quot;st_file_wth_suffix_s:fs_tib_basics.Rmd&quot; print(paste0(&#39;st_file_wno_suffix_s:&#39;, st_file_wno_suffix_s)) ## [1] &quot;st_file_wno_suffix_s:fs_tib_basics&quot; print(paste0(&#39;st_fullpath_nosufx_s:&#39;, st_fullpath_nosufx_s)) ## [1] &quot;st_fullpath_nosufx_s:C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics&quot; print(paste0(&#39;st_fullpath_noname_s:&#39;, st_fullpath_noname_s)) ## [1] &quot;st_fullpath_noname_s:C:/Users/fan/R4Econ/amto/tibble&quot; 11.1.1.3 Get Subset of Path Folder Names File Path might contain information related to the file, decompose the file path, keep the final N folder names, to be possibled stored as a variable in the datafile stored inside. # Compose together a path spn_example &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; # Replace default system slash, assume spn was generated by system. ls_srt_folders_file &lt;- strsplit(st_example, .Platform$file.sep)[[1]] # Keep the last N layers it_folders_names_to_keep = 2 snm_file_name &lt;- tail(strsplit(st_example, &quot;/&quot;)[[1]],n=1) ls_srt_folders_keep &lt;- head(tail(ls_srt_folders_file, it_folders_names_to_keep+1), it_folders_names_to_keep) # Show folder names print(paste0(&#39;snm_file_name:&#39;, snm_file_name)) ## [1] &quot;snm_file_name:fs_tib_basics.Rmd&quot; print(paste0(&#39;last &#39;, it_folders_names_to_keep, &#39; folders:&#39;)) ## [1] &quot;last 2 folders:&quot; print(ls_srt_folders_keep) ## [1] &quot;amto&quot; &quot;tibble&quot; Shorter lines, to make copying easier. # inputs it_folders_names_to_keep = 2 spn_example &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; # copy these ls_srt_folders_name_keep &lt;- tail(strsplit(st_example, &quot;/&quot;)[[1]], n=it_folders_names_to_keep+1) snm_file_name &lt;- tail(ls_srt_folders_name_keep, 1) ls_srt_folders_keep &lt;- head(ls_srt_folders_name_keep, it_folders_names_to_keep) # print print(paste0(&#39;snm_file_name:&#39;, snm_file_name)) ## [1] &quot;snm_file_name:fs_tib_basics.Rmd&quot; print(paste0(&#39;last &#39;, it_folders_names_to_keep, &#39; folders:&#39;)) ## [1] &quot;last 2 folders:&quot; print(ls_srt_folders_keep) ## [1] &quot;amto&quot; &quot;tibble&quot; 11.1.2 Text to File Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 11.1.2.1 Latex Table to File Tabular outputs, text outputs, etc are saved as variables, which could be printed in console. They can also be saved to file for future re-used. For example, latex outputs need to be saved to file. # Load Data dt &lt;- mtcars[1:4, 1:6] # Generate latex string variable st_out_tex &lt;- kable(dt, &quot;latex&quot;) print(st_out_tex) # File out # fileConn &lt;- file(&quot;./../../_file/tex/tex_sample_a_tab.tex&quot;) fileConn &lt;- file(&quot;_file/tex/tex_sample_a_tab.tex&quot;) writeLines(st_out_tex, fileConn) close(fileConn) 11.1.2.2 Create a Text File from Strings st_file &lt;- &quot;\\\\documentclass[12pt,english]{article} \\\\usepackage[bottom]{footmisc} \\\\usepackage[urlcolor=blue]{hyperref} \\\\begin{document} \\\\title{A Latex Testing File} \\\\author{\\\\href{http://fanwangecon.github.io/}{Fan Wang} \\\\thanks{See information \\\\href{https://fanwangecon.github.io/Tex4Econ/}{Tex4Econ} for more.}} \\\\maketitle Ipsum information dolor sit amet, consectetur adipiscing elit. Integer Latex placerat nunc orci. \\\\paragraph{\\\\href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3140132}{Data}} Village closure information is taken from a village head survey.\\\\footnote{Generally students went to schools.} output: pdf_document: pandoc_args: &#39;..//..//_output_kniti_pdf.yaml&#39; includes: in_header: &#39;..//..//preamble.tex&#39; html_document: pandoc_args: &#39;..//..//_output_kniti_html.yaml&#39; includes: in_header: &#39;..//..//hdga.html&#39; \\\\end{document}&quot; print(st_file) ## [1] &quot;\\\\documentclass[12pt,english]{article}\\n\\\\usepackage[bottom]{footmisc}\\n\\\\usepackage[urlcolor=blue]{hyperref}\\n\\\\begin{document}\\n\\\\title{A Latex Testing File}\\n\\\\author{\\\\href{http://fanwangecon.github.io/}{Fan Wang} \\\\thanks{See information \\\\href{https://fanwangecon.github.io/Tex4Econ/}{Tex4Econ} for more.}}\\n\\\\maketitle\\nIpsum information dolor sit amet, consectetur adipiscing elit. Integer Latex placerat nunc orci.\\n\\\\paragraph{\\\\href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3140132}{Data}}\\nVillage closure information is taken from a village head survey.\\\\footnote{Generally students went to schools.}\\noutput:\\n pdf_document:\\n pandoc_args: &#39;..//..//_output_kniti_pdf.yaml&#39;\\n includes:\\n in_header: &#39;..//..//preamble.tex&#39;\\n html_document:\\n pandoc_args: &#39;..//..//_output_kniti_html.yaml&#39;\\n includes:\\n in_header: &#39;..//..//hdga.html&#39;\\n\\\\end{document}&quot; fl_test_tex &lt;- &quot;_file/tex/test_fan.tex&quot; fileConn &lt;- file(fl_test_tex) writeLines(st_file, fileConn) close(fileConn) 11.1.2.3 Open A File and Read Lines Open and Replace Text in File: fileConn &lt;- file(fl_test_tex, &quot;r&quot;) st_file_read &lt;- readLines(fileConn) print(st_file_read) ## [1] &quot;\\\\documentclass[12pt,english]{article}&quot; ## [2] &quot;\\\\usepackage[bottom]{footmisc}&quot; ## [3] &quot;\\\\usepackage[urlcolor=blue]{hyperref}&quot; ## [4] &quot;\\\\begin{document}&quot; ## [5] &quot;\\\\title{A Latex Testing File}&quot; ## [6] &quot;\\\\author{\\\\href{http://fanwangecon.github.io/}{Fan Wang} \\\\thanks{See information \\\\href{https://fanwangecon.github.io/Tex4Econ/}{Tex4Econ} for more.}}&quot; ## [7] &quot;\\\\maketitle&quot; ## [8] &quot;Ipsum information dolor sit amet, consectetur adipiscing elit. Integer Latex placerat nunc orci.&quot; ## [9] &quot;\\\\paragraph{\\\\href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3140132}{Data}}&quot; ## [10] &quot;Village closure information is taken from a village head survey.\\\\footnote{Generally students went to schools.}&quot; ## [11] &quot;output:&quot; ## [12] &quot; pdf_document:&quot; ## [13] &quot; pandoc_args: &#39;..//..//_output_kniti_pdf.yaml&#39;&quot; ## [14] &quot; includes:&quot; ## [15] &quot; in_header: &#39;..//..//preamble.tex&#39;&quot; ## [16] &quot; html_document:&quot; ## [17] &quot; pandoc_args: &#39;..//..//_output_kniti_html.yaml&#39;&quot; ## [18] &quot; includes:&quot; ## [19] &quot; in_header: &#39;..//..//hdga.html&#39;&quot; ## [20] &quot;\\\\end{document}&quot; close(fileConn) 11.1.2.4 Open a File and Replace Some Lines Append additional strings into the file after html_document with proper spacings: # Read in Lines from existing file fileConn &lt;- file(fl_test_tex, &quot;r&quot;) st_file_read &lt;- readLines(fileConn) close(fileConn) # Search and Replace String st_search &lt;- &quot;html_document:&quot; st_replace &lt;- paste0(&quot;html_document:\\r\\n&quot;, &quot; toc: true\\r\\n&quot;, &quot; number_sections: true\\r\\n&quot;, &quot; toc_float:\\r\\n&quot;, &quot; collapsed: false\\r\\n&quot;, &quot; smooth_scroll: false\\r\\n&quot;, &quot; toc_depth: 3&quot;) # Search and Replace st_file_updated &lt;- gsub(x = st_file_read, pattern = st_search, replacement = st_replace) # Print print(st_file_updated) ## [1] &quot;\\\\documentclass[12pt,english]{article}&quot; ## [2] &quot;\\\\usepackage[bottom]{footmisc}&quot; ## [3] &quot;\\\\usepackage[urlcolor=blue]{hyperref}&quot; ## [4] &quot;\\\\begin{document}&quot; ## [5] &quot;\\\\title{A Latex Testing File}&quot; ## [6] &quot;\\\\author{\\\\href{http://fanwangecon.github.io/}{Fan Wang} \\\\thanks{See information \\\\href{https://fanwangecon.github.io/Tex4Econ/}{Tex4Econ} for more.}}&quot; ## [7] &quot;\\\\maketitle&quot; ## [8] &quot;Ipsum information dolor sit amet, consectetur adipiscing elit. Integer Latex placerat nunc orci.&quot; ## [9] &quot;\\\\paragraph{\\\\href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3140132}{Data}}&quot; ## [10] &quot;Village closure information is taken from a village head survey.\\\\footnote{Generally students went to schools.}&quot; ## [11] &quot;output:&quot; ## [12] &quot; pdf_document:&quot; ## [13] &quot; pandoc_args: &#39;..//..//_output_kniti_pdf.yaml&#39;&quot; ## [14] &quot; includes:&quot; ## [15] &quot; in_header: &#39;..//..//preamble.tex&#39;&quot; ## [16] &quot; html_document:\\r\\n toc: true\\r\\n number_sections: true\\r\\n toc_float:\\r\\n collapsed: false\\r\\n smooth_scroll: false\\r\\n toc_depth: 3&quot; ## [17] &quot; pandoc_args: &#39;..//..//_output_kniti_html.yaml&#39;&quot; ## [18] &quot; includes:&quot; ## [19] &quot; in_header: &#39;..//..//hdga.html&#39;&quot; ## [20] &quot;\\\\end{document}&quot; # Save Updated File fl_srcrep_tex &lt;- &quot;_file/tex/test_fan_search_replace.tex&quot; fileConn_sr &lt;- file(fl_srcrep_tex) writeLines(st_file_updated, fileConn_sr) close(fileConn_sr) 11.1.3 Rmd to HTML Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 11.1.3.1 Search and Find all Files in Repository Search inside directories, for all files in a repository that have a particular suffix and that dont contain skip pattern list string items. # Serch Folder and skip list spt_roots &lt;- c(&#39;C:/Users/fan/R4Econ/amto&#39;, &#39;C:/Users/fan/R4Econ/summarize&#39;) spn_skip &lt;- c(&#39;summarize&#39;, &#39;panel&#39;, &#39;support&#39;) # Search and get all Path ls_sfls &lt;- list.files(path=spt_roots, recursive=T, pattern=&quot;.Rmd&quot;, full.names=T) # Skip path if contains words in skip list if(!missing(spn_skip)) { ls_sfls &lt;- ls_sfls[!grepl(paste(spn_skip, collapse = &quot;|&quot;), ls_sfls)] } # Loop and print for (spt_file in ls_sfls) { st_fullpath_nosufx &lt;- tail(strsplit(spt_file, &quot;/&quot;)[[1]],n=1) print(paste0(spt_file, &#39;---&#39;, st_fullpath_nosufx)) } ## [1] &quot;C:/Users/fan/R4Econ/amto/array/fs_ary_basics.Rmd---fs_ary_basics.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/array/fs_ary_generate.Rmd---fs_ary_generate.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/array/fs_ary_mesh.Rmd---fs_ary_mesh.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/array/fs_ary_string.Rmd---fs_ary_string.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/array/main.Rmd---main.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/list/fs_lst_basics.Rmd---fs_lst_basics.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/list/main.Rmd---main.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/main.Rmd---main.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/matrix/fs_mat_generate.Rmd---fs_mat_generate.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/matrix/fs_mat_linear_algebra.Rmd---fs_mat_linear_algebra.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/matrix/main.Rmd---main.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd---fs_tib_basics.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_factors.Rmd---fs_tib_factors.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_na.Rmd---fs_tib_na.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_random_draws.Rmd---fs_tib_random_draws.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_string.Rmd---fs_tib_string.Rmd&quot; ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/main.Rmd---main.Rmd&quot; 11.1.3.2 Search and Find all Git Modified or New Rmd Search inside directories, for all files in a git repo folder that are new or have been modified. Ignore possible subset of file based on string search. # Serch Folder and skip list spt_roots &lt;- c(&#39;C:/Users/fan/R4Econ/amto&#39;, &#39;C:/Users/fan/R4Econ/development&#39;) spn_skip &lt;- c(&#39;summarize&#39;, &#39;panel&#39;, &#39;support&#39;) ls_sfls &lt;- list.files(path=spt_roots, recursive=T, pattern=&quot;.Rmd&quot;, full.names=T) if(!missing(spn_skip)) { ls_sfls &lt;- ls_sfls[!grepl(paste(spn_skip, collapse = &quot;|&quot;), ls_sfls)] } # Loop and print for (spt_file in ls_sfls) { spg_check_git_status &lt;- paste0(&#39;git status -s &#39;, spt_file) st_git_status &lt;- toString(system(spg_check_git_status, intern=TRUE)) bl_modified &lt;- grepl(&#39; M &#39;, st_git_status, fixed=TRUE) bl_anewfile &lt;- grepl(&#39;?? &#39;, st_git_status, fixed=TRUE) bl_nochange &lt;- (st_git_status == &quot;&quot;) if (bl_modified == 1) { print(paste0(&#39;MODIFIED: &#39;, spt_file)) } else if (bl_anewfile == 1) { print(paste0(&#39;A NEW FL: &#39;, spt_file)) } else { print(paste0(&#39;NO CHNGE: &#39;, spt_file)) } } ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/array/fs_ary_basics.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/array/fs_ary_generate.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/array/fs_ary_mesh.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/array/fs_ary_string.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/array/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/list/fs_lst_basics.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/list/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/matrix/fs_mat_generate.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/matrix/fs_mat_linear_algebra.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/matrix/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/tibble/fs_tib_factors.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/tibble/fs_tib_na.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/tibble/fs_tib_random_draws.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/tibble/fs_tib_string.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/amto/tibble/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/_file/rmd/fs_rmd_pdf_html_mod.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/_file/rmd/fs_rmd_pdf_html_mod_mod.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/_file/rmd/fs_text_save_mod.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/_file/rmd/main_mod.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/fs_path.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/fs_rmd_pdf_html.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/fs_text_save.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/inout/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/python/fs_python_reticulate.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/python/main.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/system/fs_system_shell.Rmd&quot; ## [1] &quot;NO CHNGE: C:/Users/fan/R4Econ/development/system/main.Rmd&quot; 11.1.3.3 Resave an Existing File with Different Name Different Folder Given an existing Rmd File, Resave it with a different name (add to name suffix), and then save in a different folder: old file: /R4Econ/development/fs_rmd_pdf_html.Rmd new file: *R4Econ/development/inout/_file/rmd/fs_rmd_pdf_html_mod.Rmd* # Serch Folder and skip list spt_roots &lt;- c(&#39;C:/Users/fan/R4Econ/development/inout/&#39;) spn_skip &lt;- c(&#39;_main&#39;, &#39;_file&#39;) ls_sfls &lt;- list.files(path=spt_roots, recursive=T, pattern=&quot;.Rmd&quot;, full.names=T) if(!missing(spn_skip)) { ls_sfls &lt;- ls_sfls[!grepl(paste(spn_skip, collapse = &quot;|&quot;), ls_sfls)] } # Loop and print for (spt_file in ls_sfls) { spt_new &lt;- paste0(&#39;_file/rmd/&#39;) spn_new &lt;- paste0(spt_new, sub(&#39;\\\\.Rmd$&#39;, &#39;&#39;, basename(spt_file)), &#39;_mod.Rmd&#39;) print(spt_new) print(spn_new) fileConn_rd &lt;- file(spt_file, &quot;r&quot;) st_file_read &lt;- readLines(fileConn_rd) fileConn_sr &lt;- file(spn_new) writeLines(st_file_read, fileConn_sr) close(fileConn_rd) close(fileConn_sr) } 11.1.3.3.1 Replacment Function Change Markdown Hierarchy and Add to YAML Given an existing Rmd File, Resave it with a different name, and replace (add in) additional yaml contents. spn_file = &#39;_file/rmd/fs_rmd_pdf_html_mod.Rmd&#39; fileConn_sr &lt;- file(spn_file) st_file &lt;- readLines(fileConn_sr) # print(st_file) st_search &lt;- &quot;html_document: toc: true number_sections: true toc_float: collapsed: false smooth_scroll: false toc_depth: 3&quot; st_replace &lt;- paste0(&quot;html_document: toc: true number_sections: true toc_float: collapsed: false smooth_scroll: false toc_depth: 3\\n&quot;, &quot; toc: true\\n&quot;, &quot; number_sections: true\\n&quot;, &quot; toc_float:\\n&quot;, &quot; collapsed: false\\n&quot;, &quot; smooth_scroll: false\\n&quot;, &quot; toc_depth: 3&quot;) st_file_updated &lt;- gsub(x = st_file, pattern = st_search, replacement = st_replace) st_search &lt;- &quot;../../&quot; st_replace &lt;- paste0(&quot;../../../../&quot;) st_file_updated &lt;- gsub(x = st_file_updated, pattern = st_search, replacement = st_replace) st_file_updated &lt;- gsub(x = st_file_updated, pattern = &#39;# &#39;, replacement = &#39;# &#39;) st_file_updated &lt;- gsub(x = st_file_updated, pattern = &#39;## &#39;, replacement = &#39;## &#39;) st_file_updated &lt;- gsub(x = st_file_updated, pattern = &#39;### &#39;, replacement = &#39;# &#39;) spn_file = &#39;_file/rmd/fs_rmd_pdf_html_mod.Rmd&#39; fileConn_sr &lt;- file(spn_file) st_file &lt;- writeLines(st_file_updated, fileConn_sr) 11.1.3.4 Search and Render Rmd File and Save HTML, PDF or R Search files satisfying conditions in a folder knit files to HTML (and re-run the contents of the file) Save output to a different folder # Specify Parameters ar_spt_root = c(&#39;C:/Users/fan/R4Econ/amto/array/&#39;, &#39;C:/Users/fan/R4Econ/math/integration&#39;) bl_recursive = TRUE st_rmd_suffix_pattern = &quot;*.Rmd&quot; ar_spn_skip &lt;- c(&#39;basics&#39;, &#39;integrate&#39;, &#39;main&#39;, &#39;mesh&#39;) ls_bool_convert &lt;- list(bl_pdf=TRUE, bl_html=TRUE, bl_R=TRUE) spt_out_directory &lt;- &#39;C:/Users/fan/Downloads/_data&#39; bl_verbose &lt;- TRUE # Get Path ls_sfls &lt;- list.files(path=ar_spt_root, recursive=bl_recursive, pattern=st_rmd_suffix_pattern, full.names=T) # Exclude Some Files given ar_spn_skip if(!missing(ar_spn_skip)) { ls_sfls &lt;- ls_sfls[!grepl(paste(ar_spn_skip, collapse = &quot;|&quot;), ls_sfls)] } # Loop over files for (spn_file in ls_sfls) { # Parse File Name spt_file &lt;- dirname(spn_file) sna_file &lt;- tools::file_path_sans_ext(basename(spn_file)) # Output FIles spn_file_pdf &lt;- paste0(spt_file, sna_file, &#39;.pdf&#39;) spn_file_html &lt;- paste0(spt_file, sna_file, &#39;.html&#39;) spn_file_R &lt;- paste0(spt_file, sna_file, &#39;.R&#39;) # render to PDF if (ls_bool_convert$bl_pdf) { if (bl_verbose) message(paste0(&#39;spn_file_pdf:&#39;,spn_file_pdf, &#39;, PDF started&#39;)) rmarkdown::render(spn_file, output_format=&#39;pdf_document&#39;, output_dir = spt_out_directory, output_file = sna_file) if (bl_verbose) message(paste0(&#39;spn_file_pdf:&#39;,spn_file_pdf, &#39;, PDF finished&#39;)) spn_pdf_generated &lt;- paste0(spt_out_directory, &#39;/&#39;, spn_file_pdf) } # render to HTML if (ls_bool_convert$bl_html) { if (bl_verbose) message(paste0(&#39;spth_html:&#39;,spn_file_html, &#39;, HTML started.&#39;)) rmarkdown::render(spn_file, output_format=&#39;html_document&#39;, output_dir = spt_out_directory, output_file = sna_file) if (bl_verbose) message(paste0(&#39;spth_html:&#39;,spn_file_html, &#39;, HTML finished.&#39;)) spn_html_generated &lt;- paste0(spt_out_directory, &#39;/&#39;, spn_file_html) } # purl to R if (ls_bool_convert$bl_R) { if (bl_verbose) message(paste0(&#39;purl_to:&#39;, paste0(spn_file_R, &quot;.R&quot;))) knitr::purl(spn_file, output=paste0(spt_out_directory, &#39;/&#39;, sna_file, &#39;.R&#39;), documentation = 1) spn_R_generated &lt;- paste0(spt_out_directory, &#39;/&#39;, sna_file, &#39;.R&#39;) } # return(list(ls_spt_pdf_generated=ls_spt_pdf_generated, # ls_spt_html_generated=ls_spt_html_generated, # ls_spt_R_generated=ls_spt_R_generated)) } 11.2 Python with R 11.2.1 Reticulate Basics Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 11.2.1.1 Basic Python Tests with RMD Could specify: python, engine.path = C:/ProgramData/Anaconda3/envs/wk_pyfan/python.exe, this is already set inside Rprofile: knitr::opts_chunk$set(engine.path = C:/ProgramData/Anaconda3/envs/wk_pyfan/python.exe) 1+1 ## 2 11.2.1.2 Install and Python Path Install reticulate from github directly to get latest version: devtools::install_github(rstudio/reticulate) Check python version on computer: Sys.which(&#39;python&#39;) ## python ## &quot;G:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\wk_pyfan\\\\python.exe&quot; After installing reticulate, load in the library: library(reticulate). With py_config() to see python config. First time, might generate No non-system installation of Python could be found. and ask if want to install Miniconda. Answer NO. Correct outputs upon checking py_config(): python: C:/ProgramData/Anaconda3/python.exe libpython: C:/ProgramData/Anaconda3/python37.dll pythonhome: C:/ProgramData/Anaconda3 version: 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)] Architecture: 64bit numpy: C:/ProgramData/Anaconda3/Lib/site-packages/numpy numpy_version: 1.19.1 python versions found: C:/ProgramData/Anaconda3/python.exe C:/ProgramData/Anaconda3/envs/wk_cgefi/python.exe C:/ProgramData/Anaconda3/envs/wk_jinja/python.exe C:/ProgramData/Anaconda3/envs/wk_pyfan/python.exe Set which python to use: # Sys.setenv(RETICULATE_PYTHON = &quot;C:/programdata/Anaconda3/python.exe&quot;) # Sys.setenv(RETICULATE_PYTHON = &quot;C:/ProgramData/Anaconda3/envs/wk_pyfan/python.exe&quot;) library(reticulate) # What is the python config py_config() ## python: G:/ProgramData/Anaconda3/envs/wk_pyfan/python.exe ## libpython: G:/ProgramData/Anaconda3/envs/wk_pyfan/python38.dll ## pythonhome: G:/ProgramData/Anaconda3/envs/wk_pyfan ## version: 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] ## Architecture: 64bit ## numpy: G:/ProgramData/Anaconda3/envs/wk_pyfan/Lib/site-packages/numpy ## numpy_version: 1.19.4 ## ## NOTE: Python version was forced by use_python function # set python # use_python(&quot;C:/programdata/Anaconda3/python.exe&quot;) # use_python(&quot;C:/ProgramData/Anaconda3/envs/wk_pyfan/python.exe&quot;) use_condaenv(&#39;wk_pyfan&#39;) # Sys.which(&#39;python&#39;) py_run_string(&#39;print(1+1)&#39;) 11.2.1.3 Error 11.2.1.3.1 py_call_impl error The error appeared when calling any python operations, including 1+1, resolved after installing reticulate from github: devtools::install_github(rstudio/reticulate) Error in py_call_impl(callable, dots$args, dots$keywords) : TypeError: use() got an unexpected keyword argument &#39;warn&#39; 11.3 Command Line 11.3.1 Shell and System Commands Go back to fans REconTools Package, R Code Examples Repository (bookdown site), or Intro Stats with R Repository (bookdown site). 11.3.1.1 Basic Shell Commands Run basic shell commands in windows: # detect current path print(toString(shell(paste0(&quot;echo %cd%&quot;), intern=TRUE))) ## [1] &quot;G:\\\\repos\\\\R4Econ&quot; # Show directory print(toString(shell(paste0(&quot;dir&quot;), intern=TRUE))) ## [1] &quot; Volume in drive G is New Volume, Volume Serial Number is 009D-37E9, , Directory of G:\\\\repos\\\\R4Econ, , 02/02/2021 09:13 PM &lt;DIR&gt; ., 02/02/2021 09:13 PM &lt;DIR&gt; .., 11/28/2020 12:35 PM 203 .gitattributes, 11/28/2020 12:35 PM 1,460 .gitignore, 11/28/2020 12:35 PM 30 .Rbuildignore, 11/28/2020 04:36 PM 6,555 .Rprofile, 11/28/2020 12:35 PM &lt;DIR&gt; amto, 11/28/2020 12:35 PM 278 book.bib, 01/24/2021 11:55 AM &lt;DIR&gt; bookdown, 11/28/2020 12:35 PM &lt;DIR&gt; development, 11/28/2020 12:35 PM &lt;DIR&gt; dynamic, 11/28/2020 04:01 PM &lt;DIR&gt; factor, 11/28/2020 12:35 PM &lt;DIR&gt; function, 11/28/2020 12:35 PM &lt;DIR&gt; generate, 11/28/2020 12:35 PM &lt;DIR&gt; getdata, 11/28/2020 12:35 PM 2,951 gitsetup.md, 11/28/2020 12:35 PM 321 hdga.html, 11/28/2020 12:35 PM &lt;DIR&gt; htmlpdfr, 11/28/2020 12:35 PM 2,928 index.Rmd, 11/28/2020 12:35 PM 1,086 LICENSE, 11/28/2020 12:35 PM &lt;DIR&gt; linreg, 01/08/2021 07:50 AM &lt;DIR&gt; math, 11/28/2020 12:35 PM &lt;DIR&gt; optimization, 11/28/2020 12:35 PM 11,018 packages.bib, 01/10/2021 06:10 PM &lt;DIR&gt; panel, 02/02/2021 09:13 PM 360,944 Panel Data and Optimization with R.Rmd, 01/24/2021 11:55 AM 76,062 Panel-Data-and-Optimization-with-R.log, 02/02/2021 09:13 PM 360,944 Panel-Data-and-Optimization-with-R.Rmd, 11/30/2020 10:49 PM &lt;DIR&gt; Panel-Data-and-Optimization-with-R_files, 11/28/2020 12:35 PM 480 preamble.tex, 11/28/2020 12:35 PM 583 preamble_book.tex, 11/28/2020 12:35 PM 416 R4Econ.Rproj, 02/02/2021 09:13 PM 48,018 README.md, 02/02/2021 09:13 PM 45,163 README_appendix.md, 11/28/2020 12:35 PM 762 README_end.md, 11/28/2020 12:35 PM 30,249 README_OLD.md, 11/28/2020 12:35 PM 3,221 README_pre.md, 02/02/2021 09:13 PM 44,029 README_toc.md, 11/28/2020 12:35 PM &lt;DIR&gt; regnonlin, 11/28/2020 12:35 PM 595 style.css, 11/28/2020 12:35 PM &lt;DIR&gt; summarize, 11/28/2020 12:35 PM &lt;DIR&gt; support, 01/24/2021 11:16 AM &lt;DIR&gt; tabgraph, 11/28/2020 12:35 PM 136 title.Rmd, 11/28/2020 12:35 PM 2,570 toc.css, 02/02/2021 09:12 PM 3,660 _bookdown.yml, 01/24/2021 11:55 AM &lt;DIR&gt; _bookdown_files, 11/28/2020 12:35 PM 151 _config.yml, 11/28/2020 12:35 PM &lt;DIR&gt; _data, 11/28/2020 12:35 PM &lt;DIR&gt; _file, 11/28/2020 12:35 PM &lt;DIR&gt; _img, 11/28/2020 12:35 PM &lt;DIR&gt; _log, 11/28/2020 12:35 PM 1,437 _output.yml, 11/28/2020 12:35 PM 97 _output_kniti_html.yaml, 11/28/2020 12:35 PM 166 _output_kniti_pdf.yaml, 30 File(s) 1,006,513 bytes, 25 Dir(s) 442,112,376,832 bytes free&quot; 11.3.1.2 Run Python Inside a Conda Environment Use shell rather than system to activate a conda environment, check python version: # activate conda env print(toString(shell(paste0(&quot;activate base &amp; python --version&quot;), intern=TRUE))) ## [1] &quot;, G:\\\\repos\\\\R4Econ&gt;conda.bat activate base , Python 3.8.5&quot; Activate conda env and run a line: spg_runpython &lt;- paste0(&quot;activate base &amp;&quot;, &quot;python --version &amp;&quot;, &quot;python -c &quot;, &quot;\\&quot;st_var=&#39;this is string var&#39;;&quot;, &quot;print(f&#39;{st_var}&#39;);&quot;, &quot;\\&quot;&quot;) print(toString(shell(spg_runpython, intern=TRUE))) ## [1] &quot;, G:\\\\repos\\\\R4Econ&gt;conda.bat activate base , Python 3.8.5, this is string var&quot; "],["index-and-code-links.html", "A Index and Code Links A.1 Array, Matrix, Dataframe links A.2 Summarize Data links A.3 Functions links A.4 Multi-dimensional Data Structures links A.5 Linear Regression links A.6 Nonlinear and Other Regressions links A.7 Optimization links A.8 Mathmatics and Statistics links A.9 Tables and Graphs links A.10 Get Data links A.11 Code and Development links", " A Index and Code Links A.1 Array, Matrix, Dataframe links A.1.1 Section 1.1 List links Multi-dimensional Named Lists: rmd | r | pdf | html Initiate Empty List. Named one and two dimensional lists. List of Dataframes. Collapse named and unamed list to string and print input code. r: deparse(substitute()) + vector(mode = list, length = it_N) + names(list) &lt;- paste0(e,seq()) + dimnames(ls2d)[[1]] &lt;- paste0(r,seq()) + dimnames(ls2d)[[2]] &lt;- paste0(c,seq()) tidyr: unnest() A.1.2 Section 1.2 Array links Arrays Operations in R: rmd | r | pdf | html Basic array operations in R, rep, head, tail, na, etc. E notation. Get N cuts from M points. r: rep() + head() + tail() + na_if() + Re() Generate Special Arrays: rmd | r | pdf | html Generate special arrays: log spaced array r: seq() String Operations: rmd | r | pdf | html Split, concatenate, subset, replace, substring strings r: paste0() + sub() + gsub() + grepl() + sprintf() Meshgrid Matrices, Arrays and Scalars: rmd | r | pdf | html Meshgrid Matrices, Arrays and Scalars to form all combination dataframe. tidyr: expand_grid() + expand.grid() A.1.3 Section 1.3 Matrix links Matrix Basics: rmd | r | pdf | html Generate and combine NA, fixed and random matrixes. Name columns and rows. R: rep() + rbind() + matrix(NA) + matrix(NA_real_) + matrix(NA_integer_) + colnames() + rownames() Linear Algebra Operations: rmd | r | pdf | html A.1.4 Section 1.4 Variables in Dataframes links Tibble Basics: rmd | r | pdf | html generate tibbles, rename tibble variables, tibble row and column names rename numeric sequential columns with string prefix and suffix dplyr: as_tibble(mt) + rename_all(~c(ar_names)) + rename_at(vars(starts_with(xx)), funs(str_replace(., yy, yyyy)) + rename_at(vars(num_range(,ar_it)), funs(paste0(st,.))) + rowid_to_column() + colnames + rownames Label and Combine Factor Variables: rmd | r | pdf | html Convert numeric variables to factor variables, generate joint factors, and label factors. Graph MPG and 1/4 Miles Time (qsec) from the mtcars dataset over joint shift-type (am) and engine-type (vs) categories. forcats: as_factor() + fct_recode() + fct_cross() Randomly Draw Subsets of Rows from Matrix: rmd | r | pdf | html Given matrix, randomly sample rows, or select if random value is below threshold. r: rnorm() + sample() + df[sample(dim(df)[1], it_M, replace=FALSE),] dplyr: case_when() + mutate(var = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(var == 1) Generate Variables Conditional on Other Variables: rmd | r | pdf | html Use case_when to generate elseif conditional variables: NA, approximate difference, etc. dplyr: case_when() + na_if() + mutate(var = na_if(case_when(rnorm(n())&lt; 0 ~ -99, TRUE ~ mpg), -99)) r: e-notation + all.equal() + isTRUE(all.equal(a,b,tol)) + is.na() + NA_real_ + NA_character_ + NA_integer_ R Tibble Dataframe String Manipulations: rmd | r | pdf | html There are multiple CEV files, each containing the same file structure but simulated with different parameters, gather a subset of columns from different files, and provide with correct attributes based on CSV file names. r: cbind(ls_st, ls_st) + as_tibble(mt_st) A.2 Summarize Data links A.2.1 Section 2.1 Counting Observation links Counting Basics: rmd | r | pdf | html uncount to generate panel skeleton from years in survey dplyr: uncount(yr_n) + group_by() + mutate(yr = row_number() + start_yr) A.2.2 Section 2.2 Sorting, Indexing, Slicing links Sorted Index, Interval Index and Expand Value from One Row: rmd | r | pdf | html Sort and generate index for rows Generate negative and positive index based on deviations Populate Values from one row to other rows dplyr: arrange() + row_number() + mutate(lowest = min(Sepal.Length)) + case_when(row_number()==x ~ Septal.Length) + mutate(Sepal.New = Sepal.Length[Sepal.Index == 1]) Group and sort, and Slice and Summarize: rmd | r | pdf | html Group a dataframe by a variable, sort within group by another variable, keep only highest rows. dplyr: arrange() + group_by() + slice_head(n=1) A.2.3 Section 2.3 Group Statistics links Cummean Test, Cumulative Mean within Group: rmd | r | pdf | html There is a dataframe with a grouping variable and some statistics sorted by another within group variable, calculate the cumulative mean of that variable. dplyr: cummean() + group_by(id, isna = is.na(val)) + mutate(val_cummean = ifelse(isna, NA, cummean(val))) Count Unique Groups and Mean within Groups: rmd | r | pdf | html Unique groups defined by multiple values and count obs within group. Mean, sd, observation count for non-NA within unique groups. dplyr: group_by() + summarise(n()) + summarise_if(is.numeric, funs(mean = mean(., na.rm = TRUE), n = sum(is.na(.)==0))) By Groups, One Variable All Statistics: rmd | r | pdf | html Pick stats, overall, and by multiple groups, stats as matrix or wide row with name=(ctsvar + catevar + catelabel). tidyr: group_by() + summarize_at(, funs()) + rename(!!var := !!sym(var)) + mutate(!!var := paste0(var,str,!syms(vars))) + gather() + unite() + spread(varcates, value) By within Individual Groups Variables, Averages: rmd | r | pdf | html By Multiple within Individual Groups Variables. Averages for all numeric variables within all groups of all group variables. Long to Wide to very Wide. tidyr: *gather() + group_by() + summarise_if(is.numeric, funs(mean(., na.rm = TRUE))) + mutate(all_m_cate = paste0(variable, _c, value)) + unite() + spread()* A.2.4 Section 2.4 Distributional Statistics links Tibble Basics: rmd | r | pdf | html input multiple variables with comma separated text strings quantitative/continuous and categorical/discrete variables histogram and summary statistics tibble: ar_one &lt;- c(107.72,101.28) + ar_two &lt;- c(101.72,101.28) + mt_data &lt;- cbind(ar_one, ar_two) + as_tibble(mt_data) A.2.5 Section 2.5 Summarize Multiple Variables links Apply the Same Function over Columns of Matrix: rmd | r | pdf | html Replace NA values in selected columns by alternative values. Cumulative sum over multiple variables. Rename various various with common prefix and suffix appended. r: cumsum() + gsub() + mutate_at(vars(contains(V)), .funs = list(cumu = ~cumsum(.))) + rename_at(vars(contains(V) ), list(~gsub(M, \"\", .))) dplyr: rename_at() + mutate_at() + rename_at(vars(starts_with(V)), funs(str_replace(., V, var))) + mutate_at(vars(one_of(c(var1, var2))), list(~replace_na(., 99))) A.3 Functions links A.3.1 Section 3.1 Dataframe Mutate links Nonlinear Function of Scalars and Arrays over Rows: rmd | r | pdf | html Five methods to evaluate scalar nonlinear function over matrix. Evaluate non-linear function with scalar from rows and arrays as constants. r: .\\(fl_A + fl_A=\\)`(., fl_A) + .[[svr_fl_A]] dplyr: rowwise() + mutate(out = funct(inputs)) Evaluate Functions over Rows of Meshes Matrices: rmd | r | pdf | html Mesh states and choices together and rowwise evaluate many matrixes. Cumulative sum over multiple variables. Rename various various with common prefix and suffix appended. r: ffi &lt;- function(fl_A, ar_B) tidyr: expand_grid() + rowwise() + df %&gt;% rowwise() %&gt;% mutate(var = ffi(fl_A, ar_B)) ggplot2: geom_line() + facet_wrap() + geom_hline() + facet_wrap(. ~ var_id, scales = free) + geom_hline(yintercept=0, linetype=dashed, color=red, size=1) + A.3.2 Section 3.2 Dataframe Do Anything links Dataframe Row to Array (Mx1 by N) to (MxQ by N+1): rmd | r | pdf | html Generate row value specific arrays of varying Length, and stack expanded dataframe. Given row-specific information, generate row-specific arrays that expand matrix. dplyr: do() + unnest() + left_join() + df %&gt;% group_by(ID) %&gt;% do(inc = rnorm(.\\(Q, mean=.\\)mean, sd=.$sd)) %&gt;% unnest(c(inc)) Dataframe Subset to Scalar (MxP by N) to (Mx1 by 1): rmd | r | pdf | html MxQ rows to Mx1 Rows. Group dataframe by categories, compute category specific output scalar or arrays based on within category variable information. dplyr: group_by(ID) + do(inc = rnorm(.\\(N, mean=.\\)mn, sd=.$sd)) + unnest(c(inc)) + left_join(df, by=ID) Dataframe Subset to Dataframe (MxP by N) to (MxQ by N+Z-1): rmd | r | pdf | html Group by mini dataframes as inputs for function. Stack output dataframes with group id. dplyr: group_by() + do() + unnest() A.3.3 Section 3.3 Apply and pmap links Apply and Sapply function over arrays and rows: rmd | r | pdf | html Evaluate function f(x_i,y_i,c), where c is a constant and x and y vary over each row of a matrix, with index i indicating rows. Get same results using apply and sapply with defined and anonymous functions. r: do.call() + apply(mt, 1, func) + sapply(ls_ar, func, ar1, ar2) Mutate rowwise, mutate pmap, and rowwise do unnest: rmd | r | pdf | html Evaluate function f(x_i,y_i,c), where c is a constant and x and y vary over each row of a matrix, with index i indicating rows. Get same results using various types of mutate rowwise, mutate pmap and rowwise do unnest. dplyr: rowwise() + do() + unnest() purrr: pmap(func) tidyr: unlist() A.4 Multi-dimensional Data Structures links A.4.1 Section 4.1 Generate, Gather, Bind and Join links R dplyr Group by Index and Generate Panel Data Structure: rmd | r | pdf | html Build skeleton panel frame with N observations and T periods with gender and height. Generate group Index based on a list of grouping variables. r: runif() + rnorm() + rbinom(n(), 1, 0.5) + cumsum() dplyr: group_by() + row_number() + ungroup() + one_of() + mutate(var = (row_number()==1)1)* tidyr: uncount() R DPLYR Join Multiple Dataframes Together: rmd | r | pdf | html Join dataframes together with one or multiple keys. Stack dataframes together. dplyr: filter() + rename(!!sym(vsta) := !!sym(vstb)) + mutate(var = rnom(n())) + left_join(df, by=(c(id=id, vt=vt))) + left_join(df, by=setNames(c(id, vt), c(id, vt))) + bind_rows() R Gather Data Columns from Multiple CSV Files: rmd | r | pdf | html There are multiple CEV files, each containing the same file structure but simulated with different parameters, gather a subset of columns from different files, and provide with correct attributes based on CSV file names. Separate numeric and string components of a string variable value apart. r: file() + writeLines() + readLines() + close() + gsub() + read.csv() + do.call(bind_rows, ls_df) + apply() tidyr: separate() regex: (?&lt;=[A-Za-z])(?=[-0-9]) A.4.2 Section 4.2 Wide and Long links TIDYR Pivot Wider and Pivot Longer Examples: rmd | r | pdf | html Long roster to wide roster and cumulative sum attendance by date. dplyr: mutate(var = case_when(rnorm(n()) &lt; 0 ~ 1, TRUE ~ 0)) + rename_at(vars(num_range(, ar_it)), list(~paste0(st_prefix, . ,))) + mutate_at(vars(contains(str)), list(~replace_na(., 0))) + mutate_at(vars(contains(str)), list(~cumsum(.))) R Wide Data to Long Data Example (TIDYR Pivot Longer): rmd | r | pdf | html A matrix of ev given states, rows are states and cols are shocks. Convert to Long table with shock and state values and ev. dplyr: left_join() + pivot_longer(cols = starts_with(zi), names_to = c(zi), names_pattern = paste0(zi(.)), values_to = ev) A.4.3 Section 4.3 Join and Compare links Find Closest Values Along Grids: rmd | r | pdf | html There is an array (matrix) of values, find the index of the values closest to another value. r: do.call(bind_rows, ls_df) dplyr: left_join(tb, by=(c(vr_a=vr_a, vr_b=vr_b))) A.5 Linear Regression links A.5.1 Section 5.1 OLS and IV links IV/OLS Regression: rmd | r | pdf | html R Instrumental Variables and Ordinary Least Square Regression store all Coefficients and Diagnostics as Dataframe Row. aer: library(aer) + ivreg(as.formula, diagnostics = TRUE) M Outcomes and N RHS Alternatives: rmd | r | pdf | html There are M outcome variables and N alternative explanatory variables. Regress all M outcome variables on N endogenous/independent right hand side variables one by one, with controls and/or IVs, collect coefficients. dplyr: bind_rows(lapply(listx, function(x)(bind_rows(lapply(listy, regf.iv))) + starts_with() + ends_with() + reduce(full_join) A.5.2 Section 5.2 Decomposition links Regression Decomposition: rmd | r | pdf | html Post multiple regressions, fraction of outcome variables variances explained by multiple subsets of right hand side variables. dplyr: gather() + group_by(var) + mutate_at(vars, funs(mean = mean(.))) + rowSums(matmat) + mutate_if(is.numeric, funs(frac = (./value_var)))* A.6 Nonlinear and Other Regressions links A.6.1 Section 6.1 Logit Regression links Logit Regression: rmd | r | pdf | html Logit regression testing and prediction. stats: glm(as.formula(), data, family=binomial) + predict(rs, newdata, type = response) A.6.2 Section 6.2 Quantile Regression links Quantile Regressions with Quantreg: rmd | r | pdf | html Quantile regression with continuous outcomes. Estimates and tests quantile coefficients. quantreg: rq(mpg ~ disp + hp + factor(am), tau = c(0.25, 0.50, 0.75), data = mtcars) + anova(rq(), test = Wald, joint=TRUE) + anova(rq(), test = Wald, joint=FALSE) A.7 Optimization links A.7.1 Section 7.1 Bisection links Concurrent Bisection over Dataframe Rows: rmd | r | pdf | html Post multiple regressions, fraction of outcome variables variances explained by multiple subsets of right hand side variables. tidyr: *pivot_longer(cols = starts_with(abc), names_to = c(a, b), names_pattern = paste0(prefix, \"(.)_(.)\"), values_to = val) + pivot_wider(names_from = !!sym(name), values_from = val) + mutate(!!sym(abc) := case_when(efg &lt; 0 ~ !!sym(opq), TRUE ~ iso))* gglot2: geom_line() + facet_wrap() + geom_hline() A.8 Mathmatics and Statistics links A.8.1 Section 8.1 Distributions links Integrate Normal Shocks: rmd | r | pdf | html Random Sampling (Monte Carlo) integrate shocks. Trapezoidal rule (symmetric rectangles) integrate normal shock. A.8.2 Section 8.2 Analytical Solutions links linear solve x with f(x) = 0: rmd | r | pdf | html Evaluate and solve statistically relevant problems with one equation and one unknown that permit analytical solutions. A.8.3 Section 8.3 Inequality Models links Gini for Discrete Samples: rmd | r | pdf | html Given sample of data points that are discrete, compute the approximate gini coefficient. r: sort() + cumsum() + sum() CES and Atkinson Utility: rmd | r | pdf | html Analyze how changing individual outcomes shift utility given inequality preference parameters. Draw Cobb-Douglas, Utilitarian and Leontief indifference curve r: apply(mt, 1, funct(x){}) + do.call(rbind, ls_mt) tidyr: expand_grid() ggplot2: geom_line() + facet_wrap() econ: Atkinson (JET, 1970) Inequality in Environmental Exposure Across Population Groups: rmd | r | pdf | html Simulate population distribution by location and demographic groups. Simulate pollution exposures by location. Compute inequality in environmental exposure across population groups, given location-specific environmental data and location-specific population information. r: matrix() stats: runif() + sum() dplyr: arrange() + group_by() + left_join() + filter() + slice() A.9 Tables and Graphs links A.9.1 Section 9.1 R Base Plots links R Base Plot Line with Curves and Scatter: rmd | r | pdf | html Plot scatter points, line plot and functional curve graphs together. Set margins for legend to be outside of graph area, change line, point, label and legend sizes. Generate additional lines for plots successively, record successively, and plot all steps, or initial steps results. r: plot() + curve() + legend() + title() + axis() + par() + recordPlot() A.9.2 Section 9.2 GGplot Line Related Plots links GGplot Multiple Categorical Variables With Continuous Variable: rmd | r | pdf | html One category is subplot, one category is line-color, one category is line-type. ggplot: ggplot() + facet_wrap() + geom_smooth() + geom_hline() + scale_colour_manual() + scale_shape_discrete() + scale_linetype_manual() + scale_x_continuous() + scale_y_continuous() + theme_bw() + theme() A.9.3 Section 9.3 Write and Read Plots links Base R Save Images At Different Sizes: rmd | r | pdf | html Base R store image core, add legends/titles/labels/axis of different sizes to save figures of different sizes. r: png() + setEPS() + postscript() + dev.off() A.10 Get Data links A.10.1 Section 10.1 Environmental Data links CDS ECMWF Global Enviornmental Data Download: rmd | r | pdf | html Using Python API get get ECMWF ERA5 data. Dynamically modify a python API file, run python inside a Conda virtual environment with R-reticulate. r: file() + writeLines() + unzip() + list.files() + unlink() r-reticulate: use_python() + Sys.setenv(RETICULATE_PYTHON = spth_conda_env) A.11 Code and Development links A.11.1 Section 11.1 Files In and Out links Decompose File Paths to Get Folder and Files Names: rmd | r | pdf | html Decompose file path and get file path folder names and file name. r: .Platform$file.sep + tail() + strsplit() + basename() + dirname() + substring() Save Text to File, Read Text from File, Replace Text in File: rmd | r | pdf | html Save data to file, read text from file, replace text in file. r: kable() + file() + writeLines() + readLines() + close() + gsub() Convert R Markdown File to R, PDF and HTML: rmd | r | pdf | html Find all files in a folder with a particula suffix, with exclusion. Convert R Markdow File to R, PDF and HTML. Modify markdown pounds hierarchy. r: file() + writeLines() + readLines() + close() + gsub() A.11.2 Section 11.2 Python with R links Python in R with Reticulate: rmd | r | pdf | html Use Python in R with Reticulate reticulate: py_config() + use_condaenv() + py_run_string() + Sys.which(python) A.11.3 Section 11.3 Command Line links System and Shell Commands in R: rmd | r | pdf | html Run system executable and shell commands. Activate conda environment with shell script. r: system() + shell() R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Wang, Fan. 2020. REconTools: R Tools for Panel Data and Optimization. https://fanwangecon.github.io/REconTools/. Wickham, Hadley. 2019. Tidyverse: Easily Install and Load the tidyverse. https://CRAN.R-project.org/package=tidyverse. Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown. "]]
