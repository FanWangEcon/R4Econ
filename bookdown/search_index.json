[
["index.html", "Panel Data and Optimization with R Preface", " Panel Data and Optimization with R Fan Wang 2020-04-12 Preface This is a work-in-progress website consisting of R panel data and optimization examples for Statistics/Econometrics/Economic Analysis. Materials gathered from various projects in which R code is used. Files are from Fan’s R4Econ repository. This is not a R package, but a list of examples in PDF/HTML/Rmd formats. REconTools is a package that can be installed with tools used in projects involving R. Bullet points show which base R, tidyverse or other functions/commands are used to achieve various objectives. An effort is made to use only base R (R Core Team 2019) and tidyverse (Wickham 2019) packages whenever possible to reduce dependencies. The goal of this repository is to make it easier to find/re-use codes produced for various projects. Some functions also rely on or correspond to functions from REconTools (Wang 2020). From Fan’s other repositories: For dynamic borrowing and savings problems, see Dynamic Asset Repository; For code examples, see also Matlab Example Code and Stata Example Code; For intro econ with Matlab, see Intro Mathematics for Economists, and for intro stat with R, see Intro Statistics for Undergraduates. See here for all of Fan’s public repositories. The site is built using Bookdown (Xie 2020). Please contact FanWangEcon for issues or problems. References "],
["array-matrix-dataframe.html", "Chapter 1 Array, Matrix, Dataframe 1.1 List 1.2 Array 1.3 Matrix 1.4 Dataframes (Tibble)", " Chapter 1 Array, Matrix, Dataframe 1.1 List 1.1.1 Multiple Dimensional List Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r list tutorial r vector vs list r initialize empty multiple element list r name rows and columns of 2 dimensional list r row and colum names of list list dimnames 1.1.1.1 One Dimensional Named List define list slice list # Define Lists ls_num &lt;- list(1,2,3) ls_str &lt;- list(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;) ls_num_str &lt;- list(1,2,&#39;3&#39;) # Named Lists ar_st_names &lt;- c(&#39;e1&#39;,&#39;e2&#39;,&#39;e3&#39;) ls_num_str_named &lt;- ls_num_str names(ls_num_str_named) &lt;- ar_st_names # Add Element to Named List ls_num_str_named$e4 &lt;- &#39;this is added&#39; # display print(paste0(&#39;ls_num:&#39;, str(ls_num))) ## List of 3 ## $ : num 1 ## $ : num 2 ## $ : num 3 ## [1] &quot;ls_num:&quot; print(paste0(&#39;ls_num[2:3]:&#39;, str(ls_num[2:3]))) ## List of 2 ## $ : num 2 ## $ : num 3 ## [1] &quot;ls_num[2:3]:&quot; print(paste0(&#39;ls_str:&#39;, str(ls_str))) ## List of 3 ## $ : chr &quot;1&quot; ## $ : chr &quot;2&quot; ## $ : chr &quot;3&quot; ## [1] &quot;ls_str:&quot; print(paste0(&#39;ls_str[2:3]:&#39;, str(ls_str[2:3]))) ## List of 2 ## $ : chr &quot;2&quot; ## $ : chr &quot;3&quot; ## [1] &quot;ls_str[2:3]:&quot; print(paste0(&#39;ls_num_str:&#39;, str(ls_num_str))) ## List of 3 ## $ : num 1 ## $ : num 2 ## $ : chr &quot;3&quot; ## [1] &quot;ls_num_str:&quot; print(paste0(&#39;ls_num_str[2:4]:&#39;, str(ls_num_str[2:4]))) ## List of 3 ## $ : num 2 ## $ : chr &quot;3&quot; ## $ : NULL ## [1] &quot;ls_num_str[2:4]:&quot; print(paste0(&#39;ls_num_str_named:&#39;, str(ls_num_str_named))) ## List of 4 ## $ e1: num 1 ## $ e2: num 2 ## $ e3: chr &quot;3&quot; ## $ e4: chr &quot;this is added&quot; ## [1] &quot;ls_num_str_named:&quot; print(paste0(&#39;ls_num_str_named[c(\\&#39;e2\\&#39;,\\&#39;e3\\&#39;,\\&#39;e4\\&#39;)]&#39;, str(ls_num_str_named[c(&#39;e2&#39;,&#39;e3&#39;,&#39;e4&#39;)]))) ## List of 3 ## $ e2: num 2 ## $ e3: chr &quot;3&quot; ## $ e4: chr &quot;this is added&quot; ## [1] &quot;ls_num_str_named[c(&#39;e2&#39;,&#39;e3&#39;,&#39;e4&#39;)]&quot; 1.1.1.2 Two Dimensional Unnamed List Generate a multiple dimensional list: Initiate with an N element empty list Reshape list to M by Q Fill list elements Get list element by row and column number List allows for different data types to be stored together. Note that element specific names in named list are not preserved when the list is reshaped to be two dimensional. Two dimensional list, however, could have row and column names. # Dimensions it_M &lt;- 2 it_Q &lt;- 3 it_N &lt;- it_M*it_Q # Initiate an Empty MxQ=N element list ls_2d_flat &lt;- vector(mode = &quot;list&quot;, length = it_N) ls_2d &lt;- ls_2d_flat # Named flat ls_2d_flat_named &lt;- ls_2d_flat names(ls_2d_flat_named) &lt;- paste0(&#39;e&#39;,seq(1,it_N)) ls_2d_named &lt;- ls_2d_flat_named # Reshape dim(ls_2d) &lt;- c(it_M, it_Q) # named 2d list can not carry 1d name after reshape dim(ls_2d_named) &lt;- c(it_M, it_Q) # display print(&#39;ls_2d_flat&#39;) ## [1] &quot;ls_2d_flat&quot; print(ls_2d_flat) ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL print(&#39;ls_2d_flat_named&#39;) ## [1] &quot;ls_2d_flat_named&quot; print(ls_2d_flat_named) ## $e1 ## NULL ## ## $e2 ## NULL ## ## $e3 ## NULL ## ## $e4 ## NULL ## ## $e5 ## NULL ## ## $e6 ## NULL print(&#39;ls_2d&#39;) ## [1] &quot;ls_2d&quot; print(ls_2d) ## [,1] [,2] [,3] ## [1,] NULL NULL NULL ## [2,] NULL NULL NULL print(&#39;ls_2d_named&#39;) ## [1] &quot;ls_2d_named&quot; print(ls_2d_named) ## [,1] [,2] [,3] ## [1,] NULL NULL NULL ## [2,] NULL NULL NULL # Select Values, double bracket to select from 2dim list print(&#39;ls_2d[[1,2]]&#39;) ## [1] &quot;ls_2d[[1,2]]&quot; print(ls_2d[[1,2]]) ## NULL 1.1.1.3 Define Two Dimensional Named LIst For naming two dimensional lists, rowname and colname does not work. Rather, we need to use dimnames. Note that in addition to dimnames, we can continue to have element specific names. Both can co-exist. But note that the element specific names are not preserved after dimension transform, so need to be redefined afterwards. How to select an element of a two dimensional list: row and column names: dimnames, ls_2d_flat_named[[‘row2’,‘col2’]] named elements: names, ls_2d_flat_named[[‘e5’]] select by index: index, ls_2d_flat_named[[5]] Neither dimnames nor names are required, but both can be used to select elements. # Dimensions it_M &lt;- 3 it_Q &lt;- 4 it_N &lt;- it_M*it_Q # Initiate an Empty MxQ=N element list ls_2d_flat_named &lt;- vector(mode = &quot;list&quot;, length = it_N) dim(ls_2d_flat_named) &lt;- c(it_M, it_Q) # Fill with values for (it_Q_ctr in seq(1,it_Q)) { for (it_M_ctr in seq(1,it_M)) { # linear index ls_2d_flat_named[[it_M_ctr, it_Q_ctr]] &lt;- (it_Q_ctr-1)*it_M+it_M_ctr } } # Replace row names, note rownames does not work dimnames(ls_2d_flat_named)[[1]] &lt;- paste0(&#39;row&#39;,seq(1,it_M)) dimnames(ls_2d_flat_named)[[2]] &lt;- paste0(&#39;col&#39;,seq(1,it_Q)) # Element Specific Names names(ls_2d_flat_named) &lt;- paste0(&#39;e&#39;,seq(1,it_N)) # These are not element names, can still name each element # display print(&#39;ls_2d_flat_named&#39;) ## [1] &quot;ls_2d_flat_named&quot; print(ls_2d_flat_named) ## col1 col2 col3 col4 ## row1 1 4 7 10 ## row2 2 5 8 11 ## row3 3 6 9 12 ## attr(,&quot;names&quot;) ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; &quot;e6&quot; &quot;e7&quot; &quot;e8&quot; &quot;e9&quot; &quot;e10&quot; &quot;e11&quot; &quot;e12&quot; print(&#39;str(ls_2d_flat_named)&#39;) ## [1] &quot;str(ls_2d_flat_named)&quot; print(str(ls_2d_flat_named)) ## List of 12 ## $ e1 : num 1 ## $ e2 : num 2 ## $ e3 : num 3 ## $ e4 : num 4 ## $ e5 : num 5 ## $ e6 : num 6 ## $ e7 : num 7 ## $ e8 : num 8 ## $ e9 : num 9 ## $ e10: num 10 ## $ e11: num 11 ## $ e12: num 12 ## - attr(*, &quot;dim&quot;)= int [1:2] 3 4 ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:3] &quot;row1&quot; &quot;row2&quot; &quot;row3&quot; ## ..$ : chr [1:4] &quot;col1&quot; &quot;col2&quot; &quot;col3&quot; &quot;col4&quot; ## NULL # Select elements with with dimnames print(&#39;ls_2d_flat_named[[\\&#39;row2\\&#39;,\\&#39;col2\\&#39;]]&#39;) ## [1] &quot;ls_2d_flat_named[[&#39;row2&#39;,&#39;col2&#39;]]&quot; print(ls_2d_flat_named[[&#39;row2&#39;,&#39;col2&#39;]]) ## [1] 5 # Select elements with element names print(&#39;ls_2d_flat_named[[\\&#39;e5\\&#39;]]&#39;) ## [1] &quot;ls_2d_flat_named[[&#39;e5&#39;]]&quot; print(ls_2d_flat_named[[&#39;e5&#39;]]) ## [1] 5 # Select elements with index print(&#39;ls_2d_flat_named[[5]]&#39;) ## [1] &quot;ls_2d_flat_named[[5]]&quot; print(ls_2d_flat_named[[5]]) ## [1] 5 1.2 Array 1.2.1 Array Basics Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.2.1.1 Multidimesional Arrays 1.2.1.1.1 Generate 2 Dimensional Array # Multidimensional Array # 1 is r1c1t1, 1.5 in r2c1t1, 0 in r1c2t1, etc. # Three dimensions, row first, column second, and tensor third x &lt;- array(c(1, 1.5, 0, 2, 0, 4, 0, 3), dim=c(2, 2, 2)) dim(x) ## [1] 2 2 2 print(x) ## , , 1 ## ## [,1] [,2] ## [1,] 1.0 0 ## [2,] 1.5 2 ## ## , , 2 ## ## [,1] [,2] ## [1,] 0 0 ## [2,] 4 3 1.2.1.2 Array Slicing 1.2.1.2.1 Remove Elements of Array # Remove last element of array vars.group.bydf &lt;- c(&#39;23&#39;,&#39;dfa&#39;, &#39;wer&#39;) vars.group.bydf[-length(vars.group.bydf)] ## [1] &quot;23&quot; &quot;dfa&quot; 1.2.1.3 NA in Array 1.2.1.3.1 Check if NA is in Array # Convert Inf and -Inf to NA x &lt;- c(1, -1, Inf, 10, -Inf) na_if(na_if(x, -Inf), Inf) ## [1] 1 -1 NA 10 NA 1.2.2 Generate Arrays Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.2.2.1 Generate Special Arrays 1.2.2.1.1 Log Space Arrays Often need to generate arrays on log rather than linear scale, below is log 10 scaled grid. # Parameters it.lower.bd.inc.cnt &lt;- 3 fl.log.lower &lt;- -10 fl.log.higher &lt;- -9 fl.min.rescale &lt;- 0.01 it.log.count &lt;- 4 # Generate ar.fl.log.rescaled &lt;- exp(log(10)*seq(log10(fl.min.rescale), log10(fl.min.rescale + (fl.log.higher-fl.log.lower)), length.out=it.log.count)) ar.fl.log &lt;- ar.fl.log.rescaled + fl.log.lower - fl.min.rescale # Print ar.fl.log ## [1] -10.000000 -9.963430 -9.793123 -9.000000 1.2.3 String Arrays Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.2.3.1 String Replace # String replacement gsub(x = paste0(unique(df.slds.stats.perc$it.inner.counter), &#39;:&#39;, unique(df.slds.stats.perc$z_n_a_n), collapse = &#39;;&#39;), pattern = &quot;\\n&quot;, replacement = &quot;&quot;) gsub(x = var, pattern = &quot;\\n&quot;, replacement = &quot;&quot;) gsub(x = var.input, pattern = &quot;\\\\.&quot;, replacement = &quot;_&quot;) 1.2.3.1.1 String Contains r if string contains st_example_a &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; st_example_b &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/_main.html&#39; grepl(&#39;_main&#39;, st_example_a) ## [1] FALSE grepl(&#39;_main&#39;, st_example_b) ## [1] TRUE 1.2.3.2 String Concatenate # Simple Collapse vars.group.by &lt;- c(&#39;abc&#39;, &#39;efg&#39;) paste0(vars.group.by, collapse=&#39;|&#39;) ## [1] &quot;abc|efg&quot; 1.2.3.3 String Add Leading Zero # Add Leading zero for integer values to allow for sorting when # integers are combined into strings it_z_n &lt;- 1 it_a_n &lt;- 192 print(sprintf(&quot;%02d&quot;, it_z_n)) ## [1] &quot;01&quot; print(sprintf(&quot;%04d&quot;, it_a_n)) ## [1] &quot;0192&quot; 1.2.3.4 Substring and File Name From path, get file name without suffix. r string split r list last element r get file name from path r get file path no name st_example &lt;- &#39;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics.Rmd&#39; st_file_wth_suffix &lt;- tail(strsplit(st_example, &quot;/&quot;)[[1]],n=1) st_file_wno_suffix &lt;- sub(&#39;\\\\.Rmd$&#39;, &#39;&#39;, basename(st_example)) st_fullpath_nosufx &lt;- sub(&#39;\\\\.Rmd$&#39;, &#39;&#39;, st_example) st_lastpath_noname &lt;- basename(dirname(st_example)) st_fullpath_noname &lt;- dirname(st_example) print(strsplit(st_example, &quot;/&quot;)) ## [[1]] ## [1] &quot;C:&quot; &quot;Users&quot; &quot;fan&quot; &quot;R4Econ&quot; &quot;amto&quot; &quot;tibble&quot; &quot;fs_tib_basics.Rmd&quot; print(st_file_wth_suffix) ## [1] &quot;fs_tib_basics.Rmd&quot; print(st_file_wno_suffix) ## [1] &quot;fs_tib_basics&quot; print(st_fullpath_nosufx) ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble/fs_tib_basics&quot; print(st_lastpath_noname) ## [1] &quot;tibble&quot; print(st_fullpath_noname) ## [1] &quot;C:/Users/fan/R4Econ/amto/tibble&quot; 1.2.4 Mesh Matrix and Vector Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r expand.grid meshed array to matrix r meshgrid r array to matrix r reshape array to matrix dplyr permuations rows of matrix and element of array tidyr expand_grid mesh matrix and vector In the example below, we have a matrix that is 5 by 2, and a vector that is 1 by 3. We want to generate a tibble dataset that meshes the matrix and the vector, so that all combinations show up. Note expand_grid is a from tidyr 1.0.0. # it_child_count = N, the number of children it_N_child_cnt = 5 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) mt_nP_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # Choice Grid it_N_choice_cnt = 3 fl_max = 10 fl_min = 0 ar_nN_alpha = seq(fl_min, fl_max, length.out = it_N_choice_cnt) # expand grid with dplyr expand_grid(x = 1:3, y = 1:2) ## # A tibble: 6 x 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 1 1 ## 2 1 2 ## 3 2 1 ## 4 2 2 ## 5 3 1 ## 6 3 2 tb_expanded &lt;- as_tibble(mt_nP_A_alpha) %&gt;% expand_grid(choices = ar_nN_alpha) # display kable(tb_expanded) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha choices -2 0.1 0 -2 0.1 5 -2 0.1 10 -1 0.3 0 -1 0.3 5 -1 0.3 10 0 0.5 0 0 0.5 5 0 0.5 10 1 0.7 0 1 0.7 5 1 0.7 10 2 0.9 0 2 0.9 5 2 0.9 10 1.2.4.1 Define Two Arrays and Mesh Them using expand.grid Given two arrays, mesh the two arrays together. # use expand.grid to generate all combinations of two arrays it_ar_A = 5 it_ar_alpha = 10 ar_A = seq(-2, 2, length.out=it_ar_A) ar_alpha = seq(0.1, 0.9, length.out=it_ar_alpha) mt_A_alpha = expand.grid(A = ar_A, alpha = ar_alpha) mt_A_meshed = mt_A_alpha[,1] dim(mt_A_meshed) = c(it_ar_A, it_ar_alpha) mt_alpha_meshed = mt_A_alpha[,2] dim(mt_alpha_meshed) = c(it_ar_A, it_ar_alpha) # display kable(mt_A_meshed) %&gt;% kable_styling_fc() -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 kable(mt_alpha_meshed) %&gt;% kable_styling_fc_wide() 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 0.1 0.1888889 0.2777778 0.3666667 0.4555556 0.5444444 0.6333333 0.7222222 0.8111111 0.9 1.2.4.2 Two Identical Arrays, Mesh to Generate Square using expand.grid Two Identical Arrays, individual attributes, each column is an individual for a matrix, and each row is also an individual # use expand.grid to generate all combinations of two arrays it_ar_A = 5 ar_A = seq(-2, 2, length.out=it_ar_A) mt_A_A = expand.grid(Arow = ar_A, Arow = ar_A) mt_Arow = mt_A_A[,1] dim(mt_Arow) = c(it_ar_A, it_ar_A) mt_Acol = mt_A_A[,2] dim(mt_Acol) = c(it_ar_A, it_ar_A) # display kable(mt_Arow) %&gt;% kable_styling_fc() -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 kable(mt_Acol) %&gt;% kable_styling_fc() -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 -2 -1 0 1 2 1.3 Matrix 1.3.1 Generate Matrixes Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.3.1.1 Create a N by 2 Matrix from 3 arrays Names of each array become row names automatically. ar_row_one &lt;- c(-1,+1) ar_row_two &lt;- c(-3,-2) ar_row_three &lt;- c(0.35,0.75) mt_n_by_2 &lt;- rbind(ar_row_one, ar_row_two, ar_row_three) kable(mt_n_by_2) %&gt;% kable_styling_fc() ar_row_one -1.00 1.00 ar_row_two -3.00 -2.00 ar_row_three 0.35 0.75 1.3.1.2 Generate Random Matrixes Random draw from the normal distribution, random draw from the uniform distribution, and combine resulting matrixes. # Generate 15 random normal, put in 5 rows, and 3 columns mt_rnorm &lt;- matrix(rnorm(15,mean=0,sd=1), nrow=5, ncol=3) # Generate 15 random normal, put in 5 rows, and 3 columns mt_runif &lt;- matrix(runif(15,min=0,max=1), nrow=5, ncol=5) # Combine mt_rnorm_runif &lt;- cbind(mt_rnorm, mt_runif) # Display kable(mt_rnorm_runif) %&gt;% kable_styling_fc_wide() -1.1858745 0.7264546 -2.1613182 0.2068418 0.9547658 0.6578097 0.2068418 0.9547658 -2.0055130 0.7136567 0.3952199 0.1146044 0.4543614 0.1698893 0.1146044 0.4543614 0.0075099 -0.6500629 -0.3948340 0.7504459 0.1925193 0.7443364 0.7504459 0.1925193 0.5194904 1.4986962 -0.3097584 0.9334095 0.4198546 0.0552954 0.9334095 0.4198546 -0.7462955 -1.4358281 1.3308266 0.4146961 0.1078679 0.5422845 0.4146961 0.1078679 1.3.2 Linear Algebra Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.3.2.1 Matrix Multiplication Multiply Together a 3 by 2 matrix and a 2 by 1 vector ar_row_one &lt;- c(-1,+1) ar_row_two &lt;- c(-3,-2) ar_row_three &lt;- c(0.35,0.75) mt_n_by_2 &lt;- rbind(ar_row_one, ar_row_two, ar_row_three) ar_row_four &lt;- c(3,4) # Matrix Multiplication mt_out &lt;- mt_n_by_2 %*% ar_row_four print(mt_n_by_2) ## [,1] [,2] ## ar_row_one -1.00 1.00 ## ar_row_two -3.00 -2.00 ## ar_row_three 0.35 0.75 print(ar_row_four) ## [1] 3 4 print(mt_out) ## [,1] ## ar_row_one 1.00 ## ar_row_two -17.00 ## ar_row_three 4.05 1.4 Dataframes (Tibble) 1.4.1 Generate Dataframe Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.1.1 Generate Tibble given Matrixes and Arrays Given Arrays and Matrixes, Generate Tibble and Name Variables/Columns naming tibble columns tibble variable names dplyr rename tibble dplyr rename tibble all variables dplyr rename all columns by index dplyr tibble add index column see also: SO-51205520 # Base Inputs ar_col &lt;- c(-1,+1) mt_rnorm_a &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=2, ncol=2) mt_rnorm_b &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=2, ncol=4) # Combine Matrix mt_combine &lt;- cbind(ar_col, mt_rnorm_a, mt_rnorm_b) colnames(mt_combine) &lt;- c(&#39;ar_col&#39;, paste0(&#39;matcolvar_grpa_&#39;, seq(1,dim(mt_rnorm_a)[2])), paste0(&#39;matcolvar_grpb_&#39;, seq(1,dim(mt_rnorm_b)[2]))) # Variable Names ar_st_varnames &lt;- c(&#39;var_one&#39;, paste0(&#39;tibcolvar_ga_&#39;, c(1,2)), paste0(&#39;tibcolvar_gb_&#39;, c(1,2,3,4))) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_combine) %&gt;% rename_all(~c(ar_st_varnames)) # Add an index column to the dataframe, ID column tb_combine &lt;- tb_combine %&gt;% rowid_to_column(var = &quot;ID&quot;) # Change all gb variable names tb_combine &lt;- tb_combine %&gt;% rename_at(vars(starts_with(&quot;tibcolvar_gb_&quot;)), funs(str_replace(., &quot;_gb_&quot;, &quot;_gbrenamed_&quot;))) # Tibble back to matrix mt_tb_combine_back &lt;- data.matrix(tb_combine) # Display kable(mt_combine) %&gt;% kable_styling_fc_wide() ar_col matcolvar_grpa_1 matcolvar_grpa_2 matcolvar_grpb_1 matcolvar_grpb_2 matcolvar_grpb_3 matcolvar_grpb_4 -1 -0.6015056 0.0209320 0.1754664 1.0928359 0.1754664 1.0928359 1 -2.4379080 0.7102217 1.0162244 -0.1119114 1.0162244 -0.1119114 kable(tb_combine) %&gt;% kable_styling_fc_wide() ID var_one tibcolvar_ga_1 tibcolvar_ga_2 tibcolvar_gbrenamed_1 tibcolvar_gbrenamed_2 tibcolvar_gbrenamed_3 tibcolvar_gbrenamed_4 1 -1 -0.6015056 0.0209320 0.1754664 1.0928359 0.1754664 1.0928359 2 1 -2.4379080 0.7102217 1.0162244 -0.1119114 1.0162244 -0.1119114 kable(mt_tb_combine_back) %&gt;% kable_styling_fc_wide() ID var_one tibcolvar_ga_1 tibcolvar_ga_2 tibcolvar_gbrenamed_1 tibcolvar_gbrenamed_2 tibcolvar_gbrenamed_3 tibcolvar_gbrenamed_4 1 -1 -0.6015056 0.0209320 0.1754664 1.0928359 0.1754664 1.0928359 2 1 -2.4379080 0.7102217 1.0162244 -0.1119114 1.0162244 -0.1119114 1.4.1.2 Rename Tibble with Numeric Column Names After reshaping, often could end up with variable names that are all numeric, intgers for example, how to rename these variables to add a common prefix for example. # Base Inputs ar_col &lt;- c(-1,+1) mt_rnorm_c &lt;- matrix(rnorm(4,mean=0,sd=1), nrow=5, ncol=10) ## Warning in matrix(rnorm(4, mean = 0, sd = 1), nrow = 5, ncol = 10): data length [4] is not a sub-multiple or multiple of the number of rows [5] mt_combine &lt;- cbind(ar_col, mt_rnorm_c) ## Warning in cbind(ar_col, mt_rnorm_c): number of rows of result is not a multiple of vector length (arg 1) # Variable Names ar_it_cols_ctr &lt;- seq(1, dim(mt_rnorm_c)[2]) ar_st_varnames &lt;- c(&#39;var_one&#39;, ar_it_cols_ctr) # Combine to tibble, add name col1, col2, etc. tb_combine &lt;- as_tibble(mt_combine) %&gt;% rename_all(~c(ar_st_varnames)) # Add an index column to the dataframe, ID column tb_combine_ori &lt;- tb_combine %&gt;% rowid_to_column(var = &quot;ID&quot;) # Change all gb variable names tb_combine &lt;- tb_combine_ori %&gt;% rename_at( vars(num_range(&#39;&#39;,ar_it_cols_ctr)), funs(paste0(&quot;rho&quot;, . , &#39;var&#39;)) ) # Display kable(tb_combine_ori) %&gt;% kable_styling_fc_wide() ID var_one 1 2 3 4 5 6 7 8 9 10 1 -1 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 2 1 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 3 -1 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 4 1 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 5 -1 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 kable(tb_combine) %&gt;% kable_styling_fc_wide() ID var_one rho1var rho2var rho3var rho4var rho5var rho6var rho7var rho8var rho9var rho10var 1 -1 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 2 1 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 3 -1 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 4 1 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 5 -1 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 -0.2480187 0.7538174 1.0846521 -0.3837419 1.4.1.3 Tibble Row and Column and Summarize Show what is in the table: 1, column and row names; 2, contents inside table. tb_iris &lt;- as_tibble(iris) print(rownames(tb_iris)) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; ## [26] &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; ## [51] &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; &quot;75&quot; ## [76] &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; &quot;100&quot; ## [101] &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; &quot;109&quot; &quot;110&quot; &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;114&quot; &quot;115&quot; &quot;116&quot; &quot;117&quot; &quot;118&quot; &quot;119&quot; &quot;120&quot; &quot;121&quot; &quot;122&quot; &quot;123&quot; &quot;124&quot; &quot;125&quot; ## [126] &quot;126&quot; &quot;127&quot; &quot;128&quot; &quot;129&quot; &quot;130&quot; &quot;131&quot; &quot;132&quot; &quot;133&quot; &quot;134&quot; &quot;135&quot; &quot;136&quot; &quot;137&quot; &quot;138&quot; &quot;139&quot; &quot;140&quot; &quot;141&quot; &quot;142&quot; &quot;143&quot; &quot;144&quot; &quot;145&quot; &quot;146&quot; &quot;147&quot; &quot;148&quot; &quot;149&quot; &quot;150&quot; colnames(tb_iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; colnames(tb_iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; summary(tb_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 1.4.1.4 Tibble Sorting dplyr arrange desc reverse dplyr sort # Sort in Ascending Order tb_iris %&gt;% select(Species, Sepal.Length, everything()) %&gt;% arrange(Species, Sepal.Length) %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() Species Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 4.3 3.0 1.1 0.1 setosa 4.4 2.9 1.4 0.2 setosa 4.4 3.0 1.3 0.2 setosa 4.4 3.2 1.3 0.2 setosa 4.5 2.3 1.3 0.3 setosa 4.6 3.1 1.5 0.2 setosa 4.6 3.4 1.4 0.3 setosa 4.6 3.6 1.0 0.2 setosa 4.6 3.2 1.4 0.2 setosa 4.7 3.2 1.3 0.2 # Sort in Descending Order tb_iris %&gt;% select(Species, Sepal.Length, everything()) %&gt;% arrange(desc(Species), desc(Sepal.Length)) %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() Species Sepal.Length Sepal.Width Petal.Length Petal.Width virginica 7.9 3.8 6.4 2.0 virginica 7.7 3.8 6.7 2.2 virginica 7.7 2.6 6.9 2.3 virginica 7.7 2.8 6.7 2.0 virginica 7.7 3.0 6.1 2.3 virginica 7.6 3.0 6.6 2.1 virginica 7.4 2.8 6.1 1.9 virginica 7.3 2.9 6.3 1.8 virginica 7.2 3.6 6.1 2.5 virginica 7.2 3.2 6.0 1.8 1.4.1.5 REconTools Summarize over Tible Use R4Econ’s summary tool. df_summ_stats &lt;- ff_summ_percentiles(tb_iris) kable(t(df_summ_stats)) %&gt;% kable_styling_fc_wide() stats n NAobs ZEROobs mean sd cv min p01 p05 p10 p25 p50 p75 p90 p95 p99 max Petal.Length 150 0 0 3.758000 1.7652982 0.4697441 1.0 1.149 1.300 1.4 1.6 4.35 5.1 5.80 6.100 6.700 6.9 Petal.Width 150 0 0 1.199333 0.7622377 0.6355511 0.1 0.100 0.200 0.2 0.3 1.30 1.8 2.20 2.300 2.500 2.5 Sepal.Length 150 0 0 5.843333 0.8280661 0.1417113 4.3 4.400 4.600 4.8 5.1 5.80 6.4 6.90 7.255 7.700 7.9 Sepal.Width 150 0 0 3.057333 0.4358663 0.1425642 2.0 2.200 2.345 2.5 2.8 3.00 3.3 3.61 3.800 4.151 4.4 1.4.2 Draw Random Rows Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.2.1 Draw Random Subset of Sample r random discrete We have a sample of N individuals in some dataframe. Draw without replacement a subset \\(M&lt;N\\) of rows. # parameters, it_M &lt; it_N it_N &lt;- 10 it_M &lt;- 5 # Draw it_m from indexed list of it_N set.seed(123) ar_it_rand_idx &lt;- sample(it_N, it_M, replace=FALSE) # dataframe df_full &lt;- as_tibble(matrix(rnorm(4,mean=0,sd=1), nrow=it_N, ncol=4)) %&gt;% rowid_to_column(var = &quot;ID&quot;) # random Subset df_rand_sub_a &lt;- df_full[ar_it_rand_idx,] # Random subset also df_rand_sub_b &lt;- df_full[sample(dim(df_full)[1], it_M, replace=FALSE),] # Print # Display kable(df_full) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 1 0.1292877 0.4609162 0.1292877 0.4609162 2 1.7150650 -1.2650612 1.7150650 -1.2650612 3 0.4609162 0.1292877 0.4609162 0.1292877 4 -1.2650612 1.7150650 -1.2650612 1.7150650 5 0.1292877 0.4609162 0.1292877 0.4609162 6 1.7150650 -1.2650612 1.7150650 -1.2650612 7 0.4609162 0.1292877 0.4609162 0.1292877 8 -1.2650612 1.7150650 -1.2650612 1.7150650 9 0.1292877 0.4609162 0.1292877 0.4609162 10 1.7150650 -1.2650612 1.7150650 -1.2650612 kable(df_rand_sub_a) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 3 0.4609162 0.1292877 0.4609162 0.1292877 10 1.7150650 -1.2650612 1.7150650 -1.2650612 2 1.7150650 -1.2650612 1.7150650 -1.2650612 8 -1.2650612 1.7150650 -1.2650612 1.7150650 6 1.7150650 -1.2650612 1.7150650 -1.2650612 kable(df_rand_sub_b) %&gt;% kable_styling_fc() ID V1 V2 V3 V4 5 0.1292877 0.4609162 0.1292877 0.4609162 3 0.4609162 0.1292877 0.4609162 0.1292877 9 0.1292877 0.4609162 0.1292877 0.4609162 1 0.1292877 0.4609162 0.1292877 0.4609162 4 -1.2650612 1.7150650 -1.2650612 1.7150650 1.4.2.2 Random Subset of Panel There are \\(N\\) individuals, each could be observed \\(M\\) times, but then select a subset of rows only, so each person is randomly observed only a subset of times. Specifically, there there are 3 unique students with student ids, and the second variable shows the random dates in which the student showed up in class, out of the 10 classes available. # Define it_N &lt;- 3 it_M &lt;- 10 svr_id &lt;- &#39;student_id&#39; # dataframe set.seed(123) df_panel_rand &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(date = row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), date) %&gt;% rename(date_in_class = date) # Print kable(df_panel_rand) %&gt;% kable_styling_fc() student_id date_in_class 1 1 1 2 1 8 1 9 1 10 2 5 2 8 2 10 3 1 3 2 3 3 3 4 3 5 3 6 3 9 1.4.3 Variable NA Values Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.3.1 Find and Replace Find and Replace in Dataframe. # For dataframe df.reg &lt;-df.reg %&gt;% na_if(-Inf) %&gt;% na_if(Inf) # For a specific variable in dataframe df.reg.use %&gt;% mutate(!!(var.input) := na_if(!!sym(var.input), 0)) # Setting to NA df.reg.use &lt;- df.reg.guat %&gt;% filter(!!sym(var.mth) != 0) df.reg.use.log &lt;- df.reg.use df.reg.use.log[which(is.nan(df.reg.use$prot.imputed.log)),] = NA df.reg.use.log[which(df.reg.use$prot.imputed.log==Inf),] = NA df.reg.use.log[which(df.reg.use$prot.imputed.log==-Inf),] = NA df.reg.use.log &lt;- df.reg.use.log %&gt;% drop_na(prot.imputed.log) # df.reg.use.log$prot.imputed.log 1.4.4 String Values Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 1.4.4.1 Find and Replace Find and Replace in Dataframe. # if string value is contained in variable (&quot;bridex.B&quot; %in% (df.reg.out.all$vars_var.y)) # if string value is not contained in variable: # 1. type is variable name # 2. Toyota|Mazda are strings to be excluded filter(mtcars, !grepl(&#39;Toyota|Mazda&#39;, type)) # filter does not contain string rs_hgt_prot_log_tidy %&gt;% filter(!str_detect(term, &#39;prot&#39;)) "],
["summarize-data.html", "Chapter 2 Summarize Data 2.1 Counting Observation 2.2 Sorting, Indexing, Slicing 2.3 Group Statistics 2.4 Distributional Statistics 2.5 Summarize Multiple Variables", " Chapter 2 Summarize Data 2.1 Counting Observation 2.1.1 Uncount Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. In some panel, there are \\(N\\) individuals, each observed for \\(Y_i\\) years. Given a dataset with two variables, the individual index, and the \\(Y_i\\) variable, expand the dataframe so that there is a row for each individual index’s each unique year in the survey. Search: r duplicate row by variable Links: see: Create duplicate rows based on a variable Algorithm: generate testing frame, the individual attribute dataset with invariant information over panel uncount, duplicate rows by years in survey group and generate sorted index add indiviual specific stat year to index # 1. Array of Years in the Survey ar_years_in_survey &lt;- c(2,3,1,10,2,5) ar_start_yaer &lt;- c(1,2,3,1,1,1) ar_end_year &lt;- c(2,4,3,10,2,5) mt_combine &lt;- cbind(ar_years_in_survey, ar_start_yaer, ar_end_year) # This is the individual attribute dataset, attributes that are invariant acrosss years tb_indi_attributes &lt;- as_tibble(mt_combine) %&gt;% rowid_to_column(var = &quot;ID&quot;) # 2. Sort and generate variable equal to sorted index tb_indi_panel &lt;- tb_indi_attributes %&gt;% uncount(ar_years_in_survey) # 3. Panel now construct exactly which year in survey, note that all needed is sort index # Note sorting not needed, all rows identical now tb_indi_panel &lt;- tb_indi_panel %&gt;% group_by(ID) %&gt;% mutate(yr_in_survey = row_number()) tb_indi_panel &lt;- tb_indi_panel %&gt;% mutate(calendar_year = yr_in_survey + ar_start_yaer - 1) # Show results Head 10 tb_indi_panel %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc() ID ar_start_yaer ar_end_year yr_in_survey calendar_year 1 1 2 1 1 1 1 2 2 2 2 2 4 1 2 2 2 4 2 3 2 2 4 3 4 3 3 3 1 3 4 1 10 1 1 4 1 10 2 2 4 1 10 3 3 4 1 10 4 4 2.2 Sorting, Indexing, Slicing 2.2.1 Sorting Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 2.2.1.1 Generate Sorted Index within Group with Repeating Values There is a variable, sort by this variable, then generate index from 1 to N representing sorted values of this index. If there are repeating values, still assign index, different index each value. r generate index sort dplyr mutate equals index # Sort and generate variable equal to sorted index df_iris &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% select(Sepal.Length, Sepal.Len.Index, everything()) # Show results Head 10 df_iris %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Width Petal.Length Petal.Width Species 4.3 1 3.0 1.1 0.1 setosa 4.4 2 2.9 1.4 0.2 setosa 4.4 3 3.0 1.3 0.2 setosa 4.4 4 3.2 1.3 0.2 setosa 4.5 5 2.3 1.3 0.3 setosa 4.6 6 3.1 1.5 0.2 setosa 4.6 7 3.4 1.4 0.3 setosa 4.6 8 3.6 1.0 0.2 setosa 4.6 9 3.2 1.4 0.2 setosa 4.7 10 3.2 1.3 0.2 setosa 2.2.1.2 Populate Value from Lowest Index to All other Rows We would like to calculate for example the ratio of each individual’s highest to the the person with the lowest height in a dataset. We first need to generated sorted index from lowest to highest, and then populate the lowest height to all rows, and then divide. Search Terms: r spread value to all rows from one row r other rows equal to the value of one row Conditional assignment of one variable to the value of one of two other variables dplyr mutate conditional dplyr value from one row to all rows dplyr mutate equal to value in another cell Links: see: dplyr rank see: dplyr case_when 2.2.1.2.1 Short Method: mutate and min We just want the lowest value to be in its own column, so that we can compute various statistics using the lowest value variable and the original variable. # 1. Sort df_iris_m1 &lt;- iris %&gt;% mutate(Sepal.Len.Lowest.all = min(Sepal.Length)) %&gt;% select(Sepal.Length, Sepal.Len.Lowest.all, everything()) # Show results Head 10 df_iris_m1 %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Lowest.all Sepal.Width Petal.Length Petal.Width Species 5.1 4.3 3.5 1.4 0.2 setosa 4.9 4.3 3.0 1.4 0.2 setosa 4.7 4.3 3.2 1.3 0.2 setosa 4.6 4.3 3.1 1.5 0.2 setosa 5.0 4.3 3.6 1.4 0.2 setosa 5.4 4.3 3.9 1.7 0.4 setosa 4.6 4.3 3.4 1.4 0.3 setosa 5.0 4.3 3.4 1.5 0.2 setosa 4.4 4.3 2.9 1.4 0.2 setosa 4.9 4.3 3.1 1.5 0.1 setosa 2.2.1.2.2 Long Method: row_number and case_when This is the long method, using row_number, and case_when. The benefit of this method is that it generates several intermediate variables that might be useful. And the key final step is to set a new variable (A=Sepal.Len.Lowest.all) equal to another variable’s (B=Sepal.Length’s) value at the index that satisfies condition based a third variable (C=Sepal.Len.Index). # 1. Sort # 2. generate index # 3. value at lowest index (case_when) # 4. spread value from lowest index to other rows # Note step 4 does not require step 3 df_iris_m2 &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% mutate(Sepal.Len.Lowest.one = case_when(row_number()==1 ~ Sepal.Length)) %&gt;% mutate(Sepal.Len.Lowest.all = Sepal.Length[Sepal.Len.Index==1]) %&gt;% select(Sepal.Length, Sepal.Len.Index, Sepal.Len.Lowest.one, Sepal.Len.Lowest.all) # Show results Head 10 df_iris_m2 %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Len.Lowest.one Sepal.Len.Lowest.all 4.3 1 4.3 4.3 4.4 2 NA 4.3 4.4 3 NA 4.3 4.4 4 NA 4.3 4.5 5 NA 4.3 4.6 6 NA 4.3 4.6 7 NA 4.3 4.6 8 NA 4.3 4.6 9 NA 4.3 4.7 10 NA 4.3 2.2.1.3 Generate Sorted Index based on Deviations Generate Positive and Negative Index based on Ordered Deviation from some Number. There is a variable that is continuous, substract a number from this variable, and generate index based on deviations. Think of the index as generating intervals indicating where the value lies. 0th index indicates the largest value in sequence that is smaller than or equal to number \\(x\\), 1st index indicates the smallest value in sequence that is larger than number \\(x\\). The solution below is a little bit convoluated and long, there is likely a much quicker way. The process below shows various intermediary outputs that help arrive at deviation index Sepal.Len.Devi.Index from initial sorted index Sepal.Len.Index. search: dplyr arrange ignore na dplyr index deviation from order number sequence dplyr index below above dplyr index order below above value # 1. Sort and generate variable equal to sorted index # 2. Plus or minus deviations from some value # 3. Find the zero, which means, the number closests to zero including zero from the negative side # 4. Find the index at the highest zero and below deviation point # 5. Difference of zero index and original sorted index sc_val_x &lt;- 4.65 df_iris_deviate &lt;- iris %&gt;% arrange(Sepal.Length) %&gt;% mutate(Sepal.Len.Index = row_number()) %&gt;% mutate(Sepal.Len.Devi = (Sepal.Length - sc_val_x)) %&gt;% mutate(Sepal.Len.Devi.Neg = case_when(Sepal.Len.Devi &lt;= 0 ~ (-1)*(Sepal.Len.Devi))) %&gt;% arrange((Sepal.Len.Devi.Neg), desc(Sepal.Len.Index)) %&gt;% mutate(Sepal.Len.Index.Zero = case_when(row_number() == 1 ~ Sepal.Len.Index)) %&gt;% mutate(Sepal.Len.Devi.Index = Sepal.Len.Index - Sepal.Len.Index.Zero[row_number() == 1]) %&gt;% arrange(Sepal.Len.Index) %&gt;% select(Sepal.Length, Sepal.Len.Index, Sepal.Len.Devi, Sepal.Len.Devi.Neg, Sepal.Len.Index.Zero, Sepal.Len.Devi.Index) # Show results Head 10 df_iris_deviate %&gt;% head(20) %&gt;% kable() %&gt;% kable_styling_fc_wide() Sepal.Length Sepal.Len.Index Sepal.Len.Devi Sepal.Len.Devi.Neg Sepal.Len.Index.Zero Sepal.Len.Devi.Index 4.3 1 -0.35 0.35 NA -8 4.4 2 -0.25 0.25 NA -7 4.4 3 -0.25 0.25 NA -6 4.4 4 -0.25 0.25 NA -5 4.5 5 -0.15 0.15 NA -4 4.6 6 -0.05 0.05 NA -3 4.6 7 -0.05 0.05 NA -2 4.6 8 -0.05 0.05 NA -1 4.6 9 -0.05 0.05 9 0 4.7 10 0.05 NA NA 1 4.7 11 0.05 NA NA 2 4.8 12 0.15 NA NA 3 4.8 13 0.15 NA NA 4 4.8 14 0.15 NA NA 5 4.8 15 0.15 NA NA 6 4.8 16 0.15 NA NA 7 4.9 17 0.25 NA NA 8 4.9 18 0.25 NA NA 9 4.9 19 0.25 NA NA 10 4.9 20 0.25 NA NA 11 2.3 Group Statistics 2.3.1 Groups Statistics Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 2.3.1.1 Aggrgate Groups only Unique Group and Count There are two variables that are numeric, we want to find all the unique groups of these two variables in a dataset and count how many times each unique group occurs r unique occurrence of numeric groups How to add count of unique values by group to R data.frame # Numeric value combinations unique Groups vars.group &lt;- c(&#39;hgt0&#39;, &#39;wgt0&#39;) # dataset subsetting df_use &lt;- df_hgt_wgt %&gt;% select(!!!syms(c(vars.group))) %&gt;% mutate(hgt0 = round(hgt0/5)*5, wgt0 = round(wgt0/2000)*2000) %&gt;% drop_na() # Group, count and generate means for each numeric variables # mutate_at(vars.group, funs(as.factor(.))) %&gt;% df.group.count &lt;- df_use %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(vars.group)) %&gt;% summarise(n_obs_group=n()) # Show results Head 10 df.group.count %&gt;% kable() %&gt;% kable_styling_fc() hgt0 wgt0 n_obs_group 40 2000 122 45 2000 4586 45 4000 470 50 2000 9691 50 4000 13106 55 2000 126 55 4000 1900 60 6000 18 2.3.1.2 Aggrgate Groups only Unique Group Show up With Means Several variables that are grouping identifiers. Several variables that are values which mean be unique for each group members. For example, a Panel of income for N households over T years with also household education information that is invariant over time. Want to generate a dataset where the unit of observation are households, rather than household years. Take average of all numeric variables that are household and year specific. A complicating factor potentially is that the number of observations differ within group, for example, income might be observed for all years for some households but not for other households. r dplyr aggregate group average Aggregating and analyzing data with dplyr column can’t be modified because it is a grouping variable see also: Aggregating and analyzing data with dplyr # In the df_hgt_wgt from R4Econ, there is a country id, village id, # and individual id, and various other statistics vars.group &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;) vars.values &lt;- c(&#39;hgt&#39;, &#39;momEdu&#39;) # dataset subsetting df_use &lt;- df_hgt_wgt %&gt;% select(!!!syms(c(vars.group, vars.values))) # Group, count and generate means for each numeric variables df.group &lt;- df_use %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(vars.group)) %&gt;% summarise_if(is.numeric, funs(mean = mean(., na.rm = TRUE), sd = sd(., na.rm = TRUE), n = sum(is.na(.)==0))) # Show results Head 10 df.group %&gt;% head(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id hgt_mean momEdu_mean hgt_sd momEdu_sd hgt_n momEdu_n Cebu 1 1 61.80000 5.3 9.520504 0 7 18 Cebu 1 2 68.86154 7.1 9.058931 0 13 18 Cebu 1 3 80.45882 9.4 29.894231 0 17 18 Cebu 1 4 88.10000 13.9 35.533166 0 18 18 Cebu 1 5 97.70556 11.3 41.090366 0 18 18 Cebu 1 6 87.49444 7.3 35.586439 0 18 18 Cebu 1 7 90.79412 10.4 38.722385 0 17 18 Cebu 1 8 68.45385 13.5 10.011961 0 13 18 Cebu 1 9 86.21111 10.4 35.126057 0 18 18 Cebu 1 10 87.67222 10.5 36.508127 0 18 18 # Show results Head 10 df.group %&gt;% tail(10) %&gt;% kable() %&gt;% kable_styling_fc_wide() S.country vil.id indi.id hgt_mean momEdu_mean hgt_sd momEdu_sd hgt_n momEdu_n Guatemala 14 2014 66.97000 NaN 8.967974 NaN 10 0 Guatemala 14 2015 71.71818 NaN 11.399984 NaN 11 0 Guatemala 14 2016 66.33000 NaN 9.490352 NaN 10 0 Guatemala 14 2017 76.40769 NaN 14.827871 NaN 13 0 Guatemala 14 2018 74.55385 NaN 12.707846 NaN 13 0 Guatemala 14 2019 70.47500 NaN 11.797390 NaN 12 0 Guatemala 14 2020 60.28750 NaN 7.060036 NaN 8 0 Guatemala 14 2021 84.96000 NaN 15.446193 NaN 10 0 Guatemala 14 2022 79.38667 NaN 15.824749 NaN 15 0 Guatemala 14 2023 66.50000 NaN 8.613113 NaN 8 0 2.3.2 One Variable Group Summary Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. There is a categorical variable (based on one or the interaction of multiple variables), there is a continuous variable, obtain statistics for the continuous variable conditional on the categorical variable, but also unconditionally. Store results in a matrix, but also flatten results wide to row with appropriate keys/variable-names for all group statistics. Pick which statistics to be included in final wide row 2.3.2.1 Build Program # Single Variable Group Statistics (also generate overall statistics) ff_summ_by_group_summ_one &lt;- function( df, vars.group, var.numeric, str.stats.group = &#39;main&#39;, str.stats.specify = NULL, boo.overall.stats = TRUE){ # List of statistics # https://rdrr.io/cran/dplyr/man/summarise.html strs.center &lt;- c(&#39;mean&#39;, &#39;median&#39;) strs.spread &lt;- c(&#39;sd&#39;, &#39;IQR&#39;, &#39;mad&#39;) strs.range &lt;- c(&#39;min&#39;, &#39;max&#39;) strs.pos &lt;- c(&#39;first&#39;, &#39;last&#39;) strs.count &lt;- c(&#39;n_distinct&#39;) # Grouping of Statistics if (missing(str.stats.specify)) { if (str.stats.group == &#39;main&#39;) { strs.all &lt;- c(&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;, &#39;sd&#39;) } if (str.stats.group == &#39;all&#39;) { strs.all &lt;- c(strs.center, strs.spread, strs.range, strs.pos, strs.count) } } else { strs.all &lt;- str.stats.specify } # Start Transform df &lt;- df %&gt;% drop_na() %&gt;% mutate(!!(var.numeric) := as.numeric(!!sym(var.numeric))) # Overall Statistics if (boo.overall.stats) { df.overall.stats &lt;- df %&gt;% summarize_at(vars(var.numeric), funs(!!!strs.all)) if (length(strs.all) == 1) { # give it a name, otherwise if only one stat, name of stat not saved df.overall.stats &lt;- df.overall.stats %&gt;% rename(!!strs.all := !!sym(var.numeric)) } names(df.overall.stats) &lt;- paste0(var.numeric, &#39;.&#39;, names(df.overall.stats)) } # Group Sort df.select &lt;- df %&gt;% group_by(!!!syms(vars.group)) %&gt;% arrange(!!!syms(c(vars.group, var.numeric))) # Table of Statistics df.table.grp.stats &lt;- df.select %&gt;% summarize_at(vars(var.numeric), funs(!!!strs.all)) # Add Stat Name if (length(strs.all) == 1) { # give it a name, otherwise if only one stat, name of stat not saved df.table.grp.stats &lt;- df.table.grp.stats %&gt;% rename(!!strs.all := !!sym(var.numeric)) } # Row of Statistics str.vars.group.combine &lt;- paste0(vars.group, collapse=&#39;_&#39;) if (length(vars.group) == 1) { df.row.grp.stats &lt;- df.table.grp.stats %&gt;% mutate(!!(str.vars.group.combine) := paste0(var.numeric, &#39;.&#39;, vars.group, &#39;.g&#39;, (!!!syms(vars.group)))) %&gt;% gather(variable, value, -one_of(vars.group)) %&gt;% unite(str.vars.group.combine, c(str.vars.group.combine, &#39;variable&#39;)) %&gt;% spread(str.vars.group.combine, value) } else { df.row.grp.stats &lt;- df.table.grp.stats %&gt;% mutate(vars.groups.combine := paste0(paste0(vars.group, collapse=&#39;.&#39;)), !!(str.vars.group.combine) := paste0(interaction(!!!(syms(vars.group))))) %&gt;% mutate(!!(str.vars.group.combine) := paste0(var.numeric, &#39;.&#39;, vars.groups.combine, &#39;.&#39;, (!!sym(str.vars.group.combine)))) %&gt;% ungroup() %&gt;% select(-vars.groups.combine, -one_of(vars.group)) %&gt;% gather(variable, value, -one_of(str.vars.group.combine)) %&gt;% unite(str.vars.group.combine, c(str.vars.group.combine, &#39;variable&#39;)) %&gt;% spread(str.vars.group.combine, value) } # Clean up name strings names(df.table.grp.stats) &lt;- gsub(x = names(df.table.grp.stats),pattern = &quot;_&quot;, replacement = &quot;\\\\.&quot;) names(df.row.grp.stats) &lt;- gsub(x = names(df.row.grp.stats),pattern = &quot;_&quot;, replacement = &quot;\\\\.&quot;) # Return list.return &lt;- list(df_table_grp_stats = df.table.grp.stats, df_row_grp_stats = df.row.grp.stats) # Overall Statistics, without grouping if (boo.overall.stats) { df.row.stats.all &lt;- c(df.row.grp.stats, df.overall.stats) list.return &lt;- append(list.return, list(df_overall_stats = df.overall.stats, df_row_stats_all = df.row.stats.all)) } # Return return(list.return) } 2.3.2.2 Test Load data and test # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) ## Parsed with column specification: ## cols( ## S.country = col_character(), ## vil.id = col_double(), ## indi.id = col_double(), ## sex = col_character(), ## svymthRound = col_double(), ## momEdu = col_double(), ## wealthIdx = col_double(), ## hgt = col_double(), ## wgt = col_double(), ## hgt0 = col_double(), ## wgt0 = col_double(), ## prot = col_double(), ## cal = col_double(), ## p.A.prot = col_double(), ## p.A.nProt = col_double() ## ) 2.3.2.2.1 Function Testing By Gender Groups Need two variables, a group variable that is a factor, and a numeric vars.group &lt;- &#39;sex&#39; var.numeric &lt;- &#39;hgt&#39; df.select &lt;- df %&gt;% select(one_of(vars.group, var.numeric)) %&gt;% drop_na() Main Statistics: # Single Variable Group Statistics ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.group = &#39;main&#39;) ## $df_table_grp_stats ## # A tibble: 2 x 5 ## sex mean min max sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 82.8 41.2 171. 29.8 ## 2 Male 84.7 41.3 183. 31.8 ## ## $df_row_grp_stats ## # A tibble: 1 x 8 ## hgt.sex.gFemale.max hgt.sex.gFemale.mean hgt.sex.gFemale.min hgt.sex.gFemale.sd hgt.sex.gMale.max hgt.sex.gMale.mean hgt.sex.gMale.min hgt.sex.gMale.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 171. 82.8 41.2 29.8 183. 84.7 41.3 31.8 ## ## $df_overall_stats ## # A tibble: 1 x 4 ## hgt.mean hgt.min hgt.max hgt.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 83.8 41.2 183. 30.9 ## ## $df_row_stats_all ## $df_row_stats_all$hgt.sex.gFemale.max ## [1] 170.6 ## ## $df_row_stats_all$hgt.sex.gFemale.mean ## [1] 82.81198 ## ## $df_row_stats_all$hgt.sex.gFemale.min ## [1] 41.2 ## ## $df_row_stats_all$hgt.sex.gFemale.sd ## [1] 29.79351 ## ## $df_row_stats_all$hgt.sex.gMale.max ## [1] 182.9 ## ## $df_row_stats_all$hgt.sex.gMale.mean ## [1] 84.68152 ## ## $df_row_stats_all$hgt.sex.gMale.min ## [1] 41.3 ## ## $df_row_stats_all$hgt.sex.gMale.sd ## [1] 31.75037 ## ## $df_row_stats_all$hgt.mean ## [1] 83.80921 ## ## $df_row_stats_all$hgt.min ## [1] 41.2 ## ## $df_row_stats_all$hgt.max ## [1] 182.9 ## ## $df_row_stats_all$hgt.sd ## [1] 30.86631 Specify Two Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;, &#39;sd&#39;)) ## $df_table_grp_stats ## # A tibble: 2 x 3 ## sex mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 82.8 29.8 ## 2 Male 84.7 31.8 ## ## $df_row_grp_stats ## # A tibble: 1 x 4 ## hgt.sex.gFemale.mean hgt.sex.gFemale.sd hgt.sex.gMale.mean hgt.sex.gMale.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 82.8 29.8 84.7 31.8 ## ## $df_overall_stats ## # A tibble: 1 x 2 ## hgt.mean hgt.sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 83.8 30.9 ## ## $df_row_stats_all ## $df_row_stats_all$hgt.sex.gFemale.mean ## [1] 82.81198 ## ## $df_row_stats_all$hgt.sex.gFemale.sd ## [1] 29.79351 ## ## $df_row_stats_all$hgt.sex.gMale.mean ## [1] 84.68152 ## ## $df_row_stats_all$hgt.sex.gMale.sd ## [1] 31.75037 ## ## $df_row_stats_all$hgt.mean ## [1] 83.80921 ## ## $df_row_stats_all$hgt.sd ## [1] 30.86631 Specify One Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;)) ## $df_table_grp_stats ## # A tibble: 2 x 2 ## sex mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 82.8 ## 2 Male 84.7 ## ## $df_row_grp_stats ## # A tibble: 1 x 2 ## hgt.sex.gFemale.mean hgt.sex.gMale.mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 82.8 84.7 ## ## $df_overall_stats ## # A tibble: 1 x 1 ## hgt.mean ## &lt;dbl&gt; ## 1 83.8 ## ## $df_row_stats_all ## $df_row_stats_all$hgt.sex.gFemale.mean ## [1] 82.81198 ## ## $df_row_stats_all$hgt.sex.gMale.mean ## [1] 84.68152 ## ## $df_row_stats_all$hgt.mean ## [1] 83.80921 2.3.2.2.2 Function Testing By Country and Gender Groups Need two variables, a group variable that is a factor, and a numeric. Now joint grouping variables. vars.group &lt;- c(&#39;S.country&#39;, &#39;sex&#39;) var.numeric &lt;- &#39;hgt&#39; df.select &lt;- df %&gt;% select(one_of(vars.group, var.numeric)) %&gt;% drop_na() Main Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.group = &#39;main&#39;) ## $df_table_grp_stats ## # A tibble: 4 x 6 ## # Groups: S.country [2] ## S.country sex mean min max sd ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu Female 84.6 41.3 171. 32.5 ## 2 Cebu Male 87.0 41.3 183. 35.0 ## 3 Guatemala Female 76.6 41.2 120. 15.7 ## 4 Guatemala Male 77.0 41.5 125. 15.1 ## ## $df_row_grp_stats ## # A tibble: 1 x 16 ## hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ hgt.S.country.s~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 171. 84.6 41.3 32.5 183. 87.0 41.3 35.0 120. ## # ... with 7 more variables: hgt.S.country.sex.Guatemala.Female.mean &lt;dbl&gt;, hgt.S.country.sex.Guatemala.Female.min &lt;dbl&gt;, ## # hgt.S.country.sex.Guatemala.Female.sd &lt;dbl&gt;, hgt.S.country.sex.Guatemala.Male.max &lt;dbl&gt;, hgt.S.country.sex.Guatemala.Male.mean &lt;dbl&gt;, ## # hgt.S.country.sex.Guatemala.Male.min &lt;dbl&gt;, hgt.S.country.sex.Guatemala.Male.sd &lt;dbl&gt; ## ## $df_overall_stats ## # A tibble: 1 x 4 ## hgt.mean hgt.min hgt.max hgt.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 83.8 41.2 183. 30.9 ## ## $df_row_stats_all ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.max ## [1] 170.6 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.mean ## [1] 84.61326 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.min ## [1] 41.3 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.sd ## [1] 32.53651 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.max ## [1] 182.9 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.mean ## [1] 87.02836 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.min ## [1] 41.3 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.sd ## [1] 34.9909 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.max ## [1] 119.9 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.mean ## [1] 76.58771 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.min ## [1] 41.2 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.sd ## [1] 15.71801 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.max ## [1] 124.7 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.mean ## [1] 77.0471 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.min ## [1] 41.5 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.sd ## [1] 15.11444 ## ## $df_row_stats_all$hgt.mean ## [1] 83.80921 ## ## $df_row_stats_all$hgt.min ## [1] 41.2 ## ## $df_row_stats_all$hgt.max ## [1] 182.9 ## ## $df_row_stats_all$hgt.sd ## [1] 30.86631 Specify Two Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;, &#39;sd&#39;)) ## $df_table_grp_stats ## # A tibble: 4 x 4 ## # Groups: S.country [2] ## S.country sex mean sd ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu Female 84.6 32.5 ## 2 Cebu Male 87.0 35.0 ## 3 Guatemala Female 76.6 15.7 ## 4 Guatemala Male 77.0 15.1 ## ## $df_row_grp_stats ## # A tibble: 1 x 8 ## hgt.S.country.sex.~ hgt.S.country.sex.~ hgt.S.country.sex.~ hgt.S.country.sex~ hgt.S.country.sex~ hgt.S.country.sex~ hgt.S.country.sex~ hgt.S.country.sex~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 84.6 32.5 87.0 35.0 76.6 15.7 77.0 15.1 ## ## $df_overall_stats ## # A tibble: 1 x 2 ## hgt.mean hgt.sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 83.8 30.9 ## ## $df_row_stats_all ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.mean ## [1] 84.61326 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.sd ## [1] 32.53651 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.mean ## [1] 87.02836 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.sd ## [1] 34.9909 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.mean ## [1] 76.58771 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.sd ## [1] 15.71801 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.mean ## [1] 77.0471 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.sd ## [1] 15.11444 ## ## $df_row_stats_all$hgt.mean ## [1] 83.80921 ## ## $df_row_stats_all$hgt.sd ## [1] 30.86631 Specify One Specific Statistics: ff_summ_by_group_summ_one( df.select, vars.group = vars.group, var.numeric = var.numeric, str.stats.specify = c(&#39;mean&#39;)) ## $df_table_grp_stats ## # A tibble: 4 x 3 ## # Groups: S.country [2] ## S.country sex mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Cebu Female 84.6 ## 2 Cebu Male 87.0 ## 3 Guatemala Female 76.6 ## 4 Guatemala Male 77.0 ## ## $df_row_grp_stats ## # A tibble: 1 x 4 ## hgt.S.country.sex.Cebu.Female.mean hgt.S.country.sex.Cebu.Male.mean hgt.S.country.sex.Guatemala.Female.mean hgt.S.country.sex.Guatemala.Male.mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 84.6 87.0 76.6 77.0 ## ## $df_overall_stats ## # A tibble: 1 x 1 ## hgt.mean ## &lt;dbl&gt; ## 1 83.8 ## ## $df_row_stats_all ## $df_row_stats_all$hgt.S.country.sex.Cebu.Female.mean ## [1] 84.61326 ## ## $df_row_stats_all$hgt.S.country.sex.Cebu.Male.mean ## [1] 87.02836 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Female.mean ## [1] 76.58771 ## ## $df_row_stats_all$hgt.S.country.sex.Guatemala.Male.mean ## [1] 77.0471 ## ## $df_row_stats_all$hgt.mean ## [1] 83.80921 2.3.3 Nested within Group Stats Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. By Multiple within Individual Groups Variables, Averages for All Numeric Variables within All Groups of All Group Variables (Long to very Wide). Suppose you have an individual level final outcome. The individual is observed for N periods, where each period the inputs differ. What inputs impacted the final outcome? Suppose we can divide N periods in which the individual is in the data into a number of years, a number of semi-years, a number of quarters, or uneven-staggered lengths. We might want to generate averages across individuals and within each of these different possible groups averages of inputs. Then we want to version of the data where each row is an individual, one of the variables is the final outcome, and the other variables are these different averages: averages for the 1st, 2nd, 3rd year in which indivdiual is in data, averages for 1st, …, final quarter in which indivdiual is in data. 2.3.3.1 Build Function This function takes as inputs: vars.not.groups2avg: a list of variables that are not the within-indivdiual or across-individual grouping variables, but the variables we want to average over. Withnin indivdiual grouping averages will be calculated for these variables using the not-listed variables as within indivdiual groups (excluding vars.indi.grp groups). vars.indi.grp: a list or individual variables, and also perhaps villages, province, etc id variables that are higher than individual ID. Note the groups are are ACROSS individual higher level group variables. the remaining variables are all within individual grouping variables. the function output is a dataframe: each row is an individual initial variables individual ID and across individual groups from vars.indi.grp. other variables are all averages for the variables in vars.not.groups2avg if there are 2 within individual group variables, and the first has 3 groups (years), the second has 6 groups (semi-years), then there would be 9 average variables. each average variables has the original variable name from vars.not.groups2avg plus the name of the within individual grouping variable, and at the end ‘c_x’, where x is a integer representing the category within the group (if 3 years, x=1, 2, 3) # Data Function # https://fanwangecon.github.io/R4Econ/summarize/summ/ByGroupsSummWide.html f.by.groups.summ.wide &lt;- function(df.groups.to.average, vars.not.groups2avg, vars.indi.grp = c(&#39;S.country&#39;,&#39;ID&#39;), display=TRUE) { # 1. generate categoricals for full year (m.12), half year (m.6), quarter year (m.4) # 2. generate categoricals also for uneven years (m12t14) using stagger (+2 rather than -1) # 3. reshape wide to long, so that all categorical date groups appear in var=value, # and categories in var=variable # 4. calculate mean for all numeric variables for all date groups # 5. combine date categorical variable and value, single var: # m.12.c1= first year average from m.12 averaging ######## ######## ######## ######## ####### # Step 1 ######## ######## ######## ######## ####### # 1. generate categoricals for full year (m.12), half year (m.6), quarter year (m.4) # 2. generate categoricals also for uneven years (m12t14) using stagger (+2 rather than -1) ######## ######## ######## ######## ####### # S2: reshape wide to long, so that all categorical date groups appear in var=value, # and categories in var=variable; calculate mean for all numeric variables for all date groups ######## ######## ######## ######## ####### df.avg.long &lt;- df.groups.to.average %&gt;% gather(variable, value, -one_of(c(vars.indi.grp, vars.not.groups2avg))) %&gt;% group_by(!!!syms(vars.indi.grp), variable, value) %&gt;% summarise_if(is.numeric, funs(mean(., na.rm = TRUE))) if (display){ dim(df.avg.long) options(repr.matrix.max.rows=10, repr.matrix.max.cols=20) print(df.avg.long) } ######## ######## ######## ######## ####### # S3 combine date categorical variable and value, single var: # m.12.c1= first year average from m.12 averaging; to do this make data even longer first ######## ######## ######## ######## ####### # We already have the averages, but we want them to show up as variables, # mean for each group of each variable. df.avg.allvars.wide &lt;- df.avg.long %&gt;% ungroup() %&gt;% mutate(all_m_cate = paste0(variable, &#39;_c&#39;, value)) %&gt;% select(all_m_cate, everything(), -variable, -value) %&gt;% gather(variable, value, -one_of(vars.indi.grp), -all_m_cate) %&gt;% unite(&#39;var_mcate&#39;, variable, all_m_cate) %&gt;% spread(var_mcate, value) if (display){ dim(df.avg.allvars.wide) options(repr.matrix.max.rows=10, repr.matrix.max.cols=10) print(df.avg.allvars.wide) } return(df.avg.allvars.wide) } 2.3.3.2 Test Program In our sample dataset, the number of nutrition/height/income etc information observed within each country and month of age group are different. We have a panel dataset for children observed over different months of age. We have two key grouping variables: 1. country: data are observed for guatemala and cebu 2. month-age (survey month round=svymthRound): different months of age at which each individual child is observed A child could be observed for many months, or just a few months. A child’s height information could be observed for more months-of-age than nutritional intake information. We eventually want to run regressions where the outcome is height/weight and the input is nutrition. The regressions will be at the month-of-age level. We need to know how many times different variables are observed at the month-of-age level. # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) ## Parsed with column specification: ## cols( ## S.country = col_character(), ## vil.id = col_double(), ## indi.id = col_double(), ## sex = col_character(), ## svymthRound = col_double(), ## momEdu = col_double(), ## wealthIdx = col_double(), ## hgt = col_double(), ## wgt = col_double(), ## hgt0 = col_double(), ## wgt0 = col_double(), ## prot = col_double(), ## cal = col_double(), ## p.A.prot = col_double(), ## p.A.nProt = col_double() ## ) 2.3.3.2.1 Generate Within Individual Groups In the data, children are observed for different number of months since birth. We want to calculate quarterly, semi-year, annual, etc average nutritional intakes. First generate these within-individual grouping variables. We can also generate uneven-staggered calendar groups as shown below. mth.var &lt;- &#39;svymthRound&#39; df.groups.to.average&lt;- df %&gt;% filter(!!sym(mth.var) &gt;= 0 &amp; !!sym(mth.var) &lt;= 24) %&gt;% mutate(m12t24=(floor((!!sym(mth.var) - 12) %/% 14) + 1), m8t24=(floor((!!sym(mth.var) - 8) %/% 18) + 1), m12 = pmax((floor((!!sym(mth.var)-1) %/% 12) + 1), 1), m6 = pmax((floor((!!sym(mth.var)-1) %/% 6) + 1), 1), m3 = pmax((floor((!!sym(mth.var)-1) %/% 3) + 1), 1)) # Show Results options(repr.matrix.max.rows=30, repr.matrix.max.cols=20) vars.arrange &lt;- c(&#39;S.country&#39;,&#39;indi.id&#39;,&#39;svymthRound&#39;) vars.groups.within.indi &lt;- c(&#39;m12t24&#39;, &#39;m8t24&#39;, &#39;m12&#39;, &#39;m6&#39;, &#39;m3&#39;) as.tibble(df.groups.to.average %&gt;% group_by(!!!syms(vars.arrange)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% select(!!!syms(vars.arrange), !!!syms(vars.groups.within.indi))) ## # A tibble: 23,603 x 8 ## S.country indi.id svymthRound m12t24 m8t24 m12 m6 m3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu 1 0 0 0 1 1 1 ## 2 Cebu 1 2 0 0 1 1 1 ## 3 Cebu 1 4 0 0 1 1 2 ## 4 Cebu 1 6 0 0 1 1 2 ## 5 Cebu 1 8 0 1 1 2 3 ## 6 Cebu 1 10 0 1 1 2 4 ## 7 Cebu 1 12 1 1 1 2 4 ## 8 Cebu 1 14 1 1 2 3 5 ## 9 Cebu 1 16 1 1 2 3 6 ## 10 Cebu 1 18 1 1 2 3 6 ## # ... with 23,593 more rows 2.3.3.2.2 Within Group Averages With the within-group averages created, we can generate averages for all variables within these groups. vars.not.groups2avg &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.indi.grp &lt;- c(&#39;S.country&#39;, &#39;indi.id&#39;) vars.groups.within.indi &lt;- c(&#39;m12t24&#39;, &#39;m8t24&#39;, &#39;m12&#39;, &#39;m6&#39;, &#39;m3&#39;) df.groups.to.average.select &lt;- df.groups.to.average %&gt;% select(one_of(c(vars.indi.grp, vars.not.groups2avg, vars.groups.within.indi))) df.avg.allvars.wide &lt;- f.by.groups.summ.wide(df.groups.to.average.select, vars.not.groups2avg, vars.indi.grp, display=TRUE) ## # A tibble: 36,414 x 6 ## # Groups: S.country, indi.id, variable [10,115] ## S.country indi.id variable value prot cal ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu 1 m12 1 5.36 132. ## 2 Cebu 1 m12 2 NaN NaN ## 3 Cebu 1 m12t24 0 4.37 97.1 ## 4 Cebu 1 m12t24 1 11.3 343. ## 5 Cebu 1 m3 1 0.65 9.1 ## 6 Cebu 1 m3 2 3.65 95.5 ## 7 Cebu 1 m3 3 2.6 85.3 ## 8 Cebu 1 m3 4 13.2 315. ## 9 Cebu 1 m3 5 NaN NaN ## 10 Cebu 1 m3 6 NaN NaN ## # ... with 36,404 more rows ## # A tibble: 2,023 x 38 ## S.country indi.id cal_m12_c1 cal_m12_c2 cal_m12t24_c0 cal_m12t24_c1 cal_m3_c1 cal_m3_c2 cal_m3_c3 cal_m3_c4 cal_m3_c5 cal_m3_c6 cal_m3_c7 cal_m3_c8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu 1 132. NaN 97.1 343. 9.1 95.5 85.3 315. NaN NaN NaN NaN ## 2 Cebu 2 90.7 256. 81.5 240. 83.4 12.3 155. 144. 228 153. 305 348. ## 3 Cebu 3 96.8 659. 31.6 634. 0.5 28.8 57 281. 459. 550. 612 891. ## 4 Cebu 4 27.5 372. 24.6 325. 4.5 26.0 39.4 46.0 221. 271 581. 443. ## 5 Cebu 5 101. 1081. 79.2 960. 14.1 144. 71.3 161. 453. 1345. 1178. 1082 ## 6 Cebu 6 185. 522. 162. 493. 23.8 185. 169. 356. 653. 506. 417. 523 ## 7 Cebu 7 157. 571. 146. 514. 8.3 138. 408. 200. 391. 637. 688. 570. ## 8 Cebu 8 472. 845. 379. 871. 159. 423 418. 861. 691. 898. 637. 972. ## 9 Cebu 9 32.3 415. 16.6 374. 5.05 10.4 15.1 90.0 142. 204. 753. 594. ## 10 Cebu 10 67.2 395. 68.6 347. 9.55 26.4 165. 117. 297. 303 385. 542. ## # ... with 2,013 more rows, and 24 more variables: cal_m6_c1 &lt;dbl&gt;, cal_m6_c2 &lt;dbl&gt;, cal_m6_c3 &lt;dbl&gt;, cal_m6_c4 &lt;dbl&gt;, cal_m8t24_c0 &lt;dbl&gt;, ## # cal_m8t24_c1 &lt;dbl&gt;, prot_m12_c1 &lt;dbl&gt;, prot_m12_c2 &lt;dbl&gt;, prot_m12t24_c0 &lt;dbl&gt;, prot_m12t24_c1 &lt;dbl&gt;, prot_m3_c1 &lt;dbl&gt;, prot_m3_c2 &lt;dbl&gt;, ## # prot_m3_c3 &lt;dbl&gt;, prot_m3_c4 &lt;dbl&gt;, prot_m3_c5 &lt;dbl&gt;, prot_m3_c6 &lt;dbl&gt;, prot_m3_c7 &lt;dbl&gt;, prot_m3_c8 &lt;dbl&gt;, prot_m6_c1 &lt;dbl&gt;, prot_m6_c2 &lt;dbl&gt;, ## # prot_m6_c3 &lt;dbl&gt;, prot_m6_c4 &lt;dbl&gt;, prot_m8t24_c0 &lt;dbl&gt;, prot_m8t24_c1 &lt;dbl&gt; This is the tabular version of results dim(df.avg.allvars.wide) ## [1] 2023 38 names(df.avg.allvars.wide) ## [1] &quot;S.country&quot; &quot;indi.id&quot; &quot;cal_m12_c1&quot; &quot;cal_m12_c2&quot; &quot;cal_m12t24_c0&quot; &quot;cal_m12t24_c1&quot; &quot;cal_m3_c1&quot; &quot;cal_m3_c2&quot; ## [9] &quot;cal_m3_c3&quot; &quot;cal_m3_c4&quot; &quot;cal_m3_c5&quot; &quot;cal_m3_c6&quot; &quot;cal_m3_c7&quot; &quot;cal_m3_c8&quot; &quot;cal_m6_c1&quot; &quot;cal_m6_c2&quot; ## [17] &quot;cal_m6_c3&quot; &quot;cal_m6_c4&quot; &quot;cal_m8t24_c0&quot; &quot;cal_m8t24_c1&quot; &quot;prot_m12_c1&quot; &quot;prot_m12_c2&quot; &quot;prot_m12t24_c0&quot; &quot;prot_m12t24_c1&quot; ## [25] &quot;prot_m3_c1&quot; &quot;prot_m3_c2&quot; &quot;prot_m3_c3&quot; &quot;prot_m3_c4&quot; &quot;prot_m3_c5&quot; &quot;prot_m3_c6&quot; &quot;prot_m3_c7&quot; &quot;prot_m3_c8&quot; ## [33] &quot;prot_m6_c1&quot; &quot;prot_m6_c2&quot; &quot;prot_m6_c3&quot; &quot;prot_m6_c4&quot; &quot;prot_m8t24_c0&quot; &quot;prot_m8t24_c1&quot; options(repr.matrix.max.rows=30, repr.matrix.max.cols=12) df.avg.allvars.wide ## # A tibble: 2,023 x 38 ## S.country indi.id cal_m12_c1 cal_m12_c2 cal_m12t24_c0 cal_m12t24_c1 cal_m3_c1 cal_m3_c2 cal_m3_c3 cal_m3_c4 cal_m3_c5 cal_m3_c6 cal_m3_c7 cal_m3_c8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu 1 132. NaN 97.1 343. 9.1 95.5 85.3 315. NaN NaN NaN NaN ## 2 Cebu 2 90.7 256. 81.5 240. 83.4 12.3 155. 144. 228 153. 305 348. ## 3 Cebu 3 96.8 659. 31.6 634. 0.5 28.8 57 281. 459. 550. 612 891. ## 4 Cebu 4 27.5 372. 24.6 325. 4.5 26.0 39.4 46.0 221. 271 581. 443. ## 5 Cebu 5 101. 1081. 79.2 960. 14.1 144. 71.3 161. 453. 1345. 1178. 1082 ## 6 Cebu 6 185. 522. 162. 493. 23.8 185. 169. 356. 653. 506. 417. 523 ## 7 Cebu 7 157. 571. 146. 514. 8.3 138. 408. 200. 391. 637. 688. 570. ## 8 Cebu 8 472. 845. 379. 871. 159. 423 418. 861. 691. 898. 637. 972. ## 9 Cebu 9 32.3 415. 16.6 374. 5.05 10.4 15.1 90.0 142. 204. 753. 594. ## 10 Cebu 10 67.2 395. 68.6 347. 9.55 26.4 165. 117. 297. 303 385. 542. ## # ... with 2,013 more rows, and 24 more variables: cal_m6_c1 &lt;dbl&gt;, cal_m6_c2 &lt;dbl&gt;, cal_m6_c3 &lt;dbl&gt;, cal_m6_c4 &lt;dbl&gt;, cal_m8t24_c0 &lt;dbl&gt;, ## # cal_m8t24_c1 &lt;dbl&gt;, prot_m12_c1 &lt;dbl&gt;, prot_m12_c2 &lt;dbl&gt;, prot_m12t24_c0 &lt;dbl&gt;, prot_m12t24_c1 &lt;dbl&gt;, prot_m3_c1 &lt;dbl&gt;, prot_m3_c2 &lt;dbl&gt;, ## # prot_m3_c3 &lt;dbl&gt;, prot_m3_c4 &lt;dbl&gt;, prot_m3_c5 &lt;dbl&gt;, prot_m3_c6 &lt;dbl&gt;, prot_m3_c7 &lt;dbl&gt;, prot_m3_c8 &lt;dbl&gt;, prot_m6_c1 &lt;dbl&gt;, prot_m6_c2 &lt;dbl&gt;, ## # prot_m6_c3 &lt;dbl&gt;, prot_m6_c4 &lt;dbl&gt;, prot_m8t24_c0 &lt;dbl&gt;, prot_m8t24_c1 &lt;dbl&gt; 2.4 Distributional Statistics 2.4.1 Histogram 2.4.1.1 Generate Test Score Dataset Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r generate text string as csv r tibble matrix hand input First, we will generate a test score dataset, directly from string. Below we type line by line a dataset with four variables in comma separated (csv) format, where the first row includes the variables names. These texts could be stored in a separate file, or they could be directly included in code and read in as csv 2.4.1.1.1 A Dataset with only Two Continuous Variable ar_test_scores_ec3 &lt;- c(107.72,101.28,105.92,109.31,104.27,110.27,91.92846154,81.8,109.0071429,103.07,98.97923077,101.91,96.49,97.79923077,99.07846154,99.17,103.51,112.2225,101.2964286,94.5,98.92,97.09,93.83989011,97.36304945,80.34,65.74,85.275,82.19708791,86.53758242,86.2025,86.63,82.57392857,83.66,79.76,75.55642857,86.32571429,66.41,76.06,44.225,82.28,47.77392857,70.005,69.13769231,73.52571429,60.51,56.04) ar_test_scores_ec1 &lt;- c(101.72,101.28,99.92,103.31,100.27,104.27,90.23615385,77.8,103.4357143,97.07,93.13307692,95.91,92.49,93.95307692,95.38615385,97.17,99.51,100.3475,95.83214286,92.5,94.92,91.09,90.4332967,93.52101648,80.34,59.74,79.525,77.67236264,81.59252747,82.3275,80.63,76.98464286,81.66,79.76,70.59214286,82.46857143,66.41,74.06,40.475,76.28,44.18464286,66.255,65.59923077,69.66857143,60.51,56.04) mt_test_scores &lt;- cbind(ar_test_scores_ec1, ar_test_scores_ec3) ar_st_varnames &lt;- c(&#39;course_total_ec1p&#39;,&#39;course_total_ec3p&#39;) tb_final_twovar &lt;- as_tibble(mt_test_scores) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_final_twovar) ## course_total_ec1p course_total_ec3p ## Min. : 40.48 Min. : 44.23 ## 1st Qu.: 76.46 1st Qu.: 79.91 ## Median : 86.35 Median : 89.28 ## Mean : 83.88 Mean : 87.90 ## 3rd Qu.: 95.89 3rd Qu.:100.75 ## Max. :104.27 Max. :112.22 ff_summ_percentiles(df = tb_final_twovar, bl_statsasrows = TRUE, col2varname = FALSE) ## # A tibble: 17 x 3 ## stats course.total.ec1p course.total.ec3p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n 46 46 ## 2 NAobs 0 0 ## 3 ZEROobs 0 0 ## 4 mean 83.87572 87.90239 ## 5 sd 15.87272 16.76041 ## 6 cv 0.1892409 0.1906706 ## 7 min 40.475 44.225 ## 8 p01 42.14434 45.82202 ## 9 p05 56.9650 57.1575 ## 10 p10 63.05462 66.07500 ## 11 p25 76.45616 79.90500 ## 12 p50 86.35236 89.27923 ## 13 p75 &quot; 95.89054&quot; 100.75250 ## 14 p90 100.8137 106.8200 ## 15 p95 102.9125 109.2343 ## 16 p99 103.8946 111.3439 ## 17 max 104.2700 112.2225 2.4.1.1.2 A Dataset with one Continuous Variable and Histogram ar_final_scores &lt;- c(94.28442509,95.68817475,97.25219512,77.89268293,95.08795497,93.27380863,92.3,84.25317073,86.08642991,84.69219512,71.43634146,76.21365854,71.68878049,77.46142589,79.29579268,43.7285453,63.80634146,67.92994774,100.8980488,100.0857143,99.93073171,98.4102439,97.93,97.10359756,96.97121951,96.60292683,96.23317073,93.92243902,93.82243902,92.75390244,92.65775261,92.20444653,91.73463415,90.38321161,89.37414634,86.95932458,79.58686411,78.70878049,77.2497561,76.88195122,76.52987805,74.72114313,74.27488676,71.30268293,63.70256098,37.90426829,2.292682927) mt_test_scores &lt;- cbind(seq(1,length(ar_final_scores)), ar_final_scores) ar_st_varnames &lt;- c(&#39;index&#39;, &#39;course_final&#39;) tb_onevar &lt;- as_tibble(mt_test_scores) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_onevar) ## index course_final ## Min. : 1.0 Min. : 2.293 ## 1st Qu.:12.5 1st Qu.: 76.372 ## Median :24.0 Median : 86.959 ## Mean :24.0 Mean : 82.415 ## 3rd Qu.:35.5 3rd Qu.: 94.686 ## Max. :47.0 Max. :100.898 ff_summ_percentiles(df = tb_onevar, bl_statsasrows = TRUE, col2varname = FALSE) ## # A tibble: 17 x 3 ## stats course.final index ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n 47 47 ## 2 NAobs 0 0 ## 3 ZEROobs 0 0 ## 4 mean 82.41501 24.00000 ## 5 sd 18.35476 13.71131 ## 6 cv 0.2227113 0.5713046 ## 7 min 2.292683 1.000000 ## 8 p01 18.67401 &quot; 1.46000&quot; ## 9 p05 49.72075 &quot; 3.30000&quot; ## 10 p10 66.28051 &quot; 5.60000&quot; ## 11 p25 76.37177 12.50000 ## 12 p50 86.95932 24.00000 ## 13 p75 94.68619 35.50000 ## 14 p90 97.52332 42.40000 ## 15 p95 99.47459 44.70000 ## 16 p99 100.5244 &quot; 46.5400&quot; ## 17 max 100.898 &quot; 47.000&quot; 2.4.1.1.3 A Dataset with Multiple Variables #load in data empirically by hand txt_test_data &lt;- &quot;init_prof, later_prof, class_id, exam_score &#39;SW&#39;, &#39;SW&#39;, 1, 102 &#39;SW&#39;, &#39;SW&#39;, 1, 102 &#39;SW&#39;, &#39;SW&#39;, 1, 101 &#39;SW&#39;, &#39;SW&#39;, 1, 100 &#39;SW&#39;, &#39;SW&#39;, 1, 100 &#39;SW&#39;, &#39;SW&#39;, 1, 99 &#39;SW&#39;, &#39;SW&#39;, 1, 98.5 &#39;SW&#39;, &#39;SW&#39;, 1, 98.5 &#39;SW&#39;, &#39;SW&#39;, 1, 97 &#39;SW&#39;, &#39;SW&#39;, 1, 95 &#39;SW&#39;, &#39;SW&#39;, 1, 94 &#39;SW&#39;, &#39;SW&#39;, 1, 91 &#39;SW&#39;, &#39;SW&#39;, 1, 91 &#39;SW&#39;, &#39;SW&#39;, 1, 90 &#39;SW&#39;, &#39;SW&#39;, 1, 89 &#39;SW&#39;, &#39;SW&#39;, 1, 88.5 &#39;SW&#39;, &#39;SW&#39;, 1, 88 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 87 &#39;SW&#39;, &#39;SW&#39;, 1, 86 &#39;SW&#39;, &#39;SW&#39;, 1, 86 &#39;SW&#39;, &#39;SW&#39;, 1, 84 &#39;SW&#39;, &#39;SW&#39;, 1, 82 &#39;SW&#39;, &#39;SW&#39;, 1, 78.5 &#39;SW&#39;, &#39;SW&#39;, 1, 76 &#39;SW&#39;, &#39;SW&#39;, 1, 72 &#39;SW&#39;, &#39;SW&#39;, 1, 70.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67.5 &#39;SW&#39;, &#39;SW&#39;, 1, 67 &#39;SW&#39;, &#39;SW&#39;, 1, 63.5 &#39;SW&#39;, &#39;SW&#39;, 1, 60 &#39;SW&#39;, &#39;SW&#39;, 1, 59 &#39;SW&#39;, &#39;SW&#39;, 1, 44.5 &#39;SW&#39;, &#39;SW&#39;, 1, 44 &#39;SW&#39;, &#39;SW&#39;, 1, 42.5 &#39;SW&#39;, &#39;SW&#39;, 1, 40.5 &#39;SW&#39;, &#39;SW&#39;, 1, 40.5 &#39;SW&#39;, &#39;SW&#39;, 1, 36.5 &#39;SW&#39;, &#39;SW&#39;, 1, 35.5 &#39;SW&#39;, &#39;SW&#39;, 1, 21.5 &#39;SW&#39;, &#39;SW&#39;, 1, 4 &#39;MP&#39;, &#39;MP&#39;, 2, 105 &#39;MP&#39;, &#39;MP&#39;, 2, 103 &#39;MP&#39;, &#39;MP&#39;, 2, 102 &#39;MP&#39;, &#39;MP&#39;, 2, 101 &#39;MP&#39;, &#39;MP&#39;, 2, 101 &#39;MP&#39;, &#39;MP&#39;, 2, 100.5 &#39;MP&#39;, &#39;MP&#39;, 2, 100 &#39;MP&#39;, &#39;MP&#39;, 2, 99 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 97 &#39;MP&#39;, &#39;MP&#39;, 2, 96 &#39;MP&#39;, &#39;MP&#39;, 2, 95 &#39;MP&#39;, &#39;MP&#39;, 2, 91 &#39;MP&#39;, &#39;MP&#39;, 2, 89 &#39;MP&#39;, &#39;MP&#39;, 2, 85 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 84 &#39;MP&#39;, &#39;MP&#39;, 2, 83.5 &#39;MP&#39;, &#39;MP&#39;, 2, 82.5 &#39;MP&#39;, &#39;MP&#39;, 2, 81.5 &#39;MP&#39;, &#39;MP&#39;, 2, 80.5 &#39;MP&#39;, &#39;MP&#39;, 2, 80 &#39;MP&#39;, &#39;MP&#39;, 2, 77 &#39;MP&#39;, &#39;MP&#39;, 2, 77 &#39;MP&#39;, &#39;MP&#39;, 2, 75 &#39;MP&#39;, &#39;MP&#39;, 2, 75 &#39;MP&#39;, &#39;MP&#39;, 2, 71 &#39;MP&#39;, &#39;MP&#39;, 2, 70 &#39;MP&#39;, &#39;MP&#39;, 2, 68 &#39;MP&#39;, &#39;MP&#39;, 2, 63 &#39;MP&#39;, &#39;MP&#39;, 2, 56 &#39;MP&#39;, &#39;MP&#39;, 2, 56 &#39;MP&#39;, &#39;MP&#39;, 2, 55.5 &#39;MP&#39;, &#39;MP&#39;, 2, 49.5 &#39;MP&#39;, &#39;MP&#39;, 2, 48.5 &#39;MP&#39;, &#39;MP&#39;, 2, 47.5 &#39;MP&#39;, &#39;MP&#39;, 2, 44.5 &#39;MP&#39;, &#39;MP&#39;, 2, 34.5 &#39;MP&#39;, &#39;MP&#39;, 2, 29.5 &#39;CA&#39;, &#39;MP&#39;, 3, 103 &#39;CA&#39;, &#39;MP&#39;, 3, 103 &#39;CA&#39;, &#39;MP&#39;, 3, 101 &#39;CA&#39;, &#39;MP&#39;, 3, 96.5 &#39;CA&#39;, &#39;MP&#39;, 3, 93.5 &#39;CA&#39;, &#39;MP&#39;, 3, 93 &#39;CA&#39;, &#39;MP&#39;, 3, 93 &#39;CA&#39;, &#39;MP&#39;, 3, 92 &#39;CA&#39;, &#39;MP&#39;, 3, 90 &#39;CA&#39;, &#39;MP&#39;, 3, 90 &#39;CA&#39;, &#39;MP&#39;, 3, 89 &#39;CA&#39;, &#39;MP&#39;, 3, 86.5 &#39;CA&#39;, &#39;MP&#39;, 3, 84.5 &#39;CA&#39;, &#39;MP&#39;, 3, 83 &#39;CA&#39;, &#39;MP&#39;, 3, 83 &#39;CA&#39;, &#39;MP&#39;, 3, 82 &#39;CA&#39;, &#39;MP&#39;, 3, 78 &#39;CA&#39;, &#39;MP&#39;, 3, 75 &#39;CA&#39;, &#39;MP&#39;, 3, 74.5 &#39;CA&#39;, &#39;MP&#39;, 3, 70 &#39;CA&#39;, &#39;MP&#39;, 3, 54.5 &#39;CA&#39;, &#39;MP&#39;, 3, 52 &#39;CA&#39;, &#39;MP&#39;, 3, 50 &#39;CA&#39;, &#39;MP&#39;, 3, 42 &#39;CA&#39;, &#39;MP&#39;, 3, 36.5 &#39;CA&#39;, &#39;MP&#39;, 3, 28 &#39;CA&#39;, &#39;MP&#39;, 3, 26 &#39;CA&#39;, &#39;MP&#39;, 3, 11 &#39;CA&#39;, &#39;SN&#39;, 4, 103 &#39;CA&#39;, &#39;SN&#39;, 4, 103 &#39;CA&#39;, &#39;SN&#39;, 4, 102 &#39;CA&#39;, &#39;SN&#39;, 4, 102 &#39;CA&#39;, &#39;SN&#39;, 4, 101 &#39;CA&#39;, &#39;SN&#39;, 4, 100 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 98 &#39;CA&#39;, &#39;SN&#39;, 4, 95 &#39;CA&#39;, &#39;SN&#39;, 4, 95 &#39;CA&#39;, &#39;SN&#39;, 4, 92.5 &#39;CA&#39;, &#39;SN&#39;, 4, 92 &#39;CA&#39;, &#39;SN&#39;, 4, 91 &#39;CA&#39;, &#39;SN&#39;, 4, 90 &#39;CA&#39;, &#39;SN&#39;, 4, 85.5 &#39;CA&#39;, &#39;SN&#39;, 4, 84 &#39;CA&#39;, &#39;SN&#39;, 4, 82.5 &#39;CA&#39;, &#39;SN&#39;, 4, 81 &#39;CA&#39;, &#39;SN&#39;, 4, 77.5 &#39;CA&#39;, &#39;SN&#39;, 4, 77 &#39;CA&#39;, &#39;SN&#39;, 4, 72 &#39;CA&#39;, &#39;SN&#39;, 4, 71.5 &#39;CA&#39;, &#39;SN&#39;, 4, 69 &#39;CA&#39;, &#39;SN&#39;, 4, 68.5 &#39;CA&#39;, &#39;SN&#39;, 4, 68 &#39;CA&#39;, &#39;SN&#39;, 4, 67 &#39;CA&#39;, &#39;SN&#39;, 4, 65.5 &#39;CA&#39;, &#39;SN&#39;, 4, 62.5 &#39;CA&#39;, &#39;SN&#39;, 4, 62 &#39;CA&#39;, &#39;SN&#39;, 4, 61.5 &#39;CA&#39;, &#39;SN&#39;, 4, 61 &#39;CA&#39;, &#39;SN&#39;, 4, 57.5 &#39;CA&#39;, &#39;SN&#39;, 4, 54 &#39;CA&#39;, &#39;SN&#39;, 4, 52.5 &#39;CA&#39;, &#39;SN&#39;, 4, 51 &#39;CA&#39;, &#39;SN&#39;, 4, 50.5 &#39;CA&#39;, &#39;SN&#39;, 4, 50 &#39;CA&#39;, &#39;SN&#39;, 4, 49 &#39;CA&#39;, &#39;SN&#39;, 4, 43 &#39;CA&#39;, &#39;SN&#39;, 4, 39.5 &#39;CA&#39;, &#39;SN&#39;, 4, 32.5 &#39;CA&#39;, &#39;SN&#39;, 4, 25.5 &#39;CA&#39;, &#39;SN&#39;, 4, 18&quot; csv_test_data = read.csv(text=txt_test_data, header=TRUE) ar_st_varnames &lt;- c(&#39;first_half_professor&#39;, &#39;second_half_professor&#39;, &#39;course_id&#39;, &#39;exam_score&#39;) tb_test_data &lt;- as_tibble(csv_test_data) %&gt;% rename_all(~c(ar_st_varnames)) summary(tb_test_data) ## first_half_professor second_half_professor course_id exam_score ## &#39;CA&#39;:72 &#39;MP&#39;:70 Min. :1.000 Min. : 4.00 ## &#39;MP&#39;:42 &#39;SN&#39;:44 1st Qu.:1.000 1st Qu.: 60.00 ## &#39;SW&#39;:43 &#39;SW&#39;:43 Median :2.000 Median : 82.00 ## Mean :2.465 Mean : 75.08 ## 3rd Qu.:4.000 3rd Qu.: 94.00 ## Max. :4.000 Max. :105.00 2.4.1.2 Test Score Distributions 2.4.1.2.1 Histogram ggplot(tb_final_twovar, aes(x=ar_test_scores_ec3)) + geom_histogram(bins=25) + labs(title = paste0(&#39;Sandbox: Final Distribution (Econ 2370, FW)&#39;), caption = &#39;FW Section, formula: 0.3*exam1Perc + 0.3*exam2Perc + 0.42*HWtotalPerc + 0.03*AttendancePerc \\n+ perfect attendance + 0.03 per Extra Credit&#39;) + theme_bw() ggplot(tb_test_data, aes(x=exam_score)) + geom_histogram(bins=16) + labs(title = paste0(&#39;Exam Distribution&#39;), caption = &#39;All Sections&#39;) + theme_bw() 2.4.2 Joint Quantiles from Continuous Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. There are multiple or a single continuous variables. Find which quantile each observation belongs to for each of the variables. Then also generate a joint/interaction variable of all combinations of quantiles from different variables. The program has these features: Quantiles breaks are generated based on group_by characteristics, meaning quantiles for individual level characteristics when data is panel Quantiles variables apply to full panel at within-group observation levels. Robust to non-unique breaks for quantiles (non-unique grouped together) Quantile categories have detailed labeling (specifying which non-unique groupings belong to quantile) When joining multiple quantile variables together: First check if only calculate quantiles at observations where all quantile base variables are not null Calculate Quantiles for each variable, with different quantile levels for sub-groups of variables Summary statistics by mulltiple quantile-categorical variables, summary 2.4.2.1 Build Program 2.4.2.1.1 Support Functions # Quantiles for any variable gen_quantiles &lt;- function(var, df, prob=c(0.25, 0.50, 0.75)) { enframe(quantile(as.numeric(df[[var]]), prob, na.rm=TRUE), &#39;quant.perc&#39;, var) } # Support Functions for Variable Suffix f_Q_suffix &lt;- function(seq.quantiles) { quantile.suffix &lt;- paste0(&#39;Qs&#39;, min(seq.quantiles), &#39;e&#39;, max(seq.quantiles), &#39;n&#39;, (length(seq.quantiles)-1)) } # Support Functions for Quantile Labeling f_Q_label &lt;- function(arr.quantiles, arr.sort.unique.quantile, seq.quantiles) { paste0(&#39;(&#39;, paste0(which(arr.quantiles %in% arr.sort.unique.quantile), collapse=&#39;,&#39;), &#39;) of &#39;, f_Q_suffix(seq.quantiles)) } # Generate New Variable Names with Quantile Suffix f_var_rename &lt;- function(name, seq.quantiles) { quantile.suffix &lt;- paste0(&#39;_&#39;, f_Q_suffix(seq.quantiles)) return(sub(&#39;_q&#39;, quantile.suffix, name)) } # Check Are Values within Group By Unique? If not, STOP f_check_distinct_ingroup &lt;- function(df, vars.group_by, vars.values_in_group) { df.uniqus.in.group &lt;- df %&gt;% group_by(!!!syms(vars.group_by)) %&gt;% mutate(quant_vars_paste = paste(!!!(syms(vars.values_in_group)), sep=&#39;-&#39;)) %&gt;% mutate(unique_in_group = n_distinct(quant_vars_paste)) %&gt;% slice(1L) %&gt;% ungroup() %&gt;% group_by(unique_in_group) %&gt;% summarise(n=n()) if (sum(df.uniqus.in.group$unique_in_group) &gt; 1) { print(df.uniqus.in.group) print(paste(&#39;vars.values_in_group&#39;, vars.values_in_group, sep=&#39;:&#39;)) print(paste(&#39;vars.group_by&#39;, vars.group_by, sep=&#39;:&#39;)) stop(&quot;The variables for which quantiles are to be taken are not identical within the group variables&quot;) } } 2.4.2.1.2 Data Slicing and Quantile Generation Function 1: generate quantiles based on group-specific characteristics. the groups could be at the panel observation level as well. # First Step, given groups, generate quantiles based on group characteristics # vars.cts2quantile &lt;- c(&#39;wealthIdx&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # seq.quantiles &lt;- c(0, 0.3333, 0.6666, 1.0) # vars.group_by &lt;- c(&#39;indi.id&#39;) # vars.arrange &lt;- c(&#39;indi.id&#39;, &#39;svymthRound&#39;) # vars.continuous &lt;- c(&#39;wealthIdx&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) df_sliced_quantiles &lt;- function(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange) { # Slicing data df.grp.L1 &lt;- df %&gt;% group_by(!!!syms(vars.group_by)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% slice(1L) %&gt;% ungroup() # Quantiles based on sliced data df.sliced.quantiles &lt;- lapply(vars.cts2quantile, gen_quantiles, df=df.grp.L1, prob=seq.quantiles) %&gt;% reduce(full_join) return(list(df.sliced.quantiles=df.sliced.quantiles, df.grp.L1=df.grp.L1)) } 2.4.2.1.3 Data Cutting Function 2: cut groups for full panel dataframe based on group-specific characteristics quantiles. # Cutting Function, Cut Continuous Variables into Quantiles with labeing f_cut &lt;- function(var, df.sliced.quantiles, seq.quantiles, include.lowest=TRUE, fan.labels=TRUE, print=FALSE) { # unparsed string variable name var.str &lt;- substitute(var) # Breaks arr.quantiles &lt;- df.sliced.quantiles[[var.str]] arr.sort.unique.quantiles &lt;- sort(unique(arr.quantiles)) if (print) { print(arr.sort.unique.quantiles) } # Regular cutting With Standard Labels # TRUE, means the lowest group has closed bracket left and right var.quantile &lt;- cut(var, breaks=arr.sort.unique.quantiles, include.lowest=include.lowest) # Use my custom labels if (fan.labels) { levels.suffix &lt;- lapply(arr.sort.unique.quantiles[1:(length(arr.sort.unique.quantiles)-1)], f_Q_label, arr.quantiles=arr.quantiles, seq.quantiles=seq.quantiles) if (print) { print(levels.suffix) } levels(var.quantile) &lt;- paste0(levels(var.quantile), &#39;; &#39;, levels.suffix) } # Return return(var.quantile) } # Combo Quantile Function # vars.cts2quantile &lt;- c(&#39;wealthIdx&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # seq.quantiles &lt;- c(0, 0.3333, 0.6666, 1.0) # vars.group_by &lt;- c(&#39;indi.id&#39;) # vars.arrange &lt;- c(&#39;indi.id&#39;, &#39;svymthRound&#39;) # vars.continuous &lt;- c(&#39;wealthIdx&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) df_cut_by_sliced_quantiles &lt;- function(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange) { # Check Are Values within Group By Unique? If not, STOP f_check_distinct_ingroup(df, vars.group_by, vars.values_in_group=vars.cts2quantile) # First Step Slicing df.sliced &lt;- df_sliced_quantiles(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange) # Second Step Generate Categorical Variables of Quantiles df.with.cut.quant &lt;- df %&gt;% mutate_at(vars.cts2quantile, funs(q=f_cut(., df.sliced$df.sliced.quantiles, seq.quantiles=seq.quantiles, include.lowest=TRUE, fan.labels=TRUE))) if (length(vars.cts2quantile) &gt; 1) { df.with.cut.quant &lt;- df.with.cut.quant %&gt;% rename_at(vars(contains(&#39;_q&#39;)), funs(f_var_rename(., seq.quantiles=seq.quantiles))) } else { new.var.name &lt;- paste0(vars.cts2quantile[1], &#39;_&#39;, f_Q_suffix(seq.quantiles)) df.with.cut.quant &lt;- df.with.cut.quant %&gt;% rename(!!new.var.name := q) } # Newly Generated Quantile-Cut Variables vars.quantile.cut &lt;- df.with.cut.quant %&gt;% select(matches(paste0(vars.cts2quantile, collapse=&#39;|&#39;))) %&gt;% select(matches(f_Q_suffix(seq.quantiles))) # Return return(list(df.with.cut.quant = df.with.cut.quant, df.sliced.quantiles=df.sliced$df.sliced.quantiles, df.grp.L1=df.sliced$df.grp.L1, vars.quantile.cut=vars.quantile.cut)) } 2.4.2.1.4 Different Vars Different Probabilities Joint Quantiles Accomondate multiple continuousv ariables Different percentiles list of lists generate joint categorical variables keep only values that exist for all quantile base vars # Function to handle list inputs with different quantiles vars and probabilities df_cut_by_sliced_quantiles_grps &lt;- function(quantile.grp.list, df, vars.group_by, vars.arrange) { vars.cts2quantile &lt;- quantile.grp.list$vars seq.quantiles &lt;- quantile.grp.list$prob return(df_cut_by_sliced_quantiles(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange)) } # Show Results df_cut_by_sliced_quantiles_joint_results_grped &lt;- function(df.with.cut.quant.all, vars.cts2quantile, vars.group_by, vars.arrange, vars.quantile.cut.all, var.qjnt.grp.idx) { # Show ALL df.group.panel.cnt.mean &lt;- df.with.cut.quant.all %&gt;% group_by(!!!syms(vars.quantile.cut.all), !!sym(var.qjnt.grp.idx)) %&gt;% summarise_at(vars.cts2quantile, funs(mean, n())) # Show Based on SLicing first df.group.slice1.cnt.mean &lt;- df.with.cut.quant.all %&gt;% group_by(!!!syms(vars.group_by)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% slice(1L) %&gt;% group_by(!!!syms(vars.quantile.cut.all), !!sym(var.qjnt.grp.idx)) %&gt;% summarise_at(vars.cts2quantile, funs(mean, n())) return(list(df.group.panel.cnt.mean=df.group.panel.cnt.mean, df.group.slice1.cnt.mean=df.group.slice1.cnt.mean)) } # # Joint Quantile Group Name # var.qjnt.grp.idx &lt;- &#39;group.index&#39; # # Generate Categorical Variables of Quantiles # vars.group_by &lt;- c(&#39;indi.id&#39;) # vars.arrange &lt;- c(&#39;indi.id&#39;, &#39;svymthRound&#39;) # # Quantile Variables and Quantiles # vars.cts2quantile.wealth &lt;- c(&#39;wealthIdx&#39;) # seq.quantiles.wealth &lt;- c(0, .5, 1.0) # vars.cts2quantile.wgthgt &lt;- c(&#39;hgt0&#39;, &#39;wgt0&#39;) # seq.quantiles.wgthgt &lt;- c(0, .3333, 0.6666, 1.0) # drop.any.quantile.na &lt;- TRUE # # collect to list # list.cts2quantile &lt;- list(list(vars=vars.cts2quantile.wealth, # prob=seq.quantiles.wealth), # list(vars=vars.cts2quantile.wgthgt, # prob=seq.quantiles.wgthgt)) df_cut_by_sliced_quantiles_joint &lt;- function(df, var.qjnt.grp.idx, list.cts2quantile, vars.group_by, vars.arrange, drop.any.quantile.na = TRUE, toprint = TRUE) { # Original dimensions if(toprint) { print(dim(df)) } # All Continuous Variables from lists vars.cts2quantile &lt;- unlist(lapply(list.cts2quantile, function(elist) elist$vars)) vars.cts2quantile # Keep only if not NA for all Quantile variables if (drop.any.quantile.na) { df.select &lt;- df %&gt;% drop_na(c(vars.group_by, vars.arrange, vars.cts2quantile)) } else { df.select &lt;- df } if(toprint) { print(dim(df.select)) } # Apply qunatile function to all elements of list of list df.cut.list &lt;- lapply(list.cts2quantile, df_cut_by_sliced_quantiles_grps, df=df.select, vars.group_by=vars.group_by, vars.arrange=vars.arrange) # Reduce Resulting Core Panel Matrix Together df.with.cut.quant.all &lt;- lapply(df.cut.list, function(elist) elist$df.with.cut.quant) %&gt;% reduce(left_join) df.sliced.quantiles.all &lt;- lapply(df.cut.list, function(elist) elist$df.sliced.quantiles) if(toprint) { print(dim(df.with.cut.quant.all)) } # Obrain Newly Created Quantile Group Variables vars.quantile.cut.all &lt;- unlist(lapply(df.cut.list, function(elist) names(elist$vars.quantile.cut))) if(toprint) { print(vars.quantile.cut.all) print(summary(df.with.cut.quant.all %&gt;% select(one_of(vars.quantile.cut.all)))) } # Generate Joint Quantile Index Variable df.with.cut.quant.all &lt;- df.with.cut.quant.all %&gt;% mutate(!!var.qjnt.grp.idx := group_indices(., !!!syms(vars.quantile.cut.all))) # Quantile Groups arr.group.idx &lt;- t(sort(unique(df.with.cut.quant.all[[var.qjnt.grp.idx]]))) # Results Display df.group.print &lt;- df_cut_by_sliced_quantiles_joint_results_grped(df.with.cut.quant.all, vars.cts2quantile, vars.group_by, vars.arrange, vars.quantile.cut.all, var.qjnt.grp.idx) # list to Return # These returns are the same as returns earlier: df_cut_by_sliced_quantiles # Except that they are combined together return(list(df.with.cut.quant = df.with.cut.quant.all, df.sliced.quantiles = df.sliced.quantiles.all, df.grp.L1 = (df.cut.list[[1]])$df.grp.L1, vars.quantile.cut = vars.quantile.cut.all, df.group.panel.cnt.mean = df.group.print$df.group.panel.cnt.mean, df.group.slice1.cnt.mean = df.group.print$df.group.slice1.cnt.mean)) } 2.4.2.2 Program Testing Load Data # Library library(tidyverse) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) ## Parsed with column specification: ## cols( ## S.country = col_character(), ## vil.id = col_double(), ## indi.id = col_double(), ## sex = col_character(), ## svymthRound = col_double(), ## momEdu = col_double(), ## wealthIdx = col_double(), ## hgt = col_double(), ## wgt = col_double(), ## hgt0 = col_double(), ## wgt0 = col_double(), ## prot = col_double(), ## cal = col_double(), ## p.A.prot = col_double(), ## p.A.nProt = col_double() ## ) 2.4.2.2.1 Hgt0 3 Groups # Joint Quantile Group Name var.qjnt.grp.idx &lt;- &#39;group.index&#39; list.cts2quantile &lt;- list(list(vars=c(&#39;hgt0&#39;), prob=c(0, .3333, 0.6666, 1.0))) results &lt;- df_cut_by_sliced_quantiles_joint(df, var.qjnt.grp.idx, list.cts2quantile, vars.group_by = c(&#39;indi.id&#39;), vars.arrange = c(&#39;indi.id&#39;, &#39;svymthRound&#39;), drop.any.quantile.na = TRUE, toprint = FALSE) # Show Results results$df.group.slice1.cnt.mean ## # A tibble: 3 x 4 ## # Groups: hgt0_Qs0e1n3 [3] ## hgt0_Qs0e1n3 group.index mean n ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 [40.6,48.5]; (1) of Qs0e1n3 1 47.0 580 ## 2 (48.5,50.2]; (2) of Qs0e1n3 2 49.4 561 ## 3 (50.2,58]; (3) of Qs0e1n3 3 51.7 568 2.4.2.2.2 Wealth 5 Groups Guatemala # Joint Quantile Group Name var.qjnt.grp.idx &lt;- &#39;wltQuintle.index&#39; list.cts2quantile &lt;- list(list(vars=c(&#39;wealthIdx&#39;), prob=seq(0, 1.0, 0.20))) results &lt;- df_cut_by_sliced_quantiles_joint((df %&gt;% filter(S.country == &#39;Guatemala&#39;)), var.qjnt.grp.idx, list.cts2quantile, vars.group_by = c(&#39;indi.id&#39;), vars.arrange = c(&#39;indi.id&#39;, &#39;svymthRound&#39;), drop.any.quantile.na = TRUE, toprint = FALSE) # Show Results results$df.group.slice1.cnt.mean ## # A tibble: 5 x 4 ## # Groups: wealthIdx_Qs0e1n5 [5] ## wealthIdx_Qs0e1n5 wltQuintle.index mean n ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 [1,1.6]; (1) of Qs0e1n5 1 1.25 151 ## 2 (1.6,2.1]; (2) of Qs0e1n5 2 1.82 139 ## 3 (2.1,2.3]; (3) of Qs0e1n5 3 2.25 139 ## 4 (2.3,2.9]; (4) of Qs0e1n5 4 2.70 134 ## 5 (2.9,6.6]; (5) of Qs0e1n5 5 3.77 111 2.4.2.2.3 Hgt0 2 groups, Wgt0 2 groups too # Joint Quantile Group Name var.qjnt.grp.idx &lt;- &#39;group.index&#39; list.cts2quantile &lt;- list(list(vars=c(&#39;hgt0&#39;, &#39;wgt0&#39;), prob=c(0, .5, 1.0))) results &lt;- df_cut_by_sliced_quantiles_joint(df, var.qjnt.grp.idx, list.cts2quantile, vars.group_by = c(&#39;indi.id&#39;), vars.arrange = c(&#39;indi.id&#39;, &#39;svymthRound&#39;), drop.any.quantile.na = TRUE, toprint = FALSE) ## Joining, by = &quot;quant.perc&quot; # Show Results results$df.group.slice1.cnt.mean ## # A tibble: 4 x 7 ## # Groups: hgt0_Qs0e1n2, wgt0_Qs0e1n2 [4] ## hgt0_Qs0e1n2 wgt0_Qs0e1n2 group.index hgt0_mean wgt0_mean hgt0_n wgt0_n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 [40.6,49.4]; (1) of Qs0e1n2 [1.4e+03,3.01e+03]; (1) of Qs0e1n2 1 47.4 2650. 652 652 ## 2 [40.6,49.4]; (1) of Qs0e1n2 (3.01e+03,5.49e+03]; (2) of Qs0e1n2 2 48.5 3244. 228 228 ## 3 (49.4,58]; (2) of Qs0e1n2 [1.4e+03,3.01e+03]; (1) of Qs0e1n2 3 50.4 2829. 202 202 ## 4 (49.4,58]; (2) of Qs0e1n2 (3.01e+03,5.49e+03]; (2) of Qs0e1n2 4 51.3 3483. 626 626 2.4.2.2.4 Hgt0 2 groups, Wealth 2 groups, Cebu Only # Joint Quantile Group Name var.qjnt.grp.idx &lt;- &#39;group.index&#39; list.cts2quantile &lt;- list(list(vars=c(&#39;wealthIdx&#39;), prob=c(0, .5, 1.0)), list(vars=c(&#39;hgt0&#39;), prob=c(0, .333, 0.666, 1.0))) results &lt;- df_cut_by_sliced_quantiles_joint((df %&gt;% filter(S.country == &#39;Cebu&#39;)), var.qjnt.grp.idx, list.cts2quantile, vars.group_by = c(&#39;indi.id&#39;), vars.arrange = c(&#39;indi.id&#39;, &#39;svymthRound&#39;), drop.any.quantile.na = TRUE, toprint = FALSE) ## Joining, by = c(&quot;S.country&quot;, &quot;vil.id&quot;, &quot;indi.id&quot;, &quot;sex&quot;, &quot;svymthRound&quot;, &quot;momEdu&quot;, &quot;wealthIdx&quot;, &quot;hgt&quot;, &quot;wgt&quot;, &quot;hgt0&quot;, &quot;wgt0&quot;, &quot;prot&quot;, &quot;cal&quot;, &quot;p.A.prot&quot;, ## &quot;p.A.nProt&quot;) # Show Results results$df.group.slice1.cnt.mean ## # A tibble: 6 x 7 ## # Groups: wealthIdx_Qs0e1n2, hgt0_Qs0e1n3 [6] ## wealthIdx_Qs0e1n2 hgt0_Qs0e1n3 group.index wealthIdx_mean hgt0_mean wealthIdx_n hgt0_n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 [5.2,8.3]; (1) of Qs0e1n2 [41.1,48.4]; (1) of Qs0e1n3 1 7.15 46.9 270 270 ## 2 [5.2,8.3]; (1) of Qs0e1n2 (48.4,50.1]; (2) of Qs0e1n3 2 7.18 49.2 269 269 ## 3 [5.2,8.3]; (1) of Qs0e1n2 (50.1,58]; (3) of Qs0e1n3 3 7.13 51.3 236 236 ## 4 (8.3,19.3]; (2) of Qs0e1n2 [41.1,48.4]; (1) of Qs0e1n3 4 11.1 47.2 179 179 ## 5 (8.3,19.3]; (2) of Qs0e1n2 (48.4,50.1]; (2) of Qs0e1n3 5 11.2 49.3 185 185 ## 6 (8.3,19.3]; (2) of Qs0e1n2 (50.1,58]; (3) of Qs0e1n3 6 11.6 51.7 207 207 2.4.2.2.5 Results of income + Wgt0 + Hgt0 joint Gruops in Cebu Weight at month 0 below and above median, height at month zero into three terciles. # Joint Quantile Group Name var.qjnt.grp.idx &lt;- &#39;wltHgt0Wgt0.index&#39; list.cts2quantile &lt;- list(list(vars=c(&#39;wealthIdx&#39;), prob=c(0, .5, 1.0)), list(vars=c(&#39;hgt0&#39;, &#39;wgt0&#39;), prob=c(0, .5, 1.0))) results &lt;- df_cut_by_sliced_quantiles_joint((df %&gt;% filter(S.country == &#39;Cebu&#39;)), var.qjnt.grp.idx, list.cts2quantile, vars.group_by = c(&#39;indi.id&#39;), vars.arrange = c(&#39;indi.id&#39;, &#39;svymthRound&#39;), drop.any.quantile.na = TRUE, toprint = FALSE) ## Joining, by = &quot;quant.perc&quot;Joining, by = c(&quot;S.country&quot;, &quot;vil.id&quot;, &quot;indi.id&quot;, &quot;sex&quot;, &quot;svymthRound&quot;, &quot;momEdu&quot;, &quot;wealthIdx&quot;, &quot;hgt&quot;, &quot;wgt&quot;, &quot;hgt0&quot;, &quot;wgt0&quot;, ## &quot;prot&quot;, &quot;cal&quot;, &quot;p.A.prot&quot;, &quot;p.A.nProt&quot;) # Show Results results$df.group.slice1.cnt.mean ## # A tibble: 8 x 10 ## # Groups: wealthIdx_Qs0e1n2, hgt0_Qs0e1n2, wgt0_Qs0e1n2 [8] ## wealthIdx_Qs0e1n2 hgt0_Qs0e1n2 wgt0_Qs0e1n2 wltHgt0Wgt0.ind~ wealthIdx_mean hgt0_mean wgt0_mean wealthIdx_n hgt0_n wgt0_n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [5.2,8.3]; (1) of Qs0e~ [41.1,49.2]; (1) of Qs~ [1.4e+03,2.98e+03]; (1) of ~ 1 7.16 47.3 2607. 308 308 308 ## 2 [5.2,8.3]; (1) of Qs0e~ [41.1,49.2]; (1) of Qs~ (2.98e+03,5.49e+03]; (2) of~ 2 7.27 48.4 3156. 102 102 102 ## 3 [5.2,8.3]; (1) of Qs0e~ (49.2,58]; (2) of Qs0e~ [1.4e+03,2.98e+03]; (1) of ~ 3 7.00 50.2 2781. 97 97 97 ## 4 [5.2,8.3]; (1) of Qs0e~ (49.2,58]; (2) of Qs0e~ (2.98e+03,5.49e+03]; (2) of~ 4 7.16 51.0 3328. 268 268 268 ## 5 (8.3,19.3]; (2) of Qs0~ [41.1,49.2]; (1) of Qs~ [1.4e+03,2.98e+03]; (1) of ~ 5 10.9 47.4 2632. 186 186 186 ## 6 (8.3,19.3]; (2) of Qs0~ [41.1,49.2]; (1) of Qs~ (2.98e+03,5.49e+03]; (2) of~ 6 11.3 48.5 3196. 81 81 81 ## 7 (8.3,19.3]; (2) of Qs0~ (49.2,58]; (2) of Qs0e~ [1.4e+03,2.98e+03]; (1) of ~ 7 11.3 50.2 2779. 82 82 82 ## 8 (8.3,19.3]; (2) of Qs0~ (49.2,58]; (2) of Qs0e~ (2.98e+03,5.49e+03]; (2) of~ 8 11.7 51.4 3431. 222 222 222 2.4.2.3 Line by Line–Quantiles Var by Var The idea of the function is to generate quantiles levels first, and then use those to generate the categories based on quantiles. Rather than doing this in one step. These are done in two steps, to increase clarity in the quantiles used for quantile category generation. And a dataframe with these quantiles are saved as a separate output of the function. 2.4.2.3.1 Dataframe of Variables’ Group-by Level Quantiles Quantiles from Different Variables. Note that these variables are specific to the individual, not individual/month. So we need to first slick the data, so that we only get the first rows. Do this in several steps to clarify group_by level. No speed loss. # Selected Variables, many Percentiles vars.group_by &lt;- c(&#39;indi.id&#39;) vars.arrange &lt;- c(&#39;indi.id&#39;, &#39;svymthRound&#39;) vars.cts2quantile &lt;- c(&#39;wealthIdx&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) seq.quantiles &lt;- c(0, 0.3333, 0.6666, 1.0) df.sliced &lt;- df_sliced_quantiles(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange) ## Joining, by = &quot;quant.perc&quot;Joining, by = &quot;quant.perc&quot; df.sliced.quantiles &lt;- df.sliced$df.sliced.quantiles df.grp.L1 &lt;- df.sliced$df.grp.L1 df.sliced.quantiles ## # A tibble: 4 x 4 ## quant.perc wealthIdx hgt0 wgt0 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0% 1 40.6 1402. ## 2 33.33% 5.2 48.5 2843. ## 3 66.66% 8.3 50.2 3209. ## 4 100% 19.3 58 5494. # Quantiles all Variables suppressMessages(lapply(names(df), gen_quantiles, df=df.grp.L1, prob=seq(0.1,0.9,0.10)) %&gt;% reduce(full_join)) ## Warning in quantile(as.numeric(df[[var]]), prob, na.rm = TRUE): NAs introduced by coercion ## Warning in quantile(as.numeric(df[[var]]), prob, na.rm = TRUE): NAs introduced by coercion ## # A tibble: 9 x 16 ## quant.perc S.country vil.id indi.id sex svymthRound momEdu wealthIdx hgt wgt hgt0 wgt0 prot cal p.A.prot p.A.nProt ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10% NA 3 203. NA 0 5.7 1.7 46.3 1397. 46.6 2500. 0.5 0.5 24.3 0.5 ## 2 20% NA 4 405. NA 0 6.9 2.3 47.3 1840. 47.7 2686. 0.5 0.5 172. 0.5 ## 3 30% NA 6 608. NA 0 7.7 3.3 48 2272. 48.3 2804. 0.5 0.5 721. 1.06 ## 4 40% NA 8 810. NA 0 8.6 6.3 48.7 2669. 48.8 2910. 0.5 0.5 1010. 19 ## 5 50% NA 9 1012 NA 0 9.3 7.3 49.4 3050. 49.4 3013 0.5 0.5 1273. 111. ## 6 60% NA 13 1214. NA 0 10.4 8.3 49.9 3440. 49.9 3126. 0.5 3.88 1614. 222. ## 7 70% NA 14 1416. NA 0 11.4 8.3 50.5 3857. 50.4 3250. 0.7 8.26 2680. 257. ## 8 80% NA 17 1619. NA 0 12.7 9.3 51.2 4258. 51.0 3418. 1.2 11.5 4761. 298. ## 9 90% NA 26 1821. NA 0 14.6 11.3 52.3 4704. 52 3683. 1.6 15.6 10868. 365. 2.4.2.3.2 Cut Quantile Categorical Variables Using the Quantiles we have generate, cut the continuous variables to generate categorical quantile variables in the full dataframe. Note that we can only cut based on unique breaks, but sometimes quantile break-points are the same if some values are often observed, and also if there are too few observations with respect to quantile groups. To resolve this issue, we only look at unique quantiles. We need several support Functions: 1. support functions to generate suffix for quantile variables based on quantile cuts 2. support for labeling variables of resulting quantiles beyond bracketing # Function Testing arr.quantiles &lt;- df.sliced.quantiles[[substitute(&#39;wealthIdx&#39;)]] arr.quantiles ## [1] 1.0 5.2 8.3 19.3 arr.sort.unique.quantiles &lt;- sort(unique(df.sliced.quantiles[[substitute(&#39;wealthIdx&#39;)]])) arr.sort.unique.quantiles ## [1] 1.0 5.2 8.3 19.3 f_Q_label(arr.quantiles, arr.sort.unique.quantiles[1], seq.quantiles) ## [1] &quot;(1) of Qs0e1n3&quot; f_Q_label(arr.quantiles, arr.sort.unique.quantiles[2], seq.quantiles) ## [1] &quot;(2) of Qs0e1n3&quot; lapply(arr.sort.unique.quantiles[1:(length(arr.sort.unique.quantiles)-1)], f_Q_label, arr.quantiles=arr.quantiles, seq.quantiles=seq.quantiles) ## [[1]] ## [1] &quot;(1) of Qs0e1n3&quot; ## ## [[2]] ## [1] &quot;(2) of Qs0e1n3&quot; ## ## [[3]] ## [1] &quot;(3) of Qs0e1n3&quot; # Generate Categorical Variables of Quantiles vars.group_by &lt;- c(&#39;indi.id&#39;) vars.arrange &lt;- c(&#39;indi.id&#39;, &#39;svymthRound&#39;) vars.cts2quantile &lt;- c(&#39;wealthIdx&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) seq.quantiles &lt;- c(0, 0.3333, 0.6666, 1.0) df.cut &lt;- df_cut_by_sliced_quantiles(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange) ## Joining, by = &quot;quant.perc&quot;Joining, by = &quot;quant.perc&quot; vars.quantile.cut &lt;- df.cut$vars.quantile.cut df.with.cut.quant &lt;- df.cut$df.with.cut.quant df.grp.L1 &lt;- df.cut$df.grp.L1 # Cut Variables Generated names(vars.quantile.cut) ## [1] &quot;wealthIdx_Qs0e1n3&quot; &quot;hgt0_Qs0e1n3&quot; &quot;wgt0_Qs0e1n3&quot; summary(vars.quantile.cut) ## wealthIdx_Qs0e1n3 hgt0_Qs0e1n3 wgt0_Qs0e1n3 ## [1,5.2]; (1) of Qs0e1n3 :10958 [40.6,48.5]; (1) of Qs0e1n3:10232 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 :10105 ## (5.2,8.3]; (2) of Qs0e1n3 :13812 (48.5,50.2]; (2) of Qs0e1n3: 9895 (2.84e+03,3.21e+03]; (2) of Qs0e1n3:10056 ## (8.3,19.3]; (3) of Qs0e1n3:10295 (50.2,58]; (3) of Qs0e1n3 : 9908 (3.21e+03,5.49e+03]; (3) of Qs0e1n3: 9858 ## NA&#39;s : 5030 NA&#39;s : 5046 # options(repr.matrix.max.rows=50, repr.matrix.max.cols=20) # df.with.cut.quant 2.4.2.3.3 Individual Variables’ Quantile Cuts Review Results # Group By Results f.count &lt;- function(df, var.cts, seq.quantiles) { df %&gt;% select(S.country, indi.id, svymthRound, matches(paste0(var.cts, collapse=&#39;|&#39;))) %&gt;% group_by(!!sym(f_var_rename(paste0(var.cts,&#39;_q&#39;), seq.quantiles))) %&gt;% summarise_all(funs(n=n())) } # Full Panel Results lapply(vars.cts2quantile, f.count, df=df.with.cut.quant, seq.quantiles=seq.quantiles) ## Warning: Factor `hgt0_Qs0e1n3` contains implicit NA, consider using `forcats::fct_explicit_na` ## Warning: Factor `wgt0_Qs0e1n3` contains implicit NA, consider using `forcats::fct_explicit_na` ## [[1]] ## # A tibble: 3 x 5 ## wealthIdx_Qs0e1n3 S.country_n indi.id_n svymthRound_n wealthIdx_n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [1,5.2]; (1) of Qs0e1n3 10958 10958 10958 10958 ## 2 (5.2,8.3]; (2) of Qs0e1n3 13812 13812 13812 13812 ## 3 (8.3,19.3]; (3) of Qs0e1n3 10295 10295 10295 10295 ## ## [[2]] ## # A tibble: 4 x 5 ## hgt0_Qs0e1n3 S.country_n indi.id_n svymthRound_n hgt0_n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [40.6,48.5]; (1) of Qs0e1n3 10232 10232 10232 10232 ## 2 (48.5,50.2]; (2) of Qs0e1n3 9895 9895 9895 9895 ## 3 (50.2,58]; (3) of Qs0e1n3 9908 9908 9908 9908 ## 4 &lt;NA&gt; 5030 5030 5030 5030 ## ## [[3]] ## # A tibble: 4 x 5 ## wgt0_Qs0e1n3 S.country_n indi.id_n svymthRound_n wgt0_n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 10105 10105 10105 10105 ## 2 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 10056 10056 10056 10056 ## 3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 9858 9858 9858 9858 ## 4 &lt;NA&gt; 5046 5046 5046 5046 # Results Individual Slice lapply(vars.cts2quantile, f.count, df=(df.with.cut.quant %&gt;% group_by(!!!syms(vars.group_by)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% slice(1L)), seq.quantiles = seq.quantiles) ## Warning: Factor `hgt0_Qs0e1n3` contains implicit NA, consider using `forcats::fct_explicit_na` ## Warning: Factor `wgt0_Qs0e1n3` contains implicit NA, consider using `forcats::fct_explicit_na` ## [[1]] ## # A tibble: 3 x 5 ## wealthIdx_Qs0e1n3 S.country_n indi.id_n svymthRound_n wealthIdx_n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [1,5.2]; (1) of Qs0e1n3 683 683 683 683 ## 2 (5.2,8.3]; (2) of Qs0e1n3 768 768 768 768 ## 3 (8.3,19.3]; (3) of Qs0e1n3 572 572 572 572 ## ## [[2]] ## # A tibble: 4 x 5 ## hgt0_Qs0e1n3 S.country_n indi.id_n svymthRound_n hgt0_n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [40.6,48.5]; (1) of Qs0e1n3 580 580 580 580 ## 2 (48.5,50.2]; (2) of Qs0e1n3 561 561 561 561 ## 3 (50.2,58]; (3) of Qs0e1n3 568 568 568 568 ## 4 &lt;NA&gt; 314 314 314 314 ## ## [[3]] ## # A tibble: 4 x 5 ## wgt0_Qs0e1n3 S.country_n indi.id_n svymthRound_n wgt0_n ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 569 569 569 569 ## 2 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 569 569 569 569 ## 3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 570 570 570 570 ## 4 &lt;NA&gt; 315 315 315 315 2.4.2.4 Differential Quantiles for Different Variables Then Combine to Form New Groups Collect together different quantile base variables and their percentile cuttings quantile rules. Input Parameters. # Generate Categorical Variables of Quantiles vars.group_by &lt;- c(&#39;indi.id&#39;) vars.arrange &lt;- c(&#39;indi.id&#39;, &#39;svymthRound&#39;) # Quantile Variables and Quantiles vars.cts2quantile.wealth &lt;- c(&#39;wealthIdx&#39;) seq.quantiles.wealth &lt;- c(0, .5, 1.0) vars.cts2quantile.wgthgt &lt;- c(&#39;hgt0&#39;, &#39;wgt0&#39;) seq.quantiles.wgthgt &lt;- c(0, .3333, 0.6666, 1.0) drop.any.quantile.na &lt;- TRUE # collect to list list.cts2quantile &lt;- list(list(vars=vars.cts2quantile.wealth, prob=seq.quantiles.wealth), list(vars=vars.cts2quantile.wgthgt, prob=seq.quantiles.wgthgt)) 2.4.2.5 Check if Within Group Variables Are The Same Need to make sure quantile variables are unique within groups vars.cts2quantile &lt;- unlist(lapply(list.cts2quantile, function(elist) elist$vars)) f_check_distinct_ingroup(df, vars.group_by, vars.values_in_group=vars.cts2quantile) 2.4.2.5.1 Keep only non-NA for all Quantile Variables # Original dimensions dim(df) ## [1] 35065 15 # All Continuous Variables from lists vars.cts2quantile &lt;- unlist(lapply(list.cts2quantile, function(elist) elist$vars)) vars.cts2quantile ## [1] &quot;wealthIdx&quot; &quot;hgt0&quot; &quot;wgt0&quot; # Keep only if not NA for all Quantile variables if (drop.any.quantile.na) { df.select &lt;- df %&gt;% drop_na(c(vars.group_by, vars.arrange, vars.cts2quantile)) } dim(df.select) ## [1] 30019 15 2.4.2.5.2 Apply Quantiles for Each Quantile Variable # Dealing with a list of quantile variables df.cut.wealth &lt;- df_cut_by_sliced_quantiles(df.select, vars.cts2quantile.wealth, seq.quantiles.wealth, vars.group_by, vars.arrange) summary(df.cut.wealth$vars.quantile.cut) ## wealthIdx_Qs0e1n2 ## [1,7.3]; (1) of Qs0e1n2 :14936 ## (7.3,19.3]; (2) of Qs0e1n2:15083 # summary((df.cut.wealth$df.with.cut.quant)[[&#39;wealthIdx_Qs0e1n2&#39;]]) # df.cut.wealth$df.with.cut.quant %&gt;% filter(is.na(wealthIdx_Qs0e1n2)) # df.cut.wealth$df.with.cut.quant %&gt;% filter(indi.id == 500) df.cut.wgthgt &lt;- df_cut_by_sliced_quantiles(df.select, vars.cts2quantile.wgthgt, seq.quantiles.wgthgt, vars.group_by, vars.arrange) ## Joining, by = &quot;quant.perc&quot; summary(df.cut.wgthgt$vars.quantile.cut) ## hgt0_Qs0e1n3 wgt0_Qs0e1n3 ## [40.6,48.5]; (1) of Qs0e1n3:10216 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 :10105 ## (48.5,50.2]; (2) of Qs0e1n3: 9895 (2.84e+03,3.21e+03]; (2) of Qs0e1n3:10056 ## (50.2,58]; (3) of Qs0e1n3 : 9908 (3.21e+03,5.49e+03]; (3) of Qs0e1n3: 9858 2.4.2.5.3 Apply Quantiles Functionally # Function to handle list inputs with different quantiles vars and probabilities df_cut_by_sliced_quantiles_grps &lt;- function(quantile.grp.list, df, vars.group_by, vars.arrange) { vars.cts2quantile &lt;- quantile.grp.list$vars seq.quantiles &lt;- quantile.grp.list$prob return(df_cut_by_sliced_quantiles(df, vars.cts2quantile, seq.quantiles, vars.group_by, vars.arrange)) } # Apply function df.cut.list &lt;- lapply(list.cts2quantile, df_cut_by_sliced_quantiles_grps, df=df.select, vars.group_by=vars.group_by, vars.arrange=vars.arrange) ## Joining, by = &quot;quant.perc&quot; # Reduce Resulting Matrixes Together df.with.cut.quant.all &lt;- lapply(df.cut.list, function(elist) elist$df.with.cut.quant) %&gt;% reduce(left_join) ## Joining, by = c(&quot;S.country&quot;, &quot;vil.id&quot;, &quot;indi.id&quot;, &quot;sex&quot;, &quot;svymthRound&quot;, &quot;momEdu&quot;, &quot;wealthIdx&quot;, &quot;hgt&quot;, &quot;wgt&quot;, &quot;hgt0&quot;, &quot;wgt0&quot;, &quot;prot&quot;, &quot;cal&quot;, &quot;p.A.prot&quot;, ## &quot;p.A.nProt&quot;) dim(df.with.cut.quant.all) ## [1] 30019 18 # Obrain Newly Created Quantile Group Variables vars.quantile.cut.all &lt;- unlist(lapply(df.cut.list, function(elist) names(elist$vars.quantile.cut))) vars.quantile.cut.all ## [1] &quot;wealthIdx_Qs0e1n2&quot; &quot;hgt0_Qs0e1n3&quot; &quot;wgt0_Qs0e1n3&quot; 2.4.2.5.4 Summarize by Groups Summarize by all groups. summary(df.with.cut.quant.all %&gt;% select(one_of(vars.quantile.cut.all))) ## wealthIdx_Qs0e1n2 hgt0_Qs0e1n3 wgt0_Qs0e1n3 ## [1,7.3]; (1) of Qs0e1n2 :14936 [40.6,48.5]; (1) of Qs0e1n3:10216 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 :10105 ## (7.3,19.3]; (2) of Qs0e1n2:15083 (48.5,50.2]; (2) of Qs0e1n3: 9895 (2.84e+03,3.21e+03]; (2) of Qs0e1n3:10056 ## (50.2,58]; (3) of Qs0e1n3 : 9908 (3.21e+03,5.49e+03]; (3) of Qs0e1n3: 9858 # df.with.cut.quant.all %&gt;% # group_by(!!!syms(vars.quantile.cut.all)) %&gt;% # summarise_at(vars.cts2quantile, funs(mean, n())) 2.4.2.5.5 Generate Joint Quantile Vars Unique Groups # Generate Joint Quantile Index Variable var.qjnt.grp.idx &lt;- &#39;group.index&#39; df.with.cut.quant.all &lt;- df.with.cut.quant.all %&gt;% mutate(!!var.qjnt.grp.idx := group_indices(., !!!syms(vars.quantile.cut.all))) arr.group.idx &lt;- t(sort(unique(df.with.cut.quant.all[[var.qjnt.grp.idx]]))) arr.group.idx ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] ## [1,] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 df.with.cut.quant.all %&gt;% group_by(!!!syms(vars.quantile.cut.all), !!sym(var.qjnt.grp.idx)) %&gt;% summarise_at(vars.cts2quantile, funs(mean, n())) ## # A tibble: 18 x 10 ## # Groups: wealthIdx_Qs0e1n2, hgt0_Qs0e1n3, wgt0_Qs0e1n3 [18] ## wealthIdx_Qs0e1n2 hgt0_Qs0e1n3 wgt0_Qs0e1n3 group.index wealthIdx_mean hgt0_mean wgt0_mean wealthIdx_n hgt0_n wgt0_n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 1 5.31 46.6 2498. 3304 3304 3304 ## 2 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 2 5.08 47.6 2993. 1348 1348 1348 ## 3 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 3 3.64 47.7 3429. 362 362 362 ## 4 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 4 6.04 49.2 2671. 1134 1134 1134 ## 5 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 5 5.36 49.3 3030. 2184 2184 2184 ## 6 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 6 4.36 49.6 3481. 1484 1484 1484 ## 7 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1~ [1.4e+03,2.84e+03]; (1) of Qs~ 7 6.25 51.2 2666. 196 196 196 ## 8 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1~ (2.84e+03,3.21e+03]; (2) of Q~ 8 5.45 51.0 3048. 1466 1466 1466 ## 9 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1~ (3.21e+03,5.49e+03]; (3) of Q~ 9 4.06 51.8 3660. 3458 3458 3458 ## 10 (7.3,19.3]; (2) of Qs0e~ [40.6,48.5]; (1) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 10 9.86 46.8 2540. 3438 3438 3438 ## 11 (7.3,19.3]; (2) of Qs0e~ [40.6,48.5]; (1) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 11 10.5 47.8 2980. 1440 1440 1440 ## 12 (7.3,19.3]; (2) of Qs0e~ [40.6,48.5]; (1) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 12 11.2 48.0 3403. 324 324 324 ## 13 (7.3,19.3]; (2) of Qs0e~ (48.5,50.2]; (2) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 13 10.2 49.4 2679. 1709 1709 1709 ## 14 (7.3,19.3]; (2) of Qs0e~ (48.5,50.2]; (2) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 14 10.3 49.3 3024. 2034 2034 2034 ## 15 (7.3,19.3]; (2) of Qs0e~ (48.5,50.2]; (2) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 15 10.3 49.4 3387. 1350 1350 1350 ## 16 (7.3,19.3]; (2) of Qs0e~ (50.2,58]; (3) of Qs0e1~ [1.4e+03,2.84e+03]; (1) of Qs~ 16 10.5 50.9 2677. 324 324 324 ## 17 (7.3,19.3]; (2) of Qs0e~ (50.2,58]; (3) of Qs0e1~ (2.84e+03,3.21e+03]; (2) of Q~ 17 10.3 51.3 3060. 1584 1584 1584 ## 18 (7.3,19.3]; (2) of Qs0e~ (50.2,58]; (3) of Qs0e1~ (3.21e+03,5.49e+03]; (3) of Q~ 18 11.0 52.1 3623. 2880 2880 2880 df.with.cut.quant.all %&gt;% group_by(!!!syms(vars.group_by)) %&gt;% arrange(!!!syms(vars.arrange)) %&gt;% slice(1L) %&gt;% group_by(!!!syms(vars.quantile.cut.all), !!sym(var.qjnt.grp.idx)) %&gt;% summarise_at(vars.cts2quantile, funs(mean, n())) ## # A tibble: 18 x 10 ## # Groups: wealthIdx_Qs0e1n2, hgt0_Qs0e1n3, wgt0_Qs0e1n3 [18] ## wealthIdx_Qs0e1n2 hgt0_Qs0e1n3 wgt0_Qs0e1n3 group.index wealthIdx_mean hgt0_mean wgt0_mean wealthIdx_n hgt0_n wgt0_n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 1 5.20 46.6 2499. 190 190 190 ## 2 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 2 4.96 47.6 2993. 78 78 78 ## 3 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 3 3.56 47.7 3431. 22 22 22 ## 4 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 4 5.99 49.2 2671. 64 64 64 ## 5 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 5 5.25 49.3 3031. 126 126 126 ## 6 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 6 4.24 49.6 3485. 88 88 88 ## 7 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1~ [1.4e+03,2.84e+03]; (1) of Qs~ 7 6.22 51.2 2666. 11 11 11 ## 8 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1~ (2.84e+03,3.21e+03]; (2) of Q~ 8 5.36 51.0 3048. 84 84 84 ## 9 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1~ (3.21e+03,5.49e+03]; (3) of Q~ 9 3.94 51.8 3667. 207 207 207 ## 10 (7.3,19.3]; (2) of Qs0e~ [40.6,48.5]; (1) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 10 9.86 46.8 2540. 191 191 191 ## 11 (7.3,19.3]; (2) of Qs0e~ [40.6,48.5]; (1) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 11 10.5 47.8 2980. 80 80 80 ## 12 (7.3,19.3]; (2) of Qs0e~ [40.6,48.5]; (1) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 12 11.2 48.0 3403. 18 18 18 ## 13 (7.3,19.3]; (2) of Qs0e~ (48.5,50.2]; (2) of Qs0~ [1.4e+03,2.84e+03]; (1) of Qs~ 13 10.2 49.4 2678. 95 95 95 ## 14 (7.3,19.3]; (2) of Qs0e~ (48.5,50.2]; (2) of Qs0~ (2.84e+03,3.21e+03]; (2) of Q~ 14 10.3 49.3 3024. 113 113 113 ## 15 (7.3,19.3]; (2) of Qs0e~ (48.5,50.2]; (2) of Qs0~ (3.21e+03,5.49e+03]; (3) of Q~ 15 10.3 49.4 3387. 75 75 75 ## 16 (7.3,19.3]; (2) of Qs0e~ (50.2,58]; (3) of Qs0e1~ [1.4e+03,2.84e+03]; (1) of Qs~ 16 10.5 50.9 2677. 18 18 18 ## 17 (7.3,19.3]; (2) of Qs0e~ (50.2,58]; (3) of Qs0e1~ (2.84e+03,3.21e+03]; (2) of Q~ 17 10.3 51.3 3060. 88 88 88 ## 18 (7.3,19.3]; (2) of Qs0e~ (50.2,58]; (3) of Qs0e1~ (3.21e+03,5.49e+03]; (3) of Q~ 18 11.0 52.1 3623. 160 160 160 2.4.2.5.6 Change values Based on Index Index from 1 to 18, change input values based on index # arr.group.idx.subsidy &lt;- arr.group.idx*2 - ((arr.group.idx)^2)*0.01 arr.group.idx.subsidy &lt;- arr.group.idx*2 df.with.cut.quant.all %&gt;% mutate(more_prot = prot + arr.group.idx.subsidy[!!sym(var.qjnt.grp.idx)]) %&gt;% group_by(!!!syms(vars.quantile.cut.all), !!sym(var.qjnt.grp.idx)) %&gt;% summarise_at(c(&#39;more_prot&#39;, &#39;prot&#39;), funs(mean(., na.rm=TRUE))) ## # A tibble: 18 x 6 ## # Groups: wealthIdx_Qs0e1n2, hgt0_Qs0e1n3, wgt0_Qs0e1n3 [18] ## wealthIdx_Qs0e1n2 hgt0_Qs0e1n3 wgt0_Qs0e1n3 group.index more_prot prot ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0e1n3 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 1 14.1 12.1 ## 2 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0e1n3 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 2 15.9 11.9 ## 3 [1,7.3]; (1) of Qs0e1n2 [40.6,48.5]; (1) of Qs0e1n3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 3 27.2 21.2 ## 4 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0e1n3 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 4 18.9 10.9 ## 5 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0e1n3 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 5 22.3 12.3 ## 6 [1,7.3]; (1) of Qs0e1n2 (48.5,50.2]; (2) of Qs0e1n3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 6 28.6 16.6 ## 7 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1n3 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 7 25.5 11.5 ## 8 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1n3 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 8 28.0 12.0 ## 9 [1,7.3]; (1) of Qs0e1n2 (50.2,58]; (3) of Qs0e1n3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 9 34.7 16.7 ## 10 (7.3,19.3]; (2) of Qs0e1n2 [40.6,48.5]; (1) of Qs0e1n3 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 10 30.7 10.7 ## 11 (7.3,19.3]; (2) of Qs0e1n2 [40.6,48.5]; (1) of Qs0e1n3 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 11 34.8 12.8 ## 12 (7.3,19.3]; (2) of Qs0e1n2 [40.6,48.5]; (1) of Qs0e1n3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 12 37.4 13.4 ## 13 (7.3,19.3]; (2) of Qs0e1n2 (48.5,50.2]; (2) of Qs0e1n3 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 13 37.4 11.4 ## 14 (7.3,19.3]; (2) of Qs0e1n2 (48.5,50.2]; (2) of Qs0e1n3 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 14 41.5 13.5 ## 15 (7.3,19.3]; (2) of Qs0e1n2 (48.5,50.2]; (2) of Qs0e1n3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 15 43.9 13.9 ## 16 (7.3,19.3]; (2) of Qs0e1n2 (50.2,58]; (3) of Qs0e1n3 [1.4e+03,2.84e+03]; (1) of Qs0e1n3 16 43.8 11.8 ## 17 (7.3,19.3]; (2) of Qs0e1n2 (50.2,58]; (3) of Qs0e1n3 (2.84e+03,3.21e+03]; (2) of Qs0e1n3 17 47.9 13.9 ## 18 (7.3,19.3]; (2) of Qs0e1n2 (50.2,58]; (3) of Qs0e1n3 (3.21e+03,5.49e+03]; (3) of Qs0e1n3 18 50.3 14.3 2.5 Summarize Multiple Variables 2.5.1 Generate Replace Variables Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 2.5.1.1 Replace NA for Multiple Variables Replace some variables NA by some values, and other variables’ NAs by other values. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;date&#39; # NA dataframe df_NA &lt;- as_tibble(matrix(NA, nrow=it_N, ncol=it_M)) %&gt;% rowid_to_column(var = svr_id) %&gt;% rename_at(vars(starts_with(&quot;V&quot;)), funs(str_replace(., &quot;V&quot;, &quot;var&quot;))) kable(df_NA) %&gt;% kable_styling_fc() date var1 var2 var3 var4 var5 1 NA NA NA NA NA 2 NA NA NA NA NA 3 NA NA NA NA NA # Replace NA df_NA_replace &lt;- df_NA %&gt;% mutate_at(vars(one_of(c(&#39;var1&#39;, &#39;var2&#39;))), list(~replace_na(., 0))) %&gt;% mutate_at(vars(one_of(c(&#39;var3&#39;, &#39;var5&#39;))), list(~replace_na(., 99))) kable(df_NA_replace) %&gt;% kable_styling_fc() date var1 var2 var3 var4 var5 1 0 0 99 NA 99 2 0 0 99 NA 99 3 0 0 99 NA 99 2.5.1.2 Cumulative Sum Multiple Variables Each row is a different date, each column is the profit a firms earns on a date, we want to compute cumulatively how much a person is earning. Also renames variable names below jointly. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;date&#39; # random dataframe, daily profit of firms # dp_fx: daily profit firm ID something set.seed(123) df_daily_profit &lt;- as_tibble(matrix(rnorm(it_N*it_M), nrow=it_N, ncol=it_M)) %&gt;% rowid_to_column(var = svr_id) %&gt;% rename_at(vars(starts_with(&quot;V&quot;)), funs(str_replace(., &quot;V&quot;, &quot;dp_f&quot;))) kable(df_daily_profit) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 # cumulative sum with suffix df_cumu_profit_suffix &lt;- df_daily_profit %&gt;% mutate_at(vars(contains(&#39;dp_f&#39;)), .funs = list(cumu = ~cumsum(.))) kable(df_cumu_profit_suffix) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 dp_f1_cumu dp_f2_cumu dp_f3_cumu dp_f4_cumu dp_f5_cumu 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 -0.7906531 0.1997961 -0.8041450 0.7784198 0.5114542 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 0.7680552 1.9148611 -1.4909979 1.1382337 -0.0443870 # cumulative sum variables naming to prefix df_cumu_profit &lt;- df_cumu_profit_suffix %&gt;% rename_at(vars(contains( &quot;_cumu&quot;) ), list(~paste(&quot;cp_f&quot;, gsub(&quot;_cumu&quot;, &quot;&quot;, .), sep = &quot;&quot;))) %&gt;% rename_at(vars(contains( &quot;cp_f&quot;) ), list(~gsub(&quot;dp_f&quot;, &quot;&quot;, .))) kable(df_cumu_profit) %&gt;% kable_styling_fc_wide() date dp_f1 dp_f2 dp_f3 dp_f4 dp_f5 cp_f1 cp_f2 cp_f3 cp_f4 cp_f5 1 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 -0.5604756 0.0705084 0.4609162 -0.4456620 0.4007715 2 -0.2301775 0.1292877 -1.2650612 1.2240818 0.1106827 -0.7906531 0.1997961 -0.8041450 0.7784198 0.5114542 3 1.5587083 1.7150650 -0.6868529 0.3598138 -0.5558411 0.7680552 1.9148611 -1.4909979 1.1382337 -0.0443870 "],
["functions.html", "Chapter 3 Functions 3.1 Dataframe Mutate 3.2 Dataframe Do Anything 3.3 Apply and pmap", " Chapter 3 Functions 3.1 Dataframe Mutate 3.1.1 Row Input Functions Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. We want evaluate nonlinear function f(Q_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. We would like to evaluate this nonlinear function concurrently across \\(N\\) individuals. The eventual goal is to find the \\(i\\) specific \\(Q\\) that solves the nonlinear equations. This is a continuation of R use Apply, Sapply and dplyr Mutate to Evaluate one Function Across Rows of a Matrix 3.1.1.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) ar_nN_N_choice = seq(1,it_N_child_cnt)/sum(seq(1,it_N_child_cnt)) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha, ar_nN_N_choice) # Show kable(mt_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha ar_nN_N_choice -2 0.1 0.0666667 -1 0.3 0.1333333 0 0.5 0.2000000 1 0.7 0.2666667 2 0.9 0.3333333 3.1.1.2 Testing Function Test non-linear Equation. # Test Parameters fl_N_agg = 100 fl_rho = -1 fl_N_q = ar_nN_N_choice[4]*fl_N_agg ar_A_alpha = mt_nN_by_nQ_A_alpha[4,] # Apply Function ar_p1_s1 = exp((ar_A_alpha[1] - ar_nN_A)*fl_rho) ar_p1_s2 = (ar_A_alpha[2]/ar_nN_alpha) ar_p1_s3 = (1/(ar_nN_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N_q^((ar_A_alpha[2]*fl_rho-1)/(ar_nN_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) print(fl_overall) ## [1] -598.2559 Implement the non-linear problem’s evaluation using apply over all \\(N\\) individuals. # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha # # Test Parameters # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } # Parameters fl_rho = -1 # Evaluate Function print(ffi_nonlin_dplyrdo(mt_nN_by_nQ_A_alpha[1,1], mt_nN_by_nQ_A_alpha[1,2], mt_nN_by_nQ_A_alpha[1,3]*fl_N_agg, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) ## [1] 81.86645 for (i in seq(1,dim(mt_nN_by_nQ_A_alpha)[1])){ fl_eval = ffi_nonlin_dplyrdo(mt_nN_by_nQ_A_alpha[i,1], mt_nN_by_nQ_A_alpha[i,2], mt_nN_by_nQ_A_alpha[i,3]*fl_N_agg, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho) print(fl_eval) } ## [1] 81.86645 ## [1] 54.48885 ## [1] -65.5619 ## [1] -598.2559 ## [1] -3154.072 3.1.1.3 Evaluate Nonlinear Function using dplyr mutate # Convert Matrix to Tibble ar_st_col_names = c(&#39;fl_A&#39;, &#39;fl_alpha&#39;, &#39;fl_N&#39;) tb_nN_by_nQ_A_alpha &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rename_all(~c(ar_st_col_names)) # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # Test Parameters # ar_A = ar_nN_A # ar_alpha = ar_nN_alpha # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = (fl_N*fl_N_agg)^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_nN_by_nQ_A_alpha = tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha fl_N dplyr_eval -2 0.1 0.0666667 81.86645 -1 0.3 0.1333333 54.48885 0 0.5 0.2000000 -65.56190 1 0.7 0.2666667 -598.25595 2 0.9 0.3333333 -3154.07226 3.1.2 Evaluate Choices Across States Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. See the ff_opti_bisect_pmap_multi function from Fan’s REconTools Package, which provides a resuable function based on the algorithm worked out here. We want evaluate linear function \\(0=f(z_{ij}, x_i, y_i, \\textbf{X}, \\textbf{Y}, c, d)\\). There are \\(i\\) functions that have \\(i\\) specific \\(x\\) and \\(y\\). For each \\(i\\) function, we evaluate along a grid of feasible values for \\(z\\), over \\(j\\in J\\) grid points, potentially looking for the \\(j\\) that is closest to the root. \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are arrays common across the \\(i\\) equations, and \\(c\\) and \\(d\\) are constants. The evaluation strategy is the following, given min and max for \\(z\\) that are specific for each \\(j\\), and given common number of grid points, generate a matrix of \\(z_{ij}\\). Suppose there the number of \\(i\\) is \\(I\\), and the number of grid points for \\(j\\) is \\(J\\). Generate a \\(J \\cdot I\\) by \\(3\\) matrix where the columns are \\(z,x,y\\) as tibble Follow this Mutate to evaluate the \\(f(\\cdot)\\) function. Add two categorical columns for grid levels and wich \\(i\\), \\(i\\) and \\(j\\) index. Plot Mutate output evaluated column categorized by \\(i\\) as color and \\(j\\) as x-axis. 3.1.2.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] Now we need to expand this by the number of choice grid. Each row, representing one equation, is expanded by the number of choice grids. We are graphically searching, or rather brute force searching, which means if we have 100 individuals, we want to plot out the nonlinear equation for each of these lines, and show graphically where each line crosses zero. We achieve this, by evaluating the equation for each of the 100 individuals along a grid of feasible choices. In this problem here, the feasible choices are shared across individuals. # Parameters fl_rho = 0.20 svr_id_var = &#39;INDI_ID&#39; # it_child_count = N, the number of children it_N_child_cnt = 4 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # Choice Grid for nutritional feasible choices for each fl_N_agg = 100 fl_N_min = 0 it_N_choice_cnt_ttest = 3 it_N_choice_cnt_dense = 100 ar_N_choices_ttest = seq(fl_N_min, fl_N_agg, length.out = it_N_choice_cnt_ttest) ar_N_choices_dense = seq(fl_N_min, fl_N_agg, length.out = it_N_choice_cnt_dense) # Mesh Expand tb_states_choices &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rowid_to_column(var=svr_id_var) tb_states_choices_ttest &lt;- tb_states_choices %&gt;% expand_grid(choices = ar_N_choices_ttest) tb_states_choices_dense &lt;- tb_states_choices %&gt;% expand_grid(choices = ar_N_choices_dense) # display summary(tb_states_choices_dense) ## INDI_ID ar_nN_A ar_nN_alpha choices ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 0 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 25 ## Median :2.50 Median : 0 Median :0.5 Median : 50 ## Mean :2.50 Mean : 0 Mean :0.5 Mean : 50 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.: 75 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :100 kable(tb_states_choices_ttest) %&gt;% kable_styling_fc() INDI_ID ar_nN_A ar_nN_alpha choices 1 -2.0000000 0.1000000 0 1 -2.0000000 0.1000000 50 1 -2.0000000 0.1000000 100 2 -0.6666667 0.3666667 0 2 -0.6666667 0.3666667 50 2 -0.6666667 0.3666667 100 3 0.6666667 0.6333333 0 3 0.6666667 0.6333333 50 3 0.6666667 0.6333333 100 4 2.0000000 0.9000000 0 4 2.0000000 0.9000000 50 4 2.0000000 0.9000000 100 3.1.2.2 Apply Same Function all Rows, Some Inputs Row-specific, other Shared There are two types of inputs, row-specific inputs, and inputs that should be applied for each row. The Function just requires all of these inputs, it does not know what is row-specific and what is common for all row. Dplyr recognizes which parameter inputs already existing in the piped dataframe/tibble, given rowwise, those will be row-specific inputs. Additional function parameters that do not exist in dataframe as variable names, but that are pre-defined scalars or arrays will be applied to all rows. (???) string variable name of input where functions are evaluated, these are already contained in the dataframe, existing variable names, row specific, rowwise computation over these, each rowwise calculation using different rows: fl_A, fl_alpha, fl_N (???) scalar and array values that are applied to every rowwise calculation, all rowwise calculations using the same scalars and arrays:ar_A, ar_alpha, fl_N_agg, fl_rho (???) string output variable name The function looks within group, finds min/max etc that are relevant. 3.1.2.2.1 3 Points and Denser Dataframs and Define Function # Convert Matrix to Tibble ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_states_choices &lt;- tb_states_choices %&gt;% rename_all(~c(ar_st_col_names)) ar_st_col_names = c(svr_id_var,&#39;fl_A&#39;, &#39;fl_alpha&#39;, &#39;fl_N&#39;) tb_states_choices_ttest &lt;- tb_states_choices_ttest %&gt;% rename_all(~c(ar_st_col_names)) tb_states_choices_dense &lt;- tb_states_choices_dense %&gt;% rename_all(~c(ar_st_col_names)) # Define Implicit Function ffi_nonlin_dplyrdo &lt;- function(fl_A, fl_alpha, fl_N, ar_A, ar_alpha, fl_N_agg, fl_rho){ # scalar value that are row-specific, in dataframe already: *fl_A*, *fl_alpha*, *fl_N* # array and scalars not in dataframe, common all rows: *ar_A*, *ar_alpha*, *fl_N_agg*, *fl_rho* # Test Parameters # ar_A = ar_nN_A # ar_alpha = ar_nN_alpha # fl_N = 100 # fl_rho = -1 # fl_N_q = 10 # Apply Function ar_p1_s1 = exp((fl_A - ar_A)*fl_rho) ar_p1_s2 = (fl_alpha/ar_alpha) ar_p1_s3 = (1/(ar_alpha*fl_rho - 1)) ar_p1 = (ar_p1_s1*ar_p1_s2)^ar_p1_s3 ar_p2 = fl_N^((fl_alpha*fl_rho-1)/(ar_alpha*fl_rho-1)) ar_overall = ar_p1*ar_p2 fl_overall = fl_N_agg - sum(ar_overall) return(fl_overall) } 3.1.2.2.2 Evaluate at Three Choice Points and Show Table In the example below, just show results evaluating over three choice points and show table. # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_states_choices_ttest_eval = tb_states_choices_ttest %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show kable(tb_states_choices_ttest_eval) %&gt;% kable_styling_fc() INDI_ID fl_A fl_alpha fl_N dplyr_eval 1 -2.0000000 0.1000000 0 100.00000 1 -2.0000000 0.1000000 50 -5666.95576 1 -2.0000000 0.1000000 100 -12880.28392 2 -0.6666667 0.3666667 0 100.00000 2 -0.6666667 0.3666667 50 -595.73454 2 -0.6666667 0.3666667 100 -1394.70698 3 0.6666667 0.6333333 0 100.00000 3 0.6666667 0.6333333 50 -106.51058 3 0.6666667 0.6333333 100 -323.94216 4 2.0000000 0.9000000 0 100.00000 4 2.0000000 0.9000000 50 22.55577 4 2.0000000 0.9000000 100 -51.97161 3.1.2.2.3 Evaluate at Many Choice Points and Show Graphically Same as above, but now we evaluate the function over the individuals at many choice points so that we can graph things out. # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_states_choices_dense_eval = tb_states_choices_dense %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_nonlin_dplyrdo(fl_A, fl_alpha, fl_N, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Show dim(tb_states_choices_dense_eval) ## [1] 400 5 summary(tb_states_choices_dense_eval) ## INDI_ID fl_A fl_alpha fl_N dplyr_eval ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 0 Min. :-12880.28 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 25 1st Qu.: -1167.29 ## Median :2.50 Median : 0 Median :0.5 Median : 50 Median : -202.42 ## Mean :2.50 Mean : 0 Mean :0.5 Mean : 50 Mean : -1645.65 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.: 75 3rd Qu.: 0.96 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :100 Max. : 100.00 lineplot &lt;- tb_states_choices_dense_eval %&gt;% ggplot(aes(x=fl_N, y=dplyr_eval)) + geom_line() + facet_wrap( . ~ INDI_ID, scales = &quot;free&quot;) + geom_hline(yintercept=0, linetype=&quot;dashed&quot;, color = &quot;red&quot;, size=1) labs(title = &#39;Evaluate Non-Linear Functions to Search for Roots&#39;, x = &#39;X values&#39;, y = &#39;f(x)&#39;, caption = &#39;Evaluating the Function&#39;) ## $x ## [1] &quot;X values&quot; ## ## $y ## [1] &quot;f(x)&quot; ## ## $title ## [1] &quot;Evaluate Non-Linear Functions to Search for Roots&quot; ## ## $caption ## [1] &quot;Evaluating the Function&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; print(lineplot) 3.2 Dataframe Do Anything 3.2.1 MxQ to MxP Rows Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 3.2.1.1 MxQ to Mx1 Rows: Within Group Gini There is a Panel with \\(M\\) individuals and each individual has \\(Q\\) records/rows. A function generate an individual specific outcome given the \\(Q\\) individual specific inputs, along with shared parameters and arrays across the \\(M\\) individuals. For example, suppose we have a dataframe of individual wage information from different countries, each row is an individual from one country. We want to generate country specific gini based on the individual data for each country in the dataframe. But additionally, perhaps the gini formula requires not just individual income but some additional parameters or shared dataframes as inputs. Given the within \\(m\\) income observations, we can compute gini statistics that are individual specific based on the observed distribution of incomes. For this, we will use the ff_dist_gini_vector_pos.html function from REconTools. To make this more interesting, we will generate large dataframe with more \\(M\\) and more \\(Q\\) each \\(m\\). 3.2.1.1.1 Large Dataframe There are up to ten thousand income observation per person. And there are ten people. # Parameter Setups it_M &lt;- 10 it_Q_max &lt;- 10000 fl_rnorm_mu &lt;- 1 ar_rnorm_sd &lt;- seq(0.01, 0.2, length.out=it_M) ar_it_q &lt;- sample.int(it_Q_max, it_M, replace=TRUE) # N by Q varying parameters mt_data = cbind(ar_it_q, ar_rnorm_sd) tb_M &lt;- as_tibble(mt_data) %&gt;% rowid_to_column(var = &quot;ID&quot;) %&gt;% rename(sd = ar_rnorm_sd, Q = ar_it_q) %&gt;% mutate(mean = fl_rnorm_mu) 3.2.1.1.2 Compute Group specific gini, NORMAL There is only one input for the gini function ar_pos. Note that the gini are not very large even with large SD, because these are normal distributions. By Construction, most peple are in the middle. So with almost zero standard deviation, we have perfect equality, as standard deviation increases, inequality increases, but still pretty equal overall, there is no fat upper tail. Note that there are three ways of referring to variable names with dot, which are all shown below: We can explicitly refer to names We can use the dollar dot structure to use string variable names in do anything. We can use dot bracket, this is the only option that works with string variable names # A. Normal Draw Expansion, Explicitly Name set.seed(&#39;123&#39;) tb_income_norm_dot_dollar &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.$Q, mean=.$mean, sd=.$sd)) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # Normal Draw Expansion again, dot dollar differently with string variable name set.seed(&#39;123&#39;) tb_income_norm_dollar_dot &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(`$`(., &#39;Q&#39;), mean = `$`(., &#39;mean&#39;), sd = `$`(., &#39;sd&#39;))) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # Normal Draw Expansion again, dot double bracket set.seed(&#39;123&#39;) svr_mean &lt;- &#39;mean&#39; svr_sd &lt;- &#39;sd&#39; svr_Q &lt;- &#39;Q&#39; tb_income_norm_dot_bracket_db &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.[[svr_Q]], mean = .[[svr_mean]], sd = .[[svr_sd]])) %&gt;% unnest(c(income)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) # display sum(sum(tb_income_norm_dollar_dot - tb_income_norm_dot_dollar - tb_income_norm_dot_bracket_db)) ## [1] -463785175 # display head(tb_income_norm_dot_dollar, 20) ## # A tibble: 20 x 5 ## ID income Q sd mean ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.994 9982 0.01 1 ## 2 1 0.998 9982 0.01 1 ## 3 1 1.02 9982 0.01 1 ## 4 1 1.00 9982 0.01 1 ## 5 1 1.00 9982 0.01 1 ## 6 1 1.02 9982 0.01 1 ## 7 1 1.00 9982 0.01 1 ## 8 1 0.987 9982 0.01 1 ## 9 1 0.993 9982 0.01 1 ## 10 1 0.996 9982 0.01 1 ## 11 1 1.01 9982 0.01 1 ## 12 1 1.00 9982 0.01 1 ## 13 1 1.00 9982 0.01 1 ## 14 1 1.00 9982 0.01 1 ## 15 1 0.994 9982 0.01 1 ## 16 1 1.02 9982 0.01 1 ## 17 1 1.00 9982 0.01 1 ## 18 1 0.980 9982 0.01 1 ## 19 1 1.01 9982 0.01 1 ## 20 1 0.995 9982 0.01 1 # Gini by Group tb_gini_norm &lt;- tb_income_norm_dollar_dot %&gt;% group_by(ID) %&gt;% do(inc_gini_norm = ff_dist_gini_vector_pos(.$income)) %&gt;% unnest(c(inc_gini_norm)) %&gt;% left_join(tb_M, by=&quot;ID&quot;) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) ## see REconTools for formula: DIST GINI--Compute Gini Inequality Coefficient Given Data Vector (One Variable) # display kable(tb_gini_norm) %&gt;% kable_styling_fc() ID inc_gini_norm Q sd mean 1 0.0056337 9982 0.0100000 1 2 0.0175280 2980 0.0311111 1 3 0.0293986 1614 0.0522222 1 4 0.0422304 555 0.0733333 1 5 0.0535146 4469 0.0944444 1 6 0.0653938 9359 0.1155556 1 7 0.0769135 7789 0.1366667 1 8 0.0894165 9991 0.1577778 1 9 0.1010982 9097 0.1788889 1 10 0.1124019 1047 0.2000000 1 3.2.2 Mx1 to MxQ Rows Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Case One: There is a dataframe with \\(M\\) rows, based on these \\(m\\) specific information, generate dataframes for each \\(m\\). Stack these indivdiual dataframes together and merge original \\(m\\) specific information in as well. The number of rows for each \\(m\\) is \\(Q_m\\), each \\(m\\) could have different number of expansion rows. Generate a panel with \\(M\\) individuals, each individual is observed for different spans of times (uncount). Before expanding, generate individual specific normal distribution standard deviation. All individuals share the same mean, but have increasing standard deviations. 3.2.2.1 Generate Dataframe with M Rows. This is the first step, generate \\(M\\) rows of data, to be expanded. Each row contains the number of normal draws to make and the mean and the standard deviation for normal daraws that are \\(m\\) specific. # Parameter Setups it_M &lt;- 3 it_Q_max &lt;- 5 fl_rnorm_mu &lt;- 1000 ar_rnorm_sd &lt;- seq(0.01, 200, length.out=it_M) ar_it_q &lt;- sample.int(it_Q_max, it_M, replace=TRUE) # N by Q varying parameters mt_data = cbind(ar_it_q, ar_rnorm_sd) tb_M &lt;- as_tibble(mt_data) %&gt;% rowid_to_column(var = &quot;ID&quot;) %&gt;% rename(sd = ar_rnorm_sd, Q = ar_it_q) %&gt;% mutate(mean = fl_rnorm_mu) # display kable(tb_M) %&gt;% kable_styling_fc() ID Q sd mean 1 3 0.010 1000 2 3 100.005 1000 3 1 200.000 1000 3.2.2.2 Random Normal Draw Expansion The steps are: do anything use “.$” sign to refer to variable names, or [[‘name’]] unnest left_join expanded and original Note these all give the same results Use dot dollar to get variables # Generate $Q_m$ individual specific incomes, expanded different number of times for each m tb_income &lt;- tb_M %&gt;% group_by(ID) %&gt;% do(income = rnorm(.$Q, mean=.$mean, sd=.$sd)) %&gt;% unnest(c(income)) # Merge back with tb_M tb_income_full_dd &lt;- tb_income %&gt;% left_join(tb_M) ## Joining, by = &quot;ID&quot; # display kable(tb_income) %&gt;% kable_styling_fc() ID income 1 1000.0183 1 999.9943 1 999.9822 2 1033.7465 2 1093.1374 2 862.1896 3 988.7742 kable(tb_income_full_dd) %&gt;% kable_styling_fc() ID income Q sd mean 1 1000.0183 3 0.010 1000 1 999.9943 3 0.010 1000 1 999.9822 3 0.010 1000 2 1033.7465 3 100.005 1000 2 1093.1374 3 100.005 1000 2 862.1896 3 100.005 1000 3 988.7742 1 200.000 1000 3.3 Apply and pmap 3.3.1 Apply, Sapply, Mutate Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. r apply matrix to function row by row r evaluate function on grid Apply a function to every row of a matrix or a data frame r apply r sapply sapply over matrix row by row apply dplyr vectorize function as parameters using formulas do We want evaluate linear function f(x_i, y_i, ar_x, ar_y, c, d), where c and d are constants, and ar_x and ar_y are arrays, both fixed. x_i and y_i vary over each row of matrix. More specifically, we have a functions, this function takes inputs that are individual specific. We would like to evaluate this function concurrently across \\(N\\) individuals. The function is such that across the \\(N\\) individuals, some of the function parameter inputs are the same, but others are different. If we are looking at demand for a particular product, the prices of all products enter the demand equation for each product, but the product’s own price enters also in a different way. The objective is either to just evaluate this function across \\(N\\) individuals, or this is a part of a nonlinear solution system. What is the relationship between apply, lapply and vectorization? see Is the “*apply” family really not vectorized?. 3.3.1.1 Set up Input Arrays There is a function that takes \\(M=Q+P\\) inputs, we want to evaluate this function \\(N\\) times. Each time, there are \\(M\\) inputs, where all but \\(Q\\) of the \\(M\\) inputs, meaning \\(P\\) of the \\(M\\) inputs, are the same. In particular, \\(P=Q*N\\). \\[M = Q+P = Q + Q*N\\] # it_child_count = N, the number of children it_N_child_cnt = 5 # it_heter_param = Q, number of parameters that are heterogeneous across children it_Q_hetpa_cnt = 2 # P fixed parameters, nN is N dimensional, nP is P dimensional ar_nN_A = seq(-2, 2, length.out = it_N_child_cnt) ar_nN_alpha = seq(0.1, 0.9, length.out = it_N_child_cnt) ar_nP_A_alpha = c(ar_nN_A, ar_nN_alpha) # N by Q varying parameters mt_nN_by_nQ_A_alpha = cbind(ar_nN_A, ar_nN_alpha) # display kable(mt_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() ar_nN_A ar_nN_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 3.3.1.2 Using apply 3.3.1.2.1 Apply with Named Function First we use the apply function, we have to hard-code the arrays that are fixed for each of the \\(N\\) individuals. Then apply allows us to loop over the matrix that is \\(N\\) by \\(Q\\), each row one at a time, from \\(1\\) to \\(N\\). # Define Implicit Function ffi_linear_hardcode &lt;- function(ar_A_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha fl_out = sum(ar_A_alpha[1]*ar_nN_A + 1/(ar_A_alpha[2] + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row ar_func_apply = apply(mt_nN_by_nQ_A_alpha, 1, ffi_linear_hardcode) 3.3.1.2.2 Apply using Anonymous Function apply over matrix Apply with anonymous function generating a list of arrays of different lengths. In the example below, we want to drawn \\(N\\) sets of random uniform numbers, but for each set the number of draws we want to have is \\(Q_i\\). Furthermore, we want to rescale the random uniform draws so that they all become proportions that sum u pto one for each \\(i\\), but then we multply each row’s values by the row specific aggregates. The anonymous function has hard coded parameters. Using an anonymous function here allows for parameters to be provided inside the function that are shared across each looped evaluation. This is perhaps more convenient than sapply with additional parameters. set.seed(1039) # Define the number of draws each row and total amount it_N &lt;- 4 fl_unif_min &lt;- 1 fl_unif_max &lt;- 2 mt_draw_define &lt;- cbind(seq(it_N),runif(it_N, min=1,max=10)) print(mt_draw_define) ## [,1] [,2] ## [1,] 1 2.131008 ## [2,] 2 7.016820 ## [3,] 3 4.774441 ## [4,] 4 5.023006 # apply row by row, anonymous function has hard coded min and max ls_ar_draws_shares_lvls = apply(cbind(seq(it_N),runif(it_N, min=1,max=10)), 1, function(row, min, max) { it_draw &lt;- row[1] fl_sum &lt;- row[2] ar_unif &lt;- runif(it_draw, min=fl_unif_min, max=fl_unif_max) ar_share &lt;- ar_unif/sum(ar_unif) ar_levels &lt;- ar_share*fl_sum return(list(ar_share=ar_share, ar_levels=ar_levels)) }) # Show Results print(ls_ar_draws_shares_lvls) ## [[1]] ## [[1]]$ar_share ## [1] 1 ## ## [[1]]$ar_levels ## [1] 5.361378 ## ## ## [[2]] ## [[2]]$ar_share ## [1] 0.4428811 0.5571189 ## ## [[2]]$ar_levels ## [1] 3.388957 4.263112 ## ## ## [[3]] ## [[3]]$ar_share ## [1] 0.4233740 0.2913644 0.2852616 ## ## [[3]]$ar_levels ## [1] 4.052625 2.789002 2.730584 ## ## ## [[4]] ## [[4]]$ar_share ## [1] 0.3082076 0.2913433 0.2012986 0.1991505 ## ## [[4]]$ar_levels ## [1] 2.965381 2.803123 1.936769 1.916102 3.3.1.3 Using sapply 3.3.1.3.1 sapply with named function r convert matrix to list Convert a matrix to a list of vectors in R Sapply allows us to not have tohard code in the A and alpha arrays. But Sapply works over List or Vector, not Matrix. So we have to convert the \\(N\\) by \\(Q\\) matrix to a N element list Now update the function with sapply. ls_ar_nN_by_nQ_A_alpha = as.list(data.frame(t(mt_nN_by_nQ_A_alpha))) # Define Implicit Function ffi_linear_sapply &lt;- function(ar_A_alpha, ar_A, ar_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha fl_out = sum(ar_A_alpha[1]*ar_nN_A + 1/(ar_A_alpha[2] + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row ar_func_sapply = sapply(ls_ar_nN_by_nQ_A_alpha, ffi_linear_sapply, ar_A=ar_nN_A, ar_alpha=ar_nN_alpha) 3.3.1.3.2 sapply using anonymous function sapply anonymous function r anoymous function multiple lines Sapply with anonymous function generating a list of arrays of different lengths. In the example below, we want to drawn \\(N\\) sets of random uniform numbers, but for each set the number of draws we want to have is \\(Q_i\\). Furthermore, we want to rescale the random uniform draws so that they all become proportions that sum u pto one for each \\(i\\). it_N &lt;- 4 fl_unif_min &lt;- 1 fl_unif_max &lt;- 2 # Generate using runif without anonymous function set.seed(1039) ls_ar_draws = sapply(seq(it_N), runif, min=fl_unif_min, max=fl_unif_max) print(ls_ar_draws) ## [[1]] ## [1] 1.125668 ## ## [[2]] ## [1] 1.668536 1.419382 ## ## [[3]] ## [1] 1.447001 1.484598 1.739119 ## ## [[4]] ## [1] 1.952468 1.957931 1.926995 1.539678 # Generate Using Anonymous Function set.seed(1039) ls_ar_draws_shares = sapply(seq(it_N), function(n, min, max) { ar_unif &lt;- runif(n,min,max) ar_share &lt;- ar_unif/sum(ar_unif) return(ar_share) }, min=fl_unif_min, max=fl_unif_max) # Print Share print(ls_ar_draws_shares) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 0.5403432 0.4596568 ## ## [[3]] ## [1] 0.3098027 0.3178522 0.3723451 ## ## [[4]] ## [1] 0.2646671 0.2654076 0.2612141 0.2087113 # Sapply with anonymous function to check sums sapply(seq(it_N), function(x) {sum(ls_ar_draws[[x]])}) ## [1] 1.125668 3.087918 4.670717 7.377071 sapply(seq(it_N), function(x) {sum(ls_ar_draws_shares[[x]])}) ## [1] 1 1 1 1 3.3.1.4 Using dplyr mutate rowwise dplyr mutate own function dplyr all row function dplyr do function apply function each row dplyr applying a function to every row of a table using dplyr dplyr rowwise # Convert Matrix to Tibble ar_st_col_names = c(&#39;fl_A&#39;, &#39;fl_alpha&#39;) tb_nN_by_nQ_A_alpha &lt;- as_tibble(mt_nN_by_nQ_A_alpha) %&gt;% rename_all(~c(ar_st_col_names)) # Show kable(tb_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha -2 0.1 -1 0.3 0 0.5 1 0.7 2 0.9 # Define Implicit Function ffi_linear_dplyrdo &lt;- function(fl_A, fl_alpha, ar_nN_A, ar_nN_alpha){ # ar_A_alpha[1] is A # ar_A_alpha[2] is alpha print(paste0(&#39;cur row, fl_A=&#39;, fl_A, &#39;, fl_alpha=&#39;, fl_alpha)) fl_out = sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row of tibble # fl_A, fl_alpha are from columns of tb_nN_by_nQ_A_alpha tb_nN_by_nQ_A_alpha_show &lt;- tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_linear_dplyrdo(fl_A, fl_alpha, ar_nN_A, ar_nN_alpha)) ## [1] &quot;cur row, fl_A=-2, fl_alpha=0.1&quot; ## [1] &quot;cur row, fl_A=-1, fl_alpha=0.3&quot; ## [1] &quot;cur row, fl_A=0, fl_alpha=0.5&quot; ## [1] &quot;cur row, fl_A=1, fl_alpha=0.7&quot; ## [1] &quot;cur row, fl_A=2, fl_alpha=0.9&quot; # Show kable(tb_nN_by_nQ_A_alpha_show) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 same as before, still rowwise, but hard code some inputs: # Define function, fixed inputs are not parameters, but defined earlier as a part of the function # ar_nN_A, ar_nN_alpha are fixed, not parameters ffi_linear_dplyrdo_func &lt;- function(fl_A, fl_alpha){ fl_out &lt;- sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate function row by row of tibble tbfunc_A_nN_by_nQ_A_alpha_rowwise = tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% mutate(dplyr_eval = ffi_linear_dplyrdo_func(fl_A, fl_alpha)) # Show kable(tbfunc_A_nN_by_nQ_A_alpha_rowwise) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.1.5 Using Dplyr Mutate with Pmap Apparantly rowwise() is not a good idea, and pmap should be used, below is the pmap solution to the problem. Which does seem nicer. Crucially, don’t have to define input parameter names, automatically I think they are matching up to the names in the function dplyr mutate pass function r function quosure string multiple r function multiple parameters as one string dplyr mutate anonymous function quosure style lambda pmap tibble rows dplyr pwalk # Define function, fixed inputs are not parameters, but defined earlier as a part of the function # Rorate fl_alpha and fl_A name compared to before to make sure pmap tracks by names ffi_linear_dplyrdo_func &lt;- function(fl_alpha, fl_A){ fl_out &lt;- sum(fl_A*ar_nN_A + 1/(fl_alpha + 1/ar_nN_alpha)) return(fl_out) } # Evaluate a function row by row of dataframe, generate list, then to vecotr tb_nN_by_nQ_A_alpha %&gt;% pmap(ffi_linear_dplyrdo_func) %&gt;% unlist() ## [1] 2.346356 2.094273 1.895316 1.733708 1.599477 # Same as above, but in line line and save output as new column in dataframe # note this ONLY works if the tibble only has variables that are inputs for the function # if tibble contains additional variables, those should be droppd, or only the ones needed # selected, inside the pmap call below. tbfunc_A_nN_by_nQ_A_alpha_pmap &lt;- tb_nN_by_nQ_A_alpha %&gt;% mutate(dplyr_eval_pmap = unlist( pmap(tb_nN_by_nQ_A_alpha, ffi_linear_dplyrdo_func) ) ) # Show kable(tbfunc_A_nN_by_nQ_A_alpha_pmap) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval_pmap -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.1.6 DPLYR Three Types of Inputs ROWWISE Now, we have three types of parameters, for something like a bisection type calculation. We will supply the program with a function with some hard-coded value inside, and as parameters, we will have one parameter which is a row in the current matrix, and another parameter which is a sclar values. The three types of parameters are dealt with sparately: parameters that are fixed for all bisection iterations, but differ for each row these are hard-coded into the function parameters that are fixed for all bisection iterations, but are shared across rows these are the first parameter of the function, a list parameters that differ for each iteration, but differ acoss iterations second scalar value parameter for the function dplyr mutate function applow to each row dot notation note rowwise might be bad according to Hadley, should use pmap? ffi_linear_dplyrdo_fdot &lt;- function(ls_row, fl_param){ # Type 1 Param = ar_nN_A, ar_nN_alpha # Type 2 Param = ls_row$fl_A, ls_row$fl_alpha # Type 3 Param = fl_param fl_out &lt;- (sum(ls_row$fl_A*ar_nN_A + 1/(ls_row$fl_alpha + 1/ar_nN_alpha))) + fl_param return(fl_out) } cur_func &lt;- ffi_linear_dplyrdo_fdot fl_param &lt;- 0 dplyr_eval_flex &lt;- tb_nN_by_nQ_A_alpha %&gt;% rowwise() %&gt;% do(dplyr_eval_flex = cur_func(., fl_param)) %&gt;% unnest(dplyr_eval_flex) tbfunc_B_nN_by_nQ_A_alpha &lt;- tb_nN_by_nQ_A_alpha %&gt;% add_column(dplyr_eval_flex) # Show kable(tbfunc_B_nN_by_nQ_A_alpha) %&gt;% kable_styling_fc() fl_A fl_alpha dplyr_eval_flex -2 0.1 2.346356 -1 0.3 2.094273 0 0.5 1.895316 1 0.7 1.733708 2 0.9 1.599477 3.3.1.7 Compare Apply and Mutate Results # Show overall Results mt_results &lt;- cbind(ar_func_apply, ar_func_sapply, tb_nN_by_nQ_A_alpha_show[&#39;dplyr_eval&#39;], tbfunc_A_nN_by_nQ_A_alpha_rowwise[&#39;dplyr_eval&#39;], tbfunc_A_nN_by_nQ_A_alpha_pmap[&#39;dplyr_eval_pmap&#39;], tbfunc_B_nN_by_nQ_A_alpha[&#39;dplyr_eval_flex&#39;], mt_nN_by_nQ_A_alpha) colnames(mt_results) &lt;- c(&#39;eval_lin_apply&#39;, &#39;eval_lin_sapply&#39;, &#39;eval_dplyr_mutate&#39;, &#39;eval_dplyr_mutate_hcode&#39;, &#39;eval_dplyr_mutate_pmap&#39;, &#39;eval_dplyr_mutate_flex&#39;, &#39;A_child&#39;, &#39;alpha_child&#39;) kable(mt_results) %&gt;% kable_styling_fc_wide() eval_lin_apply eval_lin_sapply eval_dplyr_mutate eval_dplyr_mutate_hcode eval_dplyr_mutate_pmap eval_dplyr_mutate_flex A_child alpha_child X1 2.346356 2.346356 2.346356 2.346356 2.346356 2.346356 -2 0.1 X2 2.094273 2.094273 2.094273 2.094273 2.094273 2.094273 -1 0.3 X3 1.895316 1.895316 1.895316 1.895316 1.895316 1.895316 0 0.5 X4 1.733708 1.733708 1.733708 1.733708 1.733708 1.733708 1 0.7 X5 1.599477 1.599477 1.599477 1.599477 1.599477 1.599477 2 0.9 "],
["panel.html", "Chapter 4 Panel 4.1 Generate and Join 4.2 Wide and Long", " Chapter 4 Panel 4.1 Generate and Join 4.1.1 Generate Panel Structure Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 4.1.1.1 Balanced Panel Skeleton There are \\(N\\) individuals, each could be observed \\(M\\) times. In the example below, there are 3 students, each observed over 4 dates. This just uses the uncount function from tidyr. # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;student_id&#39; svr_date &lt;- &#39;class_day&#39; # dataframe df_panel_skeleton &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Print kable(df_panel_skeleton) %&gt;% kable_styling_fc() student_id class_day 1 1 1 2 1 3 1 4 1 5 2 1 2 2 2 3 2 4 2 5 3 1 3 2 3 3 3 4 3 5 4.1.2 Join Datasets Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. 4.1.2.1 Join Panel with Multiple Keys We have two datasets, one for student enrollment, panel over time, but some students do not show up on some dates. The other is a skeleton panel with all student ID and all dates. Often we need to join dataframes together, and we need to join by the student ID and the panel time Key at the same time. When students show up, there is a quiz score for that day, so the joined panel should have as data column quiz score Student count is \\(N\\), total dates are \\(M\\). First we generate two panels below, then we join by both keys using left_join. First, define dataframes: # Define it_N &lt;- 4 it_M &lt;- 3 svr_id &lt;- &#39;sid&#39; svr_date &lt;- &#39;classday&#39; svr_attend &lt;- &#39;date_in_class&#39; # Panel Skeleton df_panel_balanced_skeleton &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Print kable(df_panel_balanced_skeleton) %&gt;% kable_styling_fc() sid classday 1 1 1 2 1 3 2 1 2 2 2 3 3 1 3 2 3 3 4 1 4 2 4 3 # Smaller Panel of Random Days in School set.seed(456) df_panel_attend &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), !!sym(svr_date)) %&gt;% rename(!!sym(svr_attend) := !!sym(svr_date)) %&gt;% mutate(dayquizscore = rnorm(n(),mean=80,sd=10)) # Print kable(df_panel_attend) %&gt;% kable_styling_fc() sid date_in_class dayquizscore 1 1 89.88726 2 1 96.53929 2 2 65.59195 2 3 99.47356 4 2 97.36936 Second, now join dataframes: # Join with explicit names df_quiz_joined_multikey &lt;- df_panel_balanced_skeleton %&gt;% left_join(df_panel_attend, by=(c(&#39;sid&#39;=&#39;sid&#39;, &#39;classday&#39;=&#39;date_in_class&#39;))) # Join with setname strings df_quiz_joined_multikey_setnames &lt;- df_panel_balanced_skeleton %&gt;% left_join(df_panel_attend, by=setNames(c(&#39;sid&#39;, &#39;date_in_class&#39;), c(&#39;sid&#39;, &#39;classday&#39;))) # Print kable(df_quiz_joined_multikey) %&gt;% kable_styling_fc() sid classday dayquizscore 1 1 89.88726 1 2 NA 1 3 NA 2 1 96.53929 2 2 65.59195 2 3 99.47356 3 1 NA 3 2 NA 3 3 NA 4 1 NA 4 2 97.36936 4 3 NA kable(df_quiz_joined_multikey_setnames) %&gt;% kable_styling_fc() sid classday dayquizscore 1 1 89.88726 1 2 NA 1 3 NA 2 1 96.53929 2 2 65.59195 2 3 99.47356 3 1 NA 3 2 NA 3 3 NA 4 1 NA 4 2 97.36936 4 3 NA 4.1.2.2 Stack Panel Frames Together There are multiple panel dataframe, each for different subsets of dates. All variable names and units of observations are identical. Use DPLYR bind_rows. # Define it_N &lt;- 2 # Number of individuals it_M &lt;- 3 # Number of Months svr_id &lt;- &#39;sid&#39; svr_date &lt;- &#39;date&#39; # Panel First Half of Year df_panel_m1tom3 &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number()) %&gt;% ungroup() # Panel Second Half of Year df_panel_m4tom6 &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(!!sym(svr_date) := row_number() + 3) %&gt;% ungroup() # Bind Rows df_panel_m1tm6 &lt;- bind_rows(df_panel_m1tom3, df_panel_m4tom6) %&gt;% arrange(!!!syms(c(svr_id, svr_date))) # Print kable(df_panel_m1tom3) %&gt;% kable_styling_fc() sid date 1 1 1 2 1 3 2 1 2 2 2 3 kable(df_panel_m4tom6) %&gt;% kable_styling_fc() sid date 1 4 1 5 1 6 2 4 2 5 2 6 kable(df_panel_m1tm6) %&gt;% kable_styling_fc() sid date 1 1 1 2 1 3 1 4 1 5 1 6 2 1 2 2 2 3 2 4 2 5 2 6 4.2 Wide and Long 4.2.1 Long to Wide Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Using the pivot_wider function in tidyr to reshape panel or other data structures 4.2.1.1 Panel Long Attendance Roster to Wide There are \\(N\\) students in class, but only a subset of them attend class each day. If student \\(id_i\\) is in class on day \\(Q\\), the teacher records on a sheet the date and the student ID. So if the student has been in class 10 times, the teacher has ten rows of recorded data for the student with two columns: column one is the student ID, and column two is the date on which the student was in class. Suppose there were 50 students, who on average attended exactly 10 classes each during the semester, this means we have \\(10 \\cdot 50\\) rows of data, with differing numbers of rows for each student. This is shown as df_panel_attend_date generated below. Now we want to generate a new dataframe, where each row is a date, and each column is a student. The values in the new dataframe shows, at the \\(Q^{th}\\) day, how many classes student \\(i\\) has attended so far. The following results is also in a REconTools Function. This is shown as df_attend_cumu_by_day generated below. First, generate the raw data structure, df_panel_attend_date: # Define it_N &lt;- 3 it_M &lt;- 5 svr_id &lt;- &#39;student_id&#39; # from : support/rand/fs_rand_draws.Rmd set.seed(222) df_panel_attend_date &lt;- as_tibble(matrix(it_M, nrow=it_N, ncol=1)) %&gt;% rowid_to_column(var = svr_id) %&gt;% uncount(V1) %&gt;% group_by(!!sym(svr_id)) %&gt;% mutate(date = row_number()) %&gt;% ungroup() %&gt;% mutate(in_class = case_when(rnorm(n(),mean=0,sd=1) &lt; 0 ~ 1, TRUE ~ 0)) %&gt;% filter(in_class == 1) %&gt;% select(!!sym(svr_id), date) %&gt;% rename(date_in_class = date) # Print kable(df_panel_attend_date) %&gt;% kable_styling_fc() student_id date_in_class 1 2 1 4 2 1 2 2 2 5 3 2 3 3 3 5 Second, generate wider data structure, df_attend_cumu_by_day: # Define svr_id &lt;- &#39;student_id&#39; svr_date &lt;- &#39;date_in_class&#39; st_idcol_prefix &lt;- &#39;sid_&#39; # Generate cumulative enrollment counts by date df_panel_attend_date_addone &lt;- df_panel_attend_date %&gt;% mutate(attended = 1) kable(df_panel_attend_date_addone) %&gt;% kable_styling_fc() student_id date_in_class attended 1 2 1 1 4 1 2 1 1 2 2 1 2 5 1 3 2 1 3 3 1 3 5 1 # Pivot Wide df_panel_attend_date_wider &lt;- df_panel_attend_date_addone %&gt;% pivot_wider(names_from = svr_id, values_from = attended) kable(df_panel_attend_date_wider) %&gt;% kable_styling_fc() date_in_class 1 2 3 2 1 1 1 4 1 NA NA 1 NA 1 NA 5 NA 1 1 3 NA NA 1 # Sort and rename # rename see: https://fanwangecon.github.io/R4Econ/amto/tibble/fs_tib_basics.html ar_unique_ids &lt;- sort(unique(df_panel_attend_date %&gt;% pull(!!sym(svr_id)))) df_panel_attend_date_wider_sort &lt;- df_panel_attend_date_wider %&gt;% arrange(!!sym(svr_date)) %&gt;% rename_at(vars(num_range(&#39;&#39;,ar_unique_ids)) , list(~paste0(st_idcol_prefix, . , &#39;&#39;)) ) kable(df_panel_attend_date_wider_sort) %&gt;% kable_styling_fc() date_in_class sid_1 sid_2 sid_3 1 NA 1 NA 2 1 1 1 3 NA NA 1 4 1 NA NA 5 NA 1 1 # replace NA and cumusum again # see: R4Econ/support/function/fs_func_multivar for renaming and replacing df_attend_cumu_by_day &lt;- df_panel_attend_date_wider_sort %&gt;% mutate_at(vars(contains(st_idcol_prefix)), list(~replace_na(., 0))) %&gt;% mutate_at(vars(contains(st_idcol_prefix)), list(~cumsum(.))) kable(df_attend_cumu_by_day) %&gt;% kable_styling_fc() date_in_class sid_1 sid_2 sid_3 1 0 1 0 2 1 2 1 3 1 2 2 4 2 2 2 5 2 3 3 The structure above is also a function in Fan’s REconTools Package, here the function is tested: # Parameters df &lt;- df_panel_attend_date svr_id_i &lt;- &#39;student_id&#39; svr_id_t &lt;- &#39;date_in_class&#39; st_idcol_prefix &lt;- &#39;sid_&#39; # Invoke Function ls_df_rosterwide &lt;- ff_panel_expand_longrosterwide(df, svr_id_t, svr_id_i, st_idcol_prefix) df_roster_wide_func &lt;- ls_df_rosterwide$df_roster_wide df_roster_wide_cumu_func &lt;- ls_df_rosterwide$df_roster_wide_cumu # Print print(df_roster_wide_func) ## # A tibble: 5 x 4 ## date_in_class sid_1 sid_2 sid_3 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 NA 1 NA ## 2 2 1 1 1 ## 3 3 NA NA 1 ## 4 4 1 NA NA ## 5 5 NA 1 1 print(df_roster_wide_cumu_func) ## # A tibble: 5 x 4 ## date_in_class sid_1 sid_2 sid_3 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 1 0 ## 2 2 1 2 1 ## 3 3 1 2 2 ## 4 4 2 2 2 ## 5 5 2 3 3 "],
["linear-regression.html", "Chapter 5 Linear Regression 5.1 OLS and IV 5.2 Decomposition", " Chapter 5 Linear Regression 5.1 OLS and IV Back to Fan’s R4Econ Homepage Table of Content 5.1.1 OLS and IV Regression Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. IV regression using AER package. Option to store all results in dataframe row for combining results from other estimations together. Produce Row Statistics. 5.1.1.1 Construct Program # IV regression function # The code below uses the AER library&#39;s regresison function # All results are stored in a single row as data_frame # This functoin could work with dplyr do # var.y is single outcome, vars.x, vars.c and vars.z are vectors of endogenous variables, controls and instruments. regf.iv &lt;- function(var.y, vars.x, vars.c, vars.z, df, transpose=TRUE) { # print(length(vars.z)) # A. Set-Up Equation str.vars.x &lt;- paste(vars.x, collapse=&#39;+&#39;) str.vars.c &lt;- paste(vars.c, collapse=&#39;+&#39;) df &lt;- df %&gt;% select(one_of(var.y, vars.x, vars.c, vars.z)) %&gt;% drop_na() %&gt;% filter_all(all_vars(!is.infinite(.))) if (length(vars.z) &gt;= 1) { # library(AER) str.vars.z &lt;- paste(vars.z, collapse=&#39;+&#39;) equa.iv &lt;- paste(var.y, paste(paste(str.vars.x, str.vars.c, sep=&#39;+&#39;), paste(str.vars.z, str.vars.c, sep=&#39;+&#39;), sep=&#39;|&#39;), sep=&#39;~&#39;) # print(equa.iv) # B. IV Regression ivreg.summ &lt;- summary(ivreg(as.formula(equa.iv), data=df), vcov = sandwich, df = Inf, diagnostics = TRUE) # C. Statistics from IV Regression # ivreg.summ$coef # ivreg.summ$diagnostics # D. Combine Regression Results into a Matrix df.results &lt;- suppressMessages(as_tibble(ivreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(as_tibble(ivreg.summ$diagnostics, rownames=&#39;rownames&#39;)) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.z=str.vars.z, vars.c=str.vars.c))) } else { # OLS regression equa.ols &lt;- paste(var.y, paste(paste(vars.x, collapse=&#39;+&#39;), paste(vars.c, collapse=&#39;+&#39;), sep=&#39;+&#39;), sep=&#39;~&#39;) lmreg.summ &lt;- summary(lm(as.formula(equa.ols), data=df)) lm.diagnostics &lt;- as_tibble(list(df1=lmreg.summ$df[[1]], df2=lmreg.summ$df[[2]], df3=lmreg.summ$df[[3]], sigma=lmreg.summ$sigma, r.squared=lmreg.summ$r.squared, adj.r.squared=lmreg.summ$adj.r.squared)) %&gt;% gather(variable, value) %&gt;% rename(rownames = variable) %&gt;% rename(v = value) df.results &lt;- suppressMessages(as_tibble(lmreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(lm.diagnostics) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.c=str.vars.c))) } # E. Flatten Matrix, All IV results as a single tibble row to be combined with other IV results df.row.results &lt;- df.results %&gt;% gather(variable, value, -rownames) %&gt;% drop_na() %&gt;% unite(esti.val, rownames, variable) %&gt;% mutate(esti.val = gsub(&#39; &#39;, &#39;&#39;, esti.val)) if (transpose) { df.row.results &lt;- df.row.results %&gt;% spread(esti.val, value) } # F. Return return(data.frame(df.row.results)) } 5.1.1.2 Program Testing Load Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) ## Parsed with column specification: ## cols( ## S.country = col_character(), ## vil.id = col_double(), ## indi.id = col_double(), ## sex = col_character(), ## svymthRound = col_double(), ## momEdu = col_double(), ## wealthIdx = col_double(), ## hgt = col_double(), ## wgt = col_double(), ## hgt0 = col_double(), ## wgt0 = col_double(), ## prot = col_double(), ## cal = col_double(), ## p.A.prot = col_double(), ## p.A.nProt = col_double() ## ) # Setting options(repr.matrix.max.rows=50, repr.matrix.max.cols=50) 5.1.1.2.1 Example No Instrument, OLS # One Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- NULL vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) ## esti.val value ## 1 (Intercept)_Estimate 52.1186286658651 ## 2 prot_Estimate 0.374472386357917 ## 3 sexMale_Estimate 0.611043720578292 ## 4 hgt0_Estimate 0.148513781160842 ## 5 wgt0_Estimate 0.00150560230505631 ## 6 (Intercept)_Std.Error 1.57770483608693 ## 7 prot_Std.Error 0.00418121191133815 ## 8 sexMale_Std.Error 0.118396259120659 ## 9 hgt0_Std.Error 0.0393807494783186 ## 10 wgt0_Std.Error 0.000187123663624397 ## 11 (Intercept)_tvalue 33.0344608660332 ## 12 prot_tvalue 89.5607288744356 ## 13 sexMale_tvalue 5.16100529794248 ## 14 hgt0_tvalue 3.77122790013449 ## 15 wgt0_tvalue 8.04602836377991 ## 16 (Intercept)_Pr(&gt;|t|) 9.92126150975783e-233 ## 17 prot_Pr(&gt;|t|) 0 ## 18 sexMale_Pr(&gt;|t|) 2.48105505495642e-07 ## 19 hgt0_Pr(&gt;|t|) 0.000162939618371183 ## 20 wgt0_Pr(&gt;|t|) 9.05257561534111e-16 ## 21 df1_v 5 ## 22 df2_v 18958 ## 23 df3_v 5 ## 24 sigma_v 8.06197784622979 ## 25 r.squared_v 0.319078711001325 ## 26 adj.r.squared_v 0.318935041565942 ## 27 vars_var.y hgt ## 28 vars_vars.x prot ## 29 vars_vars.c sex+hgt0+wgt0 5.1.1.2.2 Example 1 Insturment # One Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- c(&#39;momEdu&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## esti.val value ## 1 (Intercept)_Estimate 43.4301969117558 ## 2 prot_Estimate 0.130833343849446 ## 3 sexMale_Estimate 0.868121847262411 ## 4 hgt0_Estimate 0.412093881817148 ## 5 wgt0_Estimate 0.000858630042617921 ## 6 (Intercept)_Std.Error 1.82489550971182 ## 7 prot_Std.Error 0.0192036220809189 ## 8 sexMale_Std.Error 0.13373016700542 ## 9 hgt0_Std.Error 0.0459431912927002 ## 10 wgt0_Std.Error 0.00022691057702563 ## 11 (Intercept)_zvalue 23.798730766023 ## 12 prot_zvalue 6.81295139521853 ## 13 sexMale_zvalue 6.49159323361366 ## 14 hgt0_zvalue 8.96963990141069 ## 15 wgt0_zvalue 3.7840018472164 ## 16 (Intercept)_Pr(&gt;|z|) 3.4423766196876e-125 ## 17 prot_Pr(&gt;|z|) 9.56164541643828e-12 ## 18 sexMale_Pr(&gt;|z|) 8.49333228172763e-11 ## 19 hgt0_Pr(&gt;|z|) 2.97485394526792e-19 ## 20 wgt0_Pr(&gt;|z|) 0.000154326676608523 ## 21 Weakinstruments_df1 1 ## 22 Wu-Hausman_df1 1 ## 23 Sargan_df1 0 ## 24 Weakinstruments_df2 16394 ## 25 Wu-Hausman_df2 16393 ## 26 Weakinstruments_statistic 935.817456612075 ## 27 Wu-Hausman_statistic 123.595856606729 ## 28 Weakinstruments_p-value 6.39714929178024e-200 ## 29 Wu-Hausman_p-value 1.30703637796748e-28 ## 30 vars_var.y hgt ## 31 vars_vars.x prot ## 32 vars_vars.z momEdu ## 33 vars_vars.c sex+hgt0+wgt0 5.1.1.2.3 Example Multiple Instrucments # Multiple Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## esti.val value ## 1 (Intercept)_Estimate 42.2437613555242 ## 2 prot_Estimate 0.26699945194704 ## 3 sexMale_Estimate 0.695548488812932 ## 4 hgt0_Estimate 0.424954881263031 ## 5 wgt0_Estimate 0.000486951420329484 ## 6 (Intercept)_Std.Error 1.85356686789642 ## 7 prot_Std.Error 0.0154939347964083 ## 8 sexMale_Std.Error 0.133157977814374 ## 9 hgt0_Std.Error 0.0463195803786233 ## 10 wgt0_Std.Error 0.000224867994873235 ## 11 (Intercept)_zvalue 22.7905246296649 ## 12 prot_zvalue 17.2325142357597 ## 13 sexMale_zvalue 5.22348341593581 ## 14 hgt0_zvalue 9.17441129192849 ## 15 wgt0_zvalue 2.16549901022595 ## 16 (Intercept)_Pr(&gt;|z|) 5.69294074735747e-115 ## 17 prot_Pr(&gt;|z|) 1.51424021931607e-66 ## 18 sexMale_Pr(&gt;|z|) 1.75588197502565e-07 ## 19 hgt0_Pr(&gt;|z|) 4.54048595587756e-20 ## 20 wgt0_Pr(&gt;|z|) 0.030349491114332 ## 21 Weakinstruments_df1 4 ## 22 Wu-Hausman_df1 1 ## 23 Sargan_df1 3 ## 24 Weakinstruments_df2 14914 ## 25 Wu-Hausman_df2 14916 ## 26 Weakinstruments_statistic 274.147084958343 ## 27 Wu-Hausman_statistic 17.7562545747101 ## 28 Sargan_statistic 463.729664547249 ## 29 Weakinstruments_p-value 8.61731956233366e-228 ## 30 Wu-Hausman_p-value 2.52567249124181e-05 ## 31 Sargan_p-value 3.45452874915475e-100 ## 32 vars_var.y hgt ## 33 vars_vars.x prot ## 34 vars_vars.z momEdu+wealthIdx+p.A.prot+p.A.nProt ## 35 vars_vars.c sex+hgt0+wgt0 5.1.1.2.4 Example Multiple Endogenous Variables # Multiple Instrucments var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # Regression regf.iv(var.y, vars.x, vars.c, vars.z, df, transpose=FALSE) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## esti.val value ## 1 (Intercept)_Estimate 44.0243196254297 ## 2 prot_Estimate -1.4025623247106 ## 3 cal_Estimate 0.065104895750151 ## 4 sexMale_Estimate 0.120832787571818 ## 5 hgt0_Estimate 0.286525437984517 ## 6 wgt0_Estimate 0.000850481389651033 ## 7 (Intercept)_Std.Error 2.75354847244082 ## 8 prot_Std.Error 0.198640060273635 ## 9 cal_Std.Error 0.00758881298880996 ## 10 sexMale_Std.Error 0.209984580636303 ## 11 hgt0_Std.Error 0.0707828182888255 ## 12 wgt0_Std.Error 0.00033711210444429 ## 13 (Intercept)_zvalue 15.9882130516502 ## 14 prot_zvalue -7.06082309267581 ## 15 cal_zvalue 8.57906181719737 ## 16 sexMale_zvalue 0.575436478267434 ## 17 hgt0_zvalue 4.04795181812859 ## 18 wgt0_zvalue 2.52284441418383 ## 19 (Intercept)_Pr(&gt;|z|) 1.54396598126854e-57 ## 20 prot_Pr(&gt;|z|) 1.65519210848649e-12 ## 21 cal_Pr(&gt;|z|) 9.56500648203187e-18 ## 22 sexMale_Pr(&gt;|z|) 0.564996139463599 ## 23 hgt0_Pr(&gt;|z|) 5.16677787108928e-05 ## 24 wgt0_Pr(&gt;|z|) 0.0116409892837831 ## 25 Weakinstruments(prot)_df1 4 ## 26 Weakinstruments(cal)_df1 4 ## 27 Wu-Hausman_df1 2 ## 28 Sargan_df1 2 ## 29 Weakinstruments(prot)_df2 14914 ## 30 Weakinstruments(cal)_df2 14914 ## 31 Wu-Hausman_df2 14914 ## 32 Weakinstruments(prot)_statistic 274.147084958343 ## 33 Weakinstruments(cal)_statistic 315.036848606231 ## 34 Wu-Hausman_statistic 94.7020085425169 ## 35 Sargan_statistic 122.081979628898 ## 36 Weakinstruments(prot)_p-value 8.61731956233366e-228 ## 37 Weakinstruments(cal)_p-value 1.18918641220866e-260 ## 38 Wu-Hausman_p-value 1.35024050408262e-41 ## 39 Sargan_p-value 3.09196773720398e-27 ## 40 vars_var.y hgt ## 41 vars_vars.x prot+cal ## 42 vars_vars.z momEdu+wealthIdx+p.A.prot+p.A.nProt ## 43 vars_vars.c sex+hgt0+wgt0 5.1.1.2.5 Examples Line by Line The examples are just to test the code with different types of variables. # Selecting Variables var.y &lt;- c(&#39;hgt&#39;) vars.x &lt;- c(&#39;prot&#39;, &#39;cal&#39;) vars.z &lt;- c(&#39;momEdu&#39;, &#39;wealthIdx&#39;, &#39;p.A.prot&#39;, &#39;p.A.nProt&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;hgt0&#39;, &#39;wgt0&#39;) # A. create Equation str.vars.x &lt;- paste(vars.x, collapse=&#39;+&#39;) str.vars.c &lt;- paste(vars.c, collapse=&#39;+&#39;) str.vars.z &lt;- paste(vars.z, collapse=&#39;+&#39;) print(str.vars.x) ## [1] &quot;prot+cal&quot; print(str.vars.c) ## [1] &quot;sex+hgt0+wgt0&quot; print(str.vars.z) ## [1] &quot;momEdu+wealthIdx+p.A.prot+p.A.nProt&quot; equa.iv &lt;- paste(var.y, paste(paste(str.vars.x, str.vars.c, sep=&#39;+&#39;), paste(str.vars.z, str.vars.c, sep=&#39;+&#39;), sep=&#39;|&#39;), sep=&#39;~&#39;) print(equa.iv) ## [1] &quot;hgt~prot+cal+sex+hgt0+wgt0|momEdu+wealthIdx+p.A.prot+p.A.nProt+sex+hgt0+wgt0&quot; # B. regression res.ivreg &lt;- ivreg(as.formula(equa.iv), data=df) coef(res.ivreg) ## (Intercept) prot cal sexMale hgt0 wgt0 ## 44.0243196254 -1.4025623247 0.0651048958 0.1208327876 0.2865254380 0.0008504814 # C. Regression Summary ivreg.summ &lt;- summary(res.ivreg, vcov = sandwich, df = Inf, diagnostics = TRUE) ivreg.summ$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 44.0243196254 2.7535484724 15.9882131 1.543966e-57 ## prot -1.4025623247 0.1986400603 -7.0608231 1.655192e-12 ## cal 0.0651048958 0.0075888130 8.5790618 9.565006e-18 ## sexMale 0.1208327876 0.2099845806 0.5754365 5.649961e-01 ## hgt0 0.2865254380 0.0707828183 4.0479518 5.166778e-05 ## wgt0 0.0008504814 0.0003371121 2.5228444 1.164099e-02 ## attr(,&quot;df&quot;) ## [1] 0 ivreg.summ$diagnostics ## df1 df2 statistic p-value ## Weak instruments (prot) 4 14914 274.14708 8.617320e-228 ## Weak instruments (cal) 4 14914 315.03685 1.189186e-260 ## Wu-Hausman 2 14914 94.70201 1.350241e-41 ## Sargan 2 NA 122.08198 3.091968e-27 # D. Combine Regression Results into a Matrix df.results &lt;- suppressMessages(as_tibble(ivreg.summ$coef, rownames=&#39;rownames&#39;) %&gt;% full_join(as_tibble(ivreg.summ$diagnostics, rownames=&#39;rownames&#39;)) %&gt;% full_join(tibble(rownames=c(&#39;vars&#39;), var.y=var.y, vars.x=str.vars.x, vars.z=str.vars.z, vars.c=str.vars.c))) # E. Flatten Matrix, All IV results as a single tibble row to be combined with other IV results df.row.results &lt;- df.results %&gt;% gather(variable, value, -rownames) %&gt;% drop_na() %&gt;% unite(esti.val, rownames, variable) %&gt;% mutate(esti.val = gsub(&#39; &#39;, &#39;&#39;, esti.val)) ## Warning: attributes are not identical across measure variables; ## they will be dropped # F. Results as Single Colum df.row.results ## # A tibble: 43 x 2 ## esti.val value ## &lt;chr&gt; &lt;chr&gt; ## 1 (Intercept)_Estimate 44.0243196254297 ## 2 prot_Estimate -1.4025623247106 ## 3 cal_Estimate 0.065104895750151 ## 4 sexMale_Estimate 0.120832787571818 ## 5 hgt0_Estimate 0.286525437984517 ## 6 wgt0_Estimate 0.000850481389651033 ## 7 (Intercept)_Std.Error 2.75354847244082 ## 8 prot_Std.Error 0.198640060273635 ## 9 cal_Std.Error 0.00758881298880996 ## 10 sexMale_Std.Error 0.209984580636303 ## # ... with 33 more rows # G. Results as Single Row df.row.results ## # A tibble: 43 x 2 ## esti.val value ## &lt;chr&gt; &lt;chr&gt; ## 1 (Intercept)_Estimate 44.0243196254297 ## 2 prot_Estimate -1.4025623247106 ## 3 cal_Estimate 0.065104895750151 ## 4 sexMale_Estimate 0.120832787571818 ## 5 hgt0_Estimate 0.286525437984517 ## 6 wgt0_Estimate 0.000850481389651033 ## 7 (Intercept)_Std.Error 2.75354847244082 ## 8 prot_Std.Error 0.198640060273635 ## 9 cal_Std.Error 0.00758881298880996 ## 10 sexMale_Std.Error 0.209984580636303 ## # ... with 33 more rows df.row.results %&gt;% spread(esti.val, value) ## # A tibble: 1 x 43 ## `(Intercept)_Es~ `(Intercept)_Pr~ `(Intercept)_St~ `(Intercept)_zv~ cal_Estimate `cal_Pr(&gt;|z|)` cal_Std.Error cal_zvalue hgt0_Estimate `hgt0_Pr(&gt;|z|)` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44.0243196254297 1.5439659812685~ 2.75354847244082 15.9882130516502 0.065104895~ 9.56500648203~ 0.0075888129~ 8.5790618~ 0.2865254379~ 5.166777871089~ ## # ... with 33 more variables: hgt0_Std.Error &lt;chr&gt;, hgt0_zvalue &lt;chr&gt;, prot_Estimate &lt;chr&gt;, `prot_Pr(&gt;|z|)` &lt;chr&gt;, prot_Std.Error &lt;chr&gt;, prot_zvalue &lt;chr&gt;, ## # Sargan_df1 &lt;chr&gt;, `Sargan_p-value` &lt;chr&gt;, Sargan_statistic &lt;chr&gt;, sexMale_Estimate &lt;chr&gt;, `sexMale_Pr(&gt;|z|)` &lt;chr&gt;, sexMale_Std.Error &lt;chr&gt;, ## # sexMale_zvalue &lt;chr&gt;, vars_var.y &lt;chr&gt;, vars_vars.c &lt;chr&gt;, vars_vars.x &lt;chr&gt;, vars_vars.z &lt;chr&gt;, `Weakinstruments(cal)_df1` &lt;chr&gt;, ## # `Weakinstruments(cal)_df2` &lt;chr&gt;, `Weakinstruments(cal)_p-value` &lt;chr&gt;, `Weakinstruments(cal)_statistic` &lt;chr&gt;, `Weakinstruments(prot)_df1` &lt;chr&gt;, ## # `Weakinstruments(prot)_df2` &lt;chr&gt;, `Weakinstruments(prot)_p-value` &lt;chr&gt;, `Weakinstruments(prot)_statistic` &lt;chr&gt;, wgt0_Estimate &lt;chr&gt;, ## # `wgt0_Pr(&gt;|z|)` &lt;chr&gt;, wgt0_Std.Error &lt;chr&gt;, wgt0_zvalue &lt;chr&gt;, `Wu-Hausman_df1` &lt;chr&gt;, `Wu-Hausman_df2` &lt;chr&gt;, `Wu-Hausman_p-value` &lt;chr&gt;, ## # `Wu-Hausman_statistic` &lt;chr&gt; 5.1.2 IV Loop over RHS Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Regression with a Variety of Outcome Variables and Right Hand Side Variables. There are M outcome variables, and there are N alternative right hand side variables. Regress each M outcome variable and each N alternative right hand side variable, with some common sets of controls and perhaps shared instruments. The output file is a M by N matrix of coefficients, with proper variable names and row names. The matrix stores coefficients for this key endogenous variable. Dependency: R4Econ/linreg/ivreg/ivregdfrow.R 5.1.2.1 Construct Program The program relies on double lapply. lapply is used for convenience, not speed. ff_reg_mbyn &lt;- function(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;, time = FALSE) { # regf.iv() function is from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R if (time) { start_time &lt;- Sys.time() } if (return_all) { df.reg.out.all &lt;- bind_rows(lapply(list.vars.x, function(x) ( bind_rows(lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) ))) } else { df.reg.out.all &lt;- (lapply(list.vars.x, function(x) ( bind_rows(lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) %&gt;% select(vars_var.y, starts_with(x)) %&gt;% select(vars_var.y, ends_with(stats_ends)) ))) %&gt;% reduce(full_join) } if (time) { end_time &lt;- Sys.time() print(paste0(&#39;Estimation for all ys and xs took (seconds):&#39;, end_time - start_time)) } return(df.reg.out.all) } 5.1.2.2 Prepare Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) ## Parsed with column specification: ## cols( ## S.country = col_character(), ## vil.id = col_double(), ## indi.id = col_double(), ## sex = col_character(), ## svymthRound = col_double(), ## momEdu = col_double(), ## wealthIdx = col_double(), ## hgt = col_double(), ## wgt = col_double(), ## hgt0 = col_double(), ## wgt0 = col_double(), ## prot = col_double(), ## cal = col_double(), ## p.A.prot = col_double(), ## p.A.nProt = col_double() ## ) # Source Dependency source(&#39;C:/Users/fan/R4Econ/linreg/ivreg/ivregdfrow.R&#39;) # Setting options(repr.matrix.max.rows=50, repr.matrix.max.cols=50) Parameters. var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) var.y3 &lt;- c(&#39;vil.id&#39;) list.vars.y &lt;- c(var.y1, var.y2, var.y3) var.x1 &lt;- c(&#39;prot&#39;) var.x2 &lt;- c(&#39;cal&#39;) var.x3 &lt;- c(&#39;wealthIdx&#39;) var.x4 &lt;- c(&#39;p.A.prot&#39;) var.x5 &lt;- c(&#39;p.A.nProt&#39;) list.vars.x &lt;- c(var.x1, var.x2, var.x3, var.x4, var.x5) vars.z &lt;- c(&#39;indi.id&#39;) vars.c &lt;- c(&#39;sex&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) 5.1.2.3 Program Testing 5.1.2.3.1 Test Program OLS Z-Stat vars.z &lt;- NULL suppressMessages(ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;)) ## vars_var.y prot_tvalue cal_tvalue wealthIdx_tvalue p.A.prot_tvalue p.A.nProt_tvalue ## 1 hgt 18.8756010031786 23.4421863484661 13.508899618216 3.83682180045518 32.5448257554855 ## 2 wgt 16.3591125056062 17.3686031309332 14.1390521528113 1.36958319982295 12.0961557911467 ## 3 vil.id -14.9385580468907 -19.6150110809452 34.0972558327347 8.45943342783186 17.7801422421419 5.1.2.3.2 Test Program IV T-stat vars.z &lt;- c(&#39;indi.id&#39;) suppressMessages(ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;value&#39;)) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## vars_var.y prot_zvalue cal_zvalue wealthIdx_zvalue p.A.prot_zvalue p.A.nProt_zvalue ## 1 hgt 8.87674929300964 12.0739764947235 4.62589553677969 26.6373587567312 32.1162192385744 ## 2 wgt 5.60385871756365 6.1225187008946 5.17869536991717 11.9295584469998 12.3509307017263 ## 3 vil.id -9.22106223347162 -13.0586007975839 -51.5866689219593 -29.9627476577329 -38.3528894620707 5.1.2.3.3 Test Program OLS Coefficient vars.z &lt;- NULL suppressMessages(ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;Estimate&#39;)) ## vars_var.y prot_Estimate cal_Estimate wealthIdx_Estimate p.A.prot_Estimate p.A.nProt_Estimate ## 1 hgt 0.049431093806755 0.00243408846205622 0.21045655488185 3.86952250259526e-05 0.00542428867316449 ## 2 wgt 16.5557424523585 0.699072500364623 106.678721085969 0.00521731297924587 0.779514232050632 ## 3 vil.id -0.0758835879205584 -0.00395676177098486 0.451733304543324 0.000149388430455142 0.00526237555581024 5.1.2.3.4 Test Program IV coefficient vars.z &lt;- c(&#39;indi.id&#39;) suppressMessages(ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = FALSE, stats_ends = &#39;Estimate&#39;)) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## vars_var.y prot_Estimate cal_Estimate wealthIdx_Estimate p.A.prot_Estimate p.A.nProt_Estimate ## 1 hgt 0.859205733632614 0.0238724384575419 0.144503490136948 0.00148073028434642 0.0141317656200726 ## 2 wgt 98.9428234201406 2.71948246216953 69.1816142883022 0.221916473012486 2.11856940494335 ## 3 vil.id -6.02451379136132 -0.168054407187466 -1.91414470908345 -0.00520794333267238 -0.0494468877742109 5.1.2.3.5 Test Program OLS Return All vars.z &lt;- NULL ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = TRUE, stats_ends = &#39;Estimate&#39;) ## X.Intercept._Estimate X.Intercept._Pr...t.. X.Intercept._Std.Error X.Intercept._tvalue adj.r.squared_v df1_v df2_v df3_v hgt0_Estimate ## 1 27.3528514188608 5.68247182214952e-231 0.831272666092284 32.9047886867776 0.814249026159781 6 18957 6 0.60391817340617 ## 2 99.873884728925 0.75529705553815 320.450650378664 0.31166697465244 0.60716936506893 6 18962 6 56.3852027199184 ## 3 31.4646660224049 6.78164655340399e-84 1.61328519718754 19.503474077155 0.0373247512680971 6 18999 6 -0.296844389234445 ## 4 27.9038445914729 8.24252673989353e-242 0.828072565159449 33.6973421962119 0.81608922805658 6 18957 6 0.589847843438394 ## 5 219.626705179399 0.493216914827181 320.522532223672 0.685214557790078 0.607863678511207 6 18962 6 52.9707041800704 ## 6 30.5103987898551 1.62608789535248e-79 1.60831193651104 18.9704485163756 0.0453498711076042 6 18999 6 -0.273219210757899 ## 7 35.7840188807906 2.26726906489443e-145 1.38461348429899 25.8440491058106 0.935014931990565 6 25092 6 0.439374451256039 ## 8 -2662.74787734003 7.13318862990131e-05 670.301542938561 -3.97246270039407 0.92193683733695 6 25102 6 47.176969664749 ## 9 29.2381039651127 1.53578035267873e-124 1.22602177264147 23.8479483950102 0.059543122812776 6 30013 6 -0.35908163982046 ## 10 23.9948407749744 2.11912344053336e-165 0.86658104216672 27.6890903532576 0.814690803458616 6 18587 6 0.687269209411865 ## 11 -547.959546430028 0.0941551350855875 327.343126852912 -1.6739607509042 0.617300597776144 6 18591 6 72.105560623359 ## 12 22.3367814226238 3.04337266226599e-49 1.5098937308759 14.7936116071335 0.0261131074199838 6 18845 6 -0.108789161111504 ## 13 24.4904444950827 2.34941965806705e-181 0.843371070670838 29.0387533397398 0.824542352656376 6 18587 6 0.622395388389206 ## 14 -476.703973630552 0.143844033032183 326.132837036936 -1.46168652614567 0.620250730454724 6 18591 6 62.7336220289257 ## 15 22.7781908464511 9.58029450711211e-52 1.5004526558957 15.1808794212527 0.0385437355117917 6 18845 6 -0.157811627494693 ## hgt0_Pr...t.. hgt0_Std.Error hgt0_tvalue prot_Estimate prot_Pr...t.. prot_Std.Error prot_tvalue ## 1 1.14533314566771e-183 0.0206657538633713 29.2231378249683 0.049431093806755 9.54769322304645e-79 0.00261878251179557 18.8756010031786 ## 2 1.52417506966835e-12 7.96735224000553 7.0770314931977 16.5557424523585 9.61203373222183e-60 1.01201959743751 16.3591125056062 ## 3 1.40290395213743e-13 0.0401060913799595 -7.40147890309685 -0.0758835879205584 3.56396093562335e-50 0.00507971302734622 -14.9385580468907 ## 4 7.79174951119325e-177 0.0205836398278421 28.6561486875877 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 3.05720143843395e-11 7.96822145797115 6.64774497790599 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 8.49149153665126e-12 0.0399777363511633 -6.83428417151858 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 2.71000479249152e-36 0.0348701896610764 12.6002885423502 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 0.00520266507060071 16.8823489375743 2.79445531182864 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 2.41020063623865e-31 0.0307984635553859 -11.659076407325 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 1.31914432912869e-220 0.0213841849324282 32.1391351404584 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 4.78613024244006e-19 8.07744906400683 8.92677379355593 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 0.0034801146146182 0.0372288594891345 -2.92217281443323 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 1.11511327164938e-190 0.0208846437570215 29.8015803204665 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 8.38546282719268e-15 8.07589192978212 7.76801157994423 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 2.13723119924676e-05 0.0371223237183417 -4.25112470577158 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## r.squared_v sexMale_Estimate sexMale_Pr...t.. sexMale_Std.Error sexMale_tvalue sigma_v svymthRound_Estimate ## 1 0.814298005954592 0.935177182449406 2.36432111724607e-51 0.0618482294097262 15.1205166481668 4.21029844914315 0.87166589100565 ## 2 0.607272921412825 415.163616765357 2.48252880290814e-67 23.8518341439675 17.4059409544552 1623.77111076428 189.04290688382 ## 3 0.0375780335372857 -0.254089999175318 0.0343768259467621 0.120093045309631 -2.11577613441484 8.18491760066961 -0.0154759587993917 ## 4 0.816137722617266 0.893484662055608 2.08765935335877e-47 0.0616078355613525 14.5027763743757 4.18939119979502 0.851989049736817 ## 5 0.60796705182314 405.534891838028 2.51355675686752e-64 23.8567507583516 16.9987478993157 1622.33549880859 185.318286001897 ## 6 0.0456010419476623 -0.181389489610951 0.129768754080748 0.11972270545355 -1.51508010885476 8.15073036560541 0.0201471237605442 ## 7 0.93502787877066 1.80682463132073 1.26527362032354e-66 0.104475287357902 17.2942776901016 8.18607049768594 0.432815253441723 ## 8 0.921952383432195 999.926876716707 2.64630894140004e-86 50.5879876531386 19.7660931597596 3964.45339913597 189.877994795061 ## 9 0.0596997716363463 -0.33436777751525 0.000311174554787706 0.0927193334338799 -3.60623577771614 7.93450742809862 0.00215144302579706 ## 10 0.814740639193486 0.932686930233136 7.90489020586094e-47 0.0647209948973267 14.4108867873979 4.35662621773428 0.91961467696139 ## 11 0.617403496088206 397.141948675354 6.19449742677662e-59 24.4473730956481 16.2447698213453 1645.77655955938 205.597385664745 ## 12 0.0263714328556815 -0.445232370681998 7.93666802281971e-05 0.112797805327952 -3.94717228218682 7.6435668370875 -0.0509574460702806 ## 13 0.824589538985803 0.96466980500711 1.24556615236597e-52 0.0629827627260302 15.316409812052 4.23923961592693 0.921894094780682 ## 14 0.620352835549783 401.59056368102 1.18469030741261e-60 24.3549086073387 16.4891016491029 1639.42085007515 205.945143306004 ## 15 0.0387987636986586 -0.423829627017582 0.00015644693636154 0.112083516545945 -3.78137339083082 7.59462918474114 -0.0557204455206461 ## svymthRound_Pr...t.. svymthRound_Std.Error svymthRound_tvalue vars_var.y vars_vars.c vars_vars.x wgt0_Estimate ## 1 0 0.00387681209575621 224.840892330022 hgt sex+wgt0+hgt0+svymthRound prot -0.000146104685986986 ## 2 0 1.4955473831309 126.403823119306 wgt sex+wgt0+hgt0+svymthRound prot 0.637023553461055 ## 3 0.0397984032097113 0.00752730297891317 -2.05597660181154 vil.id sex+wgt0+hgt0+svymthRound prot -0.000903390591533867 ## 4 0 0.00411253488213795 207.168832400006 hgt sex+wgt0+hgt0+svymthRound cal -0.000116898230009949 ## 5 0 1.59266949679221 116.357025971267 wgt sex+wgt0+hgt0+svymthRound cal 0.649394003614758 ## 6 0.0117151185126433 0.00799217807522278 2.52085521254888 vil.id sex+wgt0+hgt0+svymthRound cal -0.000941137072743919 ## 7 0 0.000728323735328998 594.262183761197 hgt sex+wgt0+hgt0+svymthRound wealthIdx 0.00122231975126219 ## 8 0 0.352701518968252 538.353209678558 wgt sex+wgt0+hgt0+svymthRound wealthIdx 1.32870822160235 ## 9 0.000447277200167272 0.000612792699568233 3.51088227277012 vil.id sex+wgt0+hgt0+svymthRound wealthIdx -0.000845938526704796 ## 10 0 0.00331108017589107 277.738571133786 hgt sex+wgt0+hgt0+svymthRound p.A.prot -0.000489534836079617 ## 11 0 1.25083486490652 164.368128386085 wgt sex+wgt0+hgt0+svymthRound p.A.prot 0.580023505722658 ## 12 1.37139389802397e-18 0.00578476859618168 -8.80889965139067 vil.id sex+wgt0+hgt0+svymthRound p.A.prot -0.00156196911156061 ## 13 0 0.00317113547025635 290.714194782148 hgt sex+wgt0+hgt0+svymthRound p.A.nProt 3.23596154259101e-05 ## 14 0 1.22639878616071 167.926734460268 wgt sex+wgt0+hgt0+svymthRound p.A.nProt 0.65551206304675 ## 15 7.79141497751766e-23 0.00565696328562864 -9.84988636256528 vil.id sex+wgt0+hgt0+svymthRound p.A.nProt -0.00115432723977403 ## wgt0_Pr...t.. wgt0_Std.Error wgt0_tvalue cal_Estimate cal_Pr...t.. cal_Std.Error cal_tvalue ## 1 0.136011583497549 9.79994437486573e-05 -1.49087260496811 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 2.96480083692757e-63 0.0378027371614794 16.8512547316329 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 2.05763549729273e-06 0.000190221503167431 -4.74915073475531 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 0.230228828649018 9.74307633896921e-05 -1.19980821193398 0.00243408846205622 8.01672708877986e-120 0.000103833679413418 23.4421863484661 ## 5 7.43034302413852e-66 0.037739875283113 17.2071051836606 0.699072500364623 4.71331900885298e-67 0.0402492068645167 17.3686031309332 ## 6 6.66901196231733e-07 0.000189270503626621 -4.97244448929308 -0.00395676177098486 7.94646124029527e-85 0.000201721108117477 -19.6150110809452 ## 7 1.22269348058816e-13 0.000164767846917989 7.41843614592224 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 6.75367630221077e-62 0.0798131859486402 16.6477281392748 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 4.32675510884621e-09 0.000144040382619518 -5.872926128913 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 7.77000489086602e-07 9.90410500454311e-05 -4.94274682926991 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 7.42419220783427e-54 0.0374185042114355 15.5009805428138 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 1.40362012201826e-19 0.000172365145002826 -9.0619777654873 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 0.740027016459552 9.75208524392668e-05 0.331822524275644 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 4.09082062947785e-67 0.0377202854835204 17.3782370584956 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 2.75472781728448e-11 0.000173241059789276 -6.66312732777158 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## wealthIdx_Estimate wealthIdx_Pr...t.. wealthIdx_Std.Error wealthIdx_tvalue p.A.prot_Estimate p.A.prot_Pr...t.. p.A.prot_Std.Error ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 0.21045655488185 1.93494257274268e-41 0.0155791042075745 13.508899618216 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 106.678721085969 3.2548345535026e-45 7.54496977117083 14.1390521528113 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 0.451733304543324 4.82890644822007e-250 0.0132483771350785 34.0972558327347 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.86952250259526e-05 0.000125048896903791 1.00852286184785e-05 ## 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.00521731297924587 0.170833589209346 0.00380941660201464 ## 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.000149388430455142 2.88060045451681e-17 1.76593895713687e-05 ## 13 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## p.A.prot_tvalue p.A.nProt_Estimate p.A.nProt_Pr...t.. p.A.nProt_Std.Error p.A.nProt_tvalue ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 3.83682180045518 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 1.36958319982295 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 8.45943342783186 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 &lt;NA&gt; 0.00542428867316449 5.25341325077391e-226 0.000166671307872964 32.5448257554855 ## 14 &lt;NA&gt; 0.779514232050632 1.47950939943836e-33 0.06444313759758 12.0961557911467 ## 15 &lt;NA&gt; 0.00526237555581024 3.7685780281174e-70 0.000295969260771016 17.7801422421419 5.1.2.3.6 Test Program IV Return All vars.z &lt;- c(&#39;indi.id&#39;) ff_reg_mbyn(list.vars.y, list.vars.x, vars.c, vars.z, df, return_all = TRUE, stats_ends = &#39;Estimate&#39;) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped ## X.Intercept._Estimate X.Intercept._Pr...z.. X.Intercept._Std.Error X.Intercept._zvalue hgt0_Estimate hgt0_Pr...z.. hgt0_Std.Error ## 1 40.2173991882938 3.69748206920405e-59 2.47963650430699 16.2190704639323 0.403139725681418 1.25009876641748e-13 0.0543948312973965 ## 2 1408.1626637032 0.00217397545504963 459.377029874119 3.06537456626657 35.5765914326678 0.000445802636381424 10.1318250572006 ## 3 -64.490636067872 0.000109756271656929 16.673099250727 -3.86794531107106 1.20995060148712 0.00097112649404847 0.366789440587685 ## 4 39.6732302990235 1.30030240177373e-103 1.83545587849039 21.6149190857443 0.357976348180876 2.82141265004339e-17 0.0423453726223874 ## 5 1325.54736576331 0.00138952700443324 414.645900526211 3.19681772828602 31.0172706497394 0.0013100303315764 9.65135595900306 ## 6 -59.8304089440729 3.75547414421179e-07 11.7754321198995 -5.08095230263053 1.5037447089682 3.70002169470828e-08 0.273179527952317 ## 7 35.5561817357046 2.01357089467444e-142 1.39936229104453 25.4088465605032 0.460434521499963 2.98739737280869e-37 0.0361031059207763 ## 8 -2791.221534909 1.95034793045284e-05 653.605248808641 -4.27050048939585 59.1545587745268 0.000542570320022534 17.1025823111635 ## 9 21.8005242861645 1.17899313785408e-34 1.77547715237629 12.2786847788984 0.412512139031067 3.02226357947691e-20 0.0447499166716409 ## 10 24.3009261707644 1.97968607369592e-84 1.2481331128579 19.4698193008609 0.515794899569023 8.57492956381676e-59 0.0319035514861838 ## 11 -499.067024090554 0.155922992163314 351.723712333143 -1.41891776582254 46.2591615803265 2.8561488738123e-07 9.01263684093548 ## 12 21.4632286881661 1.84405333738942e-09 3.57067054655531 6.01097984491234 0.520812513246773 1.10039023747789e-08 0.0911390672920558 ## 13 25.299209739617 1.29388565624566e-157 0.945826571474308 26.748254386829 0.510868687340428 3.24936430168307e-102 0.0237991645877977 ## 14 -352.278518334717 0.287184942021997 330.990098562619 -1.0643173915611 45.5654716961559 6.3454545304127e-08 8.42434865398195 ## 15 17.9359211844992 1.13855583530306e-12 2.52170174723203 7.11262590993832 0.534362107844268 3.42500501176006e-17 0.063380058773461 ## hgt0_zvalue prot_Estimate prot_Pr...z.. prot_Std.Error prot_zvalue Sargan_df1 sexMale_Estimate sexMale_Pr...z.. ## 1 7.41136089709158 0.859205733632614 6.88427338202428e-19 0.0967928354481331 8.87674929300964 0 0.154043421788007 0.38807812932888 ## 2 3.51137048180512 98.9428234201406 2.09631602352917e-08 17.6561952052848 5.60385871756365 0 333.799680049259 5.06413216642981e-24 ## 3 3.29876072644971 -6.02451379136132 2.94171378745816e-20 0.653342710289155 -9.22106223347162 0 5.41175429817609 5.80077629932476e-06 ## 4 8.45373003027063 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 0.106307556057668 0.423490075745117 ## 5 3.21377335801252 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 330.452608866758 2.52735690930834e-27 ## 6 5.50460248701607 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 5.83118942788808 6.12283824664132e-12 ## 7 12.7533216258548 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 1.80283907885782 1.1689328480129e-65 ## 8 3.45880859967647 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 997.747599807148 2.02347084785411e-89 ## 9 9.21816552325528 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 -0.452827875182598 0.000647195788038449 ## 10 16.1673191711084 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 1.02741625216018 1.69796551008584e-27 ## 11 5.13270005180026 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 411.365911332896 2.05327249429949e-54 ## 12 5.71448149208973 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 -0.789122421167432 0.00428270841484855 ## 13 21.4658243761363 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 1.02009164592608 1.70848440093529e-51 ## 14 5.40878275196011 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 409.820707458838 2.36314216739034e-62 ## 15 8.4310762436216 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0 -0.746032636368145 6.57521045473888e-05 ## sexMale_Std.Error sexMale_zvalue svymthRound_Estimate svymthRound_Pr...z.. svymthRound_Std.Error svymthRound_zvalue vars_var.y ## 1 0.178475271469781 0.86310792817082 0.20990165085783 0.00846239710392287 0.0797183179471441 2.63304164291327 hgt ## 2 33.0216035385405 10.1085242471545 121.78985943172 5.96047652813855e-17 14.5577085129475 8.36600480930094 wgt ## 3 1.19371921154418 4.53352366774387 4.84745570027424 2.07373887977152e-19 0.538050140685815 9.00930105527994 vil.id ## 4 0.132821186086547 0.800381017440976 0.322893837128574 9.66146445882893e-11 0.0498896912188091 6.47215545416802 hgt ## 5 30.5174257711927 10.8283251459136 135.494858749214 4.48931446042076e-34 11.133488331472 12.1700274626596 wgt ## 6 0.847955715223327 6.87676174970095 4.07024693316581 5.64723572160763e-36 0.325043349284718 12.5221664806331 vil.id ## 7 0.105343525210948 17.113904962338 0.433164820953121 0 0.00120472816008751 359.553993426746 hgt ## 8 49.7632792630648 20.0498764266063 190.07735139541 0 0.739269879490032 257.11496798237 wgt ## 9 0.132754263303719 -3.41102322376347 0.0137438264666969 1.57416908709431e-66 0.000797655931686456 17.2302692435808 vil.id ## 10 0.0945646985181925 10.8646912458831 1.00582859923509 0 0.00746867714609297 134.672925279848 hgt ## 11 26.4822313532216 15.5336574870174 218.549980922774 0 1.9315711781906 113.146221785884 wgt ## 12 0.276250047248363 -2.85655126226267 -0.369567838754916 2.42696379701225e-102 0.0172056989832505 -21.4793853545086 vil.id ## 13 0.0675715533063635 15.0964658352764 0.929266902426869 0 0.00539330635998817 172.300040161061 hgt ## 14 24.5920104216267 16.6647907361992 207.078222946319 0 1.46167854745858 141.671520941705 wgt ## 15 0.18692145837209 -3.99115565898846 -0.0985678389223824 1.84569897952709e-27 0.00907867488118012 -10.8570733297996 vil.id ## vars_vars.c vars_vars.x vars_vars.z Weakinstruments_df1 Weakinstruments_df2 Weakinstruments_p.value Weakinstruments_statistic ## 1 sex+wgt0+hgt0+svymthRound prot indi.id 1 18957 1.42153759923994e-19 82.0931934821266 ## 2 sex+wgt0+hgt0+svymthRound prot indi.id 1 18962 4.45734829676713e-19 79.8251182827386 ## 3 sex+wgt0+hgt0+svymthRound prot indi.id 1 18999 5.72345606957941e-20 83.8989817367586 ## 4 sex+wgt0+hgt0+svymthRound cal indi.id 1 18957 1.77770827184424e-37 164.392129625299 ## 5 sex+wgt0+hgt0+svymthRound cal indi.id 1 18962 4.03760292920738e-37 162.747072038429 ## 6 sex+wgt0+hgt0+svymthRound cal indi.id 1 18999 5.47447735093002e-38 166.75260665498 ## 7 sex+wgt0+hgt0+svymthRound wealthIdx indi.id 1 25092 0 7029.47383089383 ## 8 sex+wgt0+hgt0+svymthRound wealthIdx indi.id 1 25102 0 7038.38467113128 ## 9 sex+wgt0+hgt0+svymthRound wealthIdx indi.id 1 30013 0 12942.6315513372 ## 10 sex+wgt0+hgt0+svymthRound p.A.prot indi.id 1 18587 0 1710.98122418591 ## 11 sex+wgt0+hgt0+svymthRound p.A.prot indi.id 1 18591 0 1715.15052113399 ## 12 sex+wgt0+hgt0+svymthRound p.A.prot indi.id 1 18845 0 1725.71954882902 ## 13 sex+wgt0+hgt0+svymthRound p.A.nProt indi.id 1 18587 0 5097.88462603711 ## 14 sex+wgt0+hgt0+svymthRound p.A.nProt indi.id 1 18591 0 5110.7741807338 ## 15 sex+wgt0+hgt0+svymthRound p.A.nProt indi.id 1 18845 0 5136.55662964887 ## wgt0_Estimate wgt0_Pr...z.. wgt0_Std.Error wgt0_zvalue Wu.Hausman_df1 Wu.Hausman_df2 Wu.Hausman_p.value ## 1 -0.00163274724538111 4.88365163639597e-08 0.00029928487659495 -5.45549532591606 1 18956 1.53929570343279e-118 ## 2 0.492582112313709 2.33136555228405e-20 0.0532753838702833 9.24596082710666 1 18961 3.13415891402799e-08 ## 3 0.00999798623641602 7.95432753711715e-07 0.00202532507408065 4.93648469787221 1 18998 0 ## 4 -0.000658938519302931 0.00032843149807424 0.000183457551985601 -3.59177647456371 1 18956 2.88592507054107e-108 ## 5 0.601258436431587 2.0921134733036e-48 0.0411255751282477 14.6200614716414 1 18961 7.6495944085204e-07 ## 6 0.00326074237566435 0.00667886646012294 0.00120214094164169 2.71244598924594 1 18998 0 ## 7 0.00112485055604169 2.26123807446765e-11 0.000168187467853553 6.68807593334564 1 25091 0.0221987672063003 ## 8 1.27282038539707 6.67525280062144e-56 0.08080475140115 15.7518012657231 1 25101 0.0099360023036833 ## 9 -0.00512158791392237 6.51923753120087e-127 0.000213715312589078 -23.9645341827701 1 30012 0 ## 10 0.000716628918444932 2.43477572076212e-06 0.000152036990658929 4.71351685756907 1 18586 1.80909125272768e-238 ## 11 0.761704518610475 8.2201479288098e-69 0.0434474820359048 17.531614789115 1 18590 2.14946499922491e-35 ## 12 -0.00601345031606092 5.19751747217521e-44 0.00043218241369976 -13.9141485757875 1 18844 0 ## 13 0.000922100117259348 1.68237436753105e-15 0.00011580150512068 7.96276452796019 1 18586 3.15182965429765e-108 ## 14 0.792700893714085 4.81415543564975e-82 0.0413159097814445 19.1863351892132 1 18590 1.7681125741529e-17 ## 15 -0.00668277875606482 2.54848840100353e-105 0.000306609919182859 -21.7957030675165 1 18844 0 ## Wu.Hausman_statistic cal_Estimate cal_Pr...z.. cal_Std.Error cal_zvalue wealthIdx_Estimate wealthIdx_Pr...z.. ## 1 543.467268879953 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 30.6481856102772 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 5652.51924792859 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 494.955883488045 0.0238724384575419 1.44956616452661e-33 0.00197718112735887 12.0739764947235 &lt;NA&gt; &lt;NA&gt; ## 5 24.4605456760994 2.71948246216953 9.21076021290446e-10 0.444177077282291 6.1225187008946 &lt;NA&gt; &lt;NA&gt; ## 6 5583.56513052781 -0.168054407187466 5.67614501764414e-39 0.0128692506794877 -13.0586007975839 &lt;NA&gt; &lt;NA&gt; ## 7 5.23078768861684 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.144503490136948 3.72983264926432e-06 ## 8 6.6473469952822 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 69.1816142883022 2.23442991281176e-07 ## 9 25949.7118056025 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; -1.91414470908345 0 ## 10 1119.87022468742 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 154.793296861581 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 4826.92242730041 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 494.903094649183 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 72.530787010352 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 7607.83405438193 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## wealthIdx_Std.Error wealthIdx_zvalue p.A.prot_Estimate p.A.prot_Pr...z.. p.A.prot_Std.Error p.A.prot_zvalue p.A.nProt_Estimate ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 0.0312379492766376 4.62589553677969 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 13.358888551386 5.17869536991717 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 0.0371054140359243 -51.5866689219593 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; &lt;NA&gt; 0.00148073028434642 2.50759287066563e-156 5.55884799941827e-05 26.6373587567312 &lt;NA&gt; ## 11 &lt;NA&gt; &lt;NA&gt; 0.221916473012486 8.30126393398654e-33 0.0186022369560791 11.9295584469998 &lt;NA&gt; ## 12 &lt;NA&gt; &lt;NA&gt; -0.00520794333267238 3.00201194005694e-197 0.000173813943639721 -29.9627476577329 &lt;NA&gt; ## 13 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.0141317656200726 ## 14 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2.11856940494335 ## 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; -0.0494468877742109 ## p.A.nProt_Pr...z.. p.A.nProt_Std.Error p.A.nProt_zvalue ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 2.61782083774363e-226 0.000440019589949091 32.1162192385744 ## 14 4.81511329043196e-35 0.17153115470458 12.3509307017263 ## 15 0 0.00128926108222202 -38.3528894620707 5.1.2.4 Program Line by Line Set Up Parameters vars.z &lt;- c(&#39;indi.id&#39;) vars.z &lt;- NULL vars.c &lt;- c(&#39;sex&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) 5.1.2.4.1 Lapply df.reg.out &lt;- as_tibble(bind_rows(lapply(list.vars.y, regf.iv, vars.x=var.x1, vars.c=vars.c, vars.z=vars.z, df=df))) 5.1.2.4.2 Nested Lapply Test lapply(list.vars.y, function(y) (mean(df[[var.x1]], na.rm=TRUE) + mean(df[[y]], na.rm=TRUE))) ## [[1]] ## [1] 98.3272 ## ## [[2]] ## [1] 13626.51 ## ## [[3]] ## [1] 26.11226 lapplytwice &lt;- lapply(list.vars.x, function(x) (lapply(list.vars.y, function(y) (mean(df[[x]], na.rm=TRUE) + mean(df[[y]], na.rm=TRUE))))) lapplytwice ## [[1]] ## [[1]][[1]] ## [1] 98.3272 ## ## [[1]][[2]] ## [1] 13626.51 ## ## [[1]][[3]] ## [1] 26.11226 ## ## ## [[2]] ## [[2]][[1]] ## [1] 525.4708 ## ## [[2]][[2]] ## [1] 14053.65 ## ## [[2]][[3]] ## [1] 453.2558 ## ## ## [[3]] ## [[3]][[1]] ## [1] 90.69287 ## ## [[3]][[2]] ## [1] 13618.87 ## ## [[3]][[3]] ## [1] 18.47793 ## ## ## [[4]] ## [[4]][[1]] ## [1] 2095.3 ## ## [[4]][[2]] ## [1] 15623.48 ## ## [[4]][[3]] ## [1] 2023.085 ## ## ## [[5]] ## [[5]][[1]] ## [1] 271.2886 ## ## [[5]][[2]] ## [1] 13799.47 ## ## [[5]][[3]] ## [1] 199.0737 5.1.2.4.3 Nested Lapply All df.reg.out.all &lt;- bind_rows(lapply(list.vars.x, function(x) ( bind_rows(lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) ))) df.reg.out.all ## X.Intercept._Estimate X.Intercept._Pr...t.. X.Intercept._Std.Error X.Intercept._tvalue adj.r.squared_v df1_v df2_v df3_v hgt0_Estimate ## 1 27.3528514188608 5.68247182214952e-231 0.831272666092284 32.9047886867776 0.814249026159781 6 18957 6 0.60391817340617 ## 2 99.873884728925 0.75529705553815 320.450650378664 0.31166697465244 0.60716936506893 6 18962 6 56.3852027199184 ## 3 31.4646660224049 6.78164655340399e-84 1.61328519718754 19.503474077155 0.0373247512680971 6 18999 6 -0.296844389234445 ## 4 27.9038445914729 8.24252673989353e-242 0.828072565159449 33.6973421962119 0.81608922805658 6 18957 6 0.589847843438394 ## 5 219.626705179399 0.493216914827181 320.522532223672 0.685214557790078 0.607863678511207 6 18962 6 52.9707041800704 ## 6 30.5103987898551 1.62608789535248e-79 1.60831193651104 18.9704485163756 0.0453498711076042 6 18999 6 -0.273219210757899 ## 7 35.7840188807906 2.26726906489443e-145 1.38461348429899 25.8440491058106 0.935014931990565 6 25092 6 0.439374451256039 ## 8 -2662.74787734003 7.13318862990131e-05 670.301542938561 -3.97246270039407 0.92193683733695 6 25102 6 47.176969664749 ## 9 29.2381039651127 1.53578035267873e-124 1.22602177264147 23.8479483950102 0.059543122812776 6 30013 6 -0.35908163982046 ## 10 23.9948407749744 2.11912344053336e-165 0.86658104216672 27.6890903532576 0.814690803458616 6 18587 6 0.687269209411865 ## 11 -547.959546430028 0.0941551350855875 327.343126852912 -1.6739607509042 0.617300597776144 6 18591 6 72.105560623359 ## 12 22.3367814226238 3.04337266226599e-49 1.5098937308759 14.7936116071335 0.0261131074199838 6 18845 6 -0.108789161111504 ## 13 24.4904444950827 2.34941965806705e-181 0.843371070670838 29.0387533397398 0.824542352656376 6 18587 6 0.622395388389206 ## 14 -476.703973630552 0.143844033032183 326.132837036936 -1.46168652614567 0.620250730454724 6 18591 6 62.7336220289257 ## 15 22.7781908464511 9.58029450711211e-52 1.5004526558957 15.1808794212527 0.0385437355117917 6 18845 6 -0.157811627494693 ## hgt0_Pr...t.. hgt0_Std.Error hgt0_tvalue prot_Estimate prot_Pr...t.. prot_Std.Error prot_tvalue ## 1 1.14533314566771e-183 0.0206657538633713 29.2231378249683 0.049431093806755 9.54769322304645e-79 0.00261878251179557 18.8756010031786 ## 2 1.52417506966835e-12 7.96735224000553 7.0770314931977 16.5557424523585 9.61203373222183e-60 1.01201959743751 16.3591125056062 ## 3 1.40290395213743e-13 0.0401060913799595 -7.40147890309685 -0.0758835879205584 3.56396093562335e-50 0.00507971302734622 -14.9385580468907 ## 4 7.79174951119325e-177 0.0205836398278421 28.6561486875877 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 3.05720143843395e-11 7.96822145797115 6.64774497790599 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 8.49149153665126e-12 0.0399777363511633 -6.83428417151858 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 2.71000479249152e-36 0.0348701896610764 12.6002885423502 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 0.00520266507060071 16.8823489375743 2.79445531182864 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 2.41020063623865e-31 0.0307984635553859 -11.659076407325 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 1.31914432912869e-220 0.0213841849324282 32.1391351404584 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 4.78613024244006e-19 8.07744906400683 8.92677379355593 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 0.0034801146146182 0.0372288594891345 -2.92217281443323 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 1.11511327164938e-190 0.0208846437570215 29.8015803204665 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 8.38546282719268e-15 8.07589192978212 7.76801157994423 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 2.13723119924676e-05 0.0371223237183417 -4.25112470577158 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## r.squared_v sexMale_Estimate sexMale_Pr...t.. sexMale_Std.Error sexMale_tvalue sigma_v svymthRound_Estimate ## 1 0.814298005954592 0.935177182449406 2.36432111724607e-51 0.0618482294097262 15.1205166481668 4.21029844914315 0.87166589100565 ## 2 0.607272921412825 415.163616765357 2.48252880290814e-67 23.8518341439675 17.4059409544552 1623.77111076428 189.04290688382 ## 3 0.0375780335372857 -0.254089999175318 0.0343768259467621 0.120093045309631 -2.11577613441484 8.18491760066961 -0.0154759587993917 ## 4 0.816137722617266 0.893484662055608 2.08765935335877e-47 0.0616078355613525 14.5027763743757 4.18939119979502 0.851989049736817 ## 5 0.60796705182314 405.534891838028 2.51355675686752e-64 23.8567507583516 16.9987478993157 1622.33549880859 185.318286001897 ## 6 0.0456010419476623 -0.181389489610951 0.129768754080748 0.11972270545355 -1.51508010885476 8.15073036560541 0.0201471237605442 ## 7 0.93502787877066 1.80682463132073 1.26527362032354e-66 0.104475287357902 17.2942776901016 8.18607049768594 0.432815253441723 ## 8 0.921952383432195 999.926876716707 2.64630894140004e-86 50.5879876531386 19.7660931597596 3964.45339913597 189.877994795061 ## 9 0.0596997716363463 -0.33436777751525 0.000311174554787706 0.0927193334338799 -3.60623577771614 7.93450742809862 0.00215144302579706 ## 10 0.814740639193486 0.932686930233136 7.90489020586094e-47 0.0647209948973267 14.4108867873979 4.35662621773428 0.91961467696139 ## 11 0.617403496088206 397.141948675354 6.19449742677662e-59 24.4473730956481 16.2447698213453 1645.77655955938 205.597385664745 ## 12 0.0263714328556815 -0.445232370681998 7.93666802281971e-05 0.112797805327952 -3.94717228218682 7.6435668370875 -0.0509574460702806 ## 13 0.824589538985803 0.96466980500711 1.24556615236597e-52 0.0629827627260302 15.316409812052 4.23923961592693 0.921894094780682 ## 14 0.620352835549783 401.59056368102 1.18469030741261e-60 24.3549086073387 16.4891016491029 1639.42085007515 205.945143306004 ## 15 0.0387987636986586 -0.423829627017582 0.00015644693636154 0.112083516545945 -3.78137339083082 7.59462918474114 -0.0557204455206461 ## svymthRound_Pr...t.. svymthRound_Std.Error svymthRound_tvalue vars_var.y vars_vars.c vars_vars.x wgt0_Estimate ## 1 0 0.00387681209575621 224.840892330022 hgt sex+wgt0+hgt0+svymthRound prot -0.000146104685986986 ## 2 0 1.4955473831309 126.403823119306 wgt sex+wgt0+hgt0+svymthRound prot 0.637023553461055 ## 3 0.0397984032097113 0.00752730297891317 -2.05597660181154 vil.id sex+wgt0+hgt0+svymthRound prot -0.000903390591533867 ## 4 0 0.00411253488213795 207.168832400006 hgt sex+wgt0+hgt0+svymthRound cal -0.000116898230009949 ## 5 0 1.59266949679221 116.357025971267 wgt sex+wgt0+hgt0+svymthRound cal 0.649394003614758 ## 6 0.0117151185126433 0.00799217807522278 2.52085521254888 vil.id sex+wgt0+hgt0+svymthRound cal -0.000941137072743919 ## 7 0 0.000728323735328998 594.262183761197 hgt sex+wgt0+hgt0+svymthRound wealthIdx 0.00122231975126219 ## 8 0 0.352701518968252 538.353209678558 wgt sex+wgt0+hgt0+svymthRound wealthIdx 1.32870822160235 ## 9 0.000447277200167272 0.000612792699568233 3.51088227277012 vil.id sex+wgt0+hgt0+svymthRound wealthIdx -0.000845938526704796 ## 10 0 0.00331108017589107 277.738571133786 hgt sex+wgt0+hgt0+svymthRound p.A.prot -0.000489534836079617 ## 11 0 1.25083486490652 164.368128386085 wgt sex+wgt0+hgt0+svymthRound p.A.prot 0.580023505722658 ## 12 1.37139389802397e-18 0.00578476859618168 -8.80889965139067 vil.id sex+wgt0+hgt0+svymthRound p.A.prot -0.00156196911156061 ## 13 0 0.00317113547025635 290.714194782148 hgt sex+wgt0+hgt0+svymthRound p.A.nProt 3.23596154259101e-05 ## 14 0 1.22639878616071 167.926734460268 wgt sex+wgt0+hgt0+svymthRound p.A.nProt 0.65551206304675 ## 15 7.79141497751766e-23 0.00565696328562864 -9.84988636256528 vil.id sex+wgt0+hgt0+svymthRound p.A.nProt -0.00115432723977403 ## wgt0_Pr...t.. wgt0_Std.Error wgt0_tvalue cal_Estimate cal_Pr...t.. cal_Std.Error cal_tvalue ## 1 0.136011583497549 9.79994437486573e-05 -1.49087260496811 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 2.96480083692757e-63 0.0378027371614794 16.8512547316329 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 2.05763549729273e-06 0.000190221503167431 -4.74915073475531 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 0.230228828649018 9.74307633896921e-05 -1.19980821193398 0.00243408846205622 8.01672708877986e-120 0.000103833679413418 23.4421863484661 ## 5 7.43034302413852e-66 0.037739875283113 17.2071051836606 0.699072500364623 4.71331900885298e-67 0.0402492068645167 17.3686031309332 ## 6 6.66901196231733e-07 0.000189270503626621 -4.97244448929308 -0.00395676177098486 7.94646124029527e-85 0.000201721108117477 -19.6150110809452 ## 7 1.22269348058816e-13 0.000164767846917989 7.41843614592224 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 6.75367630221077e-62 0.0798131859486402 16.6477281392748 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 4.32675510884621e-09 0.000144040382619518 -5.872926128913 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 7.77000489086602e-07 9.90410500454311e-05 -4.94274682926991 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 7.42419220783427e-54 0.0374185042114355 15.5009805428138 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 1.40362012201826e-19 0.000172365145002826 -9.0619777654873 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 0.740027016459552 9.75208524392668e-05 0.331822524275644 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 4.09082062947785e-67 0.0377202854835204 17.3782370584956 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 2.75472781728448e-11 0.000173241059789276 -6.66312732777158 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## wealthIdx_Estimate wealthIdx_Pr...t.. wealthIdx_Std.Error wealthIdx_tvalue p.A.prot_Estimate p.A.prot_Pr...t.. p.A.prot_Std.Error ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 0.21045655488185 1.93494257274268e-41 0.0155791042075745 13.508899618216 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 106.678721085969 3.2548345535026e-45 7.54496977117083 14.1390521528113 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 0.451733304543324 4.82890644822007e-250 0.0132483771350785 34.0972558327347 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.86952250259526e-05 0.000125048896903791 1.00852286184785e-05 ## 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.00521731297924587 0.170833589209346 0.00380941660201464 ## 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.000149388430455142 2.88060045451681e-17 1.76593895713687e-05 ## 13 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## p.A.prot_tvalue p.A.nProt_Estimate p.A.nProt_Pr...t.. p.A.nProt_Std.Error p.A.nProt_tvalue ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 3.83682180045518 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 1.36958319982295 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 8.45943342783186 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 &lt;NA&gt; 0.00542428867316449 5.25341325077391e-226 0.000166671307872964 32.5448257554855 ## 14 &lt;NA&gt; 0.779514232050632 1.47950939943836e-33 0.06444313759758 12.0961557911467 ## 15 &lt;NA&gt; 0.00526237555581024 3.7685780281174e-70 0.000295969260771016 17.7801422421419 5.1.2.4.4 Nested Lapply Select df.reg.out.all &lt;- (lapply(list.vars.x, function(x) ( bind_rows(lapply(list.vars.y, regf.iv, vars.x=x, vars.c=vars.c, vars.z=vars.z, df=df)) %&gt;% select(vars_var.y, starts_with(x)) %&gt;% select(vars_var.y, ends_with(&#39;value&#39;)) ))) %&gt;% reduce(full_join) ## Joining, by = &quot;vars_var.y&quot;Joining, by = &quot;vars_var.y&quot;Joining, by = &quot;vars_var.y&quot;Joining, by = &quot;vars_var.y&quot; df.reg.out.all ## vars_var.y prot_tvalue cal_tvalue wealthIdx_tvalue p.A.prot_tvalue p.A.nProt_tvalue ## 1 hgt 18.8756010031786 23.4421863484661 13.508899618216 3.83682180045518 32.5448257554855 ## 2 wgt 16.3591125056062 17.3686031309332 14.1390521528113 1.36958319982295 12.0961557911467 ## 3 vil.id -14.9385580468907 -19.6150110809452 34.0972558327347 8.45943342783186 17.7801422421419 5.2 Decomposition 5.2.1 Decompose RHS Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. One runs a number of regressions. With different outcomes, and various right hand side variables. What is the remaining variation in the left hand side variable if right hand side variable one by one is set to the average of the observed values. Dependency: R4Econ/linreg/ivreg/ivregdfrow.R The code below does not work with categorical variables (except for dummies). Dummy variable inputs need to be converted to zero/one first. 5.2.1.1 Decomposition Program ff_lr_decompose &lt;- function(df, vars.y, vars.x, vars.c, vars.z, vars.other.keep, list.vars.tomean, list.vars.tomean.name.suffix, df.reg.out = NULL, graph=FALSE, graph.nrow=2) { vars.xc &lt;- c(vars.x, vars.c) # Regressions # regf.iv from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R if(is.null(df.reg.out)) { df.reg.out &lt;- as_tibble( bind_rows(lapply(vars.y, regf.iv, vars.x=vars.x, vars.c=vars.c, vars.z=vars.z, df=df))) } # Select Variables str.esti.suffix &lt;- &#39;_Estimate&#39; arr.esti.name &lt;- paste0(vars.xc, str.esti.suffix) str.outcome.name &lt;- &#39;vars_var.y&#39; arr.columns2select &lt;- c(arr.esti.name, str.outcome.name) # arr.columns2select # Generate dataframe for coefficients df.coef &lt;- df.reg.out[,c(arr.columns2select)] %&gt;% mutate_at(vars(arr.esti.name), as.numeric) %&gt;% column_to_rownames(str.outcome.name) # df.coef # str(df.coef) # Decomposition Step 1: gather df.decompose &lt;- df %&gt;% filter(svymthRound %in% c(12, 18, 24)) %&gt;% select(one_of(c(vars.other.keep, vars.xc, vars.y))) %&gt;% drop_na() %&gt;% gather(variable, value, -one_of(c(vars.other.keep, vars.xc))) # Decomposition Step 2: mutate_at(vars, funs(mean = mean(.))) # the xc averaging could have taken place earlier, no difference in mean across variables df.decompose &lt;- df.decompose %&gt;% group_by(variable) %&gt;% mutate_at(vars(c(vars.xc, &#39;value&#39;)), funs(mean = mean(.))) %&gt;% ungroup() # Decomposition Step 3 With Loop for (i in 1:length(list.vars.tomean)) { var.decomp.cur &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[i]])) vars.tomean &lt;- list.vars.tomean[[i]] var.decomp.cur df.decompose &lt;- df.decompose %&gt;% mutate((!!var.decomp.cur) := ff_lr_decompose_valadj(., df.coef, vars.tomean, str.esti.suffix)) } # Additional Statistics df.decompose.var.frac &lt;- df.decompose %&gt;% select(variable, contains(&#39;value&#39;)) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% select(variable, matches(&#39;value&#39;)) %&gt;% select(variable, ends_with(&quot;_var&quot;)) %&gt;% mutate_if(is.numeric, funs( frac = (./value_var))) %&gt;% mutate_if(is.numeric, round, 3) # Graph g.graph.dist &lt;- NULL if (graph) { g.graph.dist &lt;- df.decompose %&gt;% select(variable, contains(&#39;value&#39;), -value_mean) %&gt;% rename(outcome = variable) %&gt;% gather(variable, value, -outcome) %&gt;% ggplot(aes(x=value, color = variable, fill = variable)) + geom_line(stat = &quot;density&quot;) + facet_wrap(~ outcome, scales=&#39;free&#39;, nrow=graph.nrow) } # Return return(list(dfmain = df.decompose, dfsumm = df.decompose.var.frac, graph = g.graph.dist)) } # Support Function ff_lr_decompose_valadj &lt;- function(df, df.coef, vars.tomean, str.esti.suffix) { new_value &lt;- (df$value + rowSums((df[paste0(vars.tomean, &#39;_mean&#39;)] - df[vars.tomean]) *df.coef[df$variable, paste0(vars.tomean, str.esti.suffix)])) return(new_value) } 5.2.1.2 Prepare Decomposition Data # Library library(tidyverse) library(AER) # Load Sample Data setwd(&#39;C:/Users/fan/R4Econ/_data/&#39;) df &lt;- read_csv(&#39;height_weight.csv&#39;) ## Parsed with column specification: ## cols( ## S.country = col_character(), ## vil.id = col_double(), ## indi.id = col_double(), ## sex = col_character(), ## svymthRound = col_double(), ## momEdu = col_double(), ## wealthIdx = col_double(), ## hgt = col_double(), ## wgt = col_double(), ## hgt0 = col_double(), ## wgt0 = col_double(), ## prot = col_double(), ## cal = col_double(), ## p.A.prot = col_double(), ## p.A.nProt = col_double() ## ) # Source Dependency source(&#39;C:/Users/fan/R4Econ/linreg/ivreg/ivregdfrow.R&#39;) # Setting options(repr.matrix.max.rows=50, repr.matrix.max.cols=50) Data Cleaning. # Convert Variable for Sex which is categorical to Numeric df &lt;- df df$male &lt;- (as.numeric(factor(df$sex)) - 1) summary(factor(df$sex)) ## Female Male ## 16446 18619 summary(df$male) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 0.000 1.000 0.531 1.000 1.000 Parameters. var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) vars.y &lt;- c(var.y1, var.y2) vars.x &lt;- c(&#39;prot&#39;) vars.c &lt;- c(&#39;male&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) vars.other.keep &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;, &#39;svymthRound&#39;) # Decompose sequence vars.tomean.first &lt;- c(&#39;male&#39;, &#39;hgt0&#39;) var.tomean.first.name.suffix &lt;- &#39;_A&#39; vars.tomean.third &lt;- c(vars.tomean.first, &#39;prot&#39;) var.tomean.third.name.suffix &lt;- &#39;_B&#39; vars.tomean.fourth &lt;- c(vars.tomean.third, &#39;svymthRound&#39;) var.tomean.fourth.name.suffix &lt;- &#39;_C&#39; list.vars.tomean = list(vars.tomean.first, vars.tomean.third, vars.tomean.fourth) list.vars.tomean.name.suffix &lt;- list(var.tomean.first.name.suffix, var.tomean.third.name.suffix, var.tomean.fourth.name.suffix) 5.2.1.3 Example Guatemala OLS df.use &lt;- df %&gt;% filter(S.country == &#39;Guatemala&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) vars.z &lt;- NULL list.out &lt;- ff_lr_decompose(df=df.use, vars.y, vars.x, vars.c, vars.z, vars.other.keep, list.vars.tomean, list.vars.tomean.name.suffix, graph=TRUE, graph.nrow=1) options(repr.matrix.max.rows=10, repr.matrix.max.cols=50) list.out$dfmain ## # A tibble: 1,382 x 19 ## S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean value_A ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Guatemala 3 1352 18 13.3 1 2545. 47.4 hgt 70.2 20.6 0.550 3312. 49.8 18.4 73.4 71.4 ## 2 Guatemala 3 1352 24 46.3 1 2545. 47.4 hgt 75.8 20.6 0.550 3312. 49.8 18.4 73.4 77.0 ## 3 Guatemala 3 1354 12 1 1 3634. 51.2 hgt 66.3 20.6 0.550 3312. 49.8 18.4 73.4 65.2 ## 4 Guatemala 3 1354 18 9.8 1 3634. 51.2 hgt 69.2 20.6 0.550 3312. 49.8 18.4 73.4 68.1 ## 5 Guatemala 3 1354 24 15.4 1 3634. 51.2 hgt 75.3 20.6 0.550 3312. 49.8 18.4 73.4 74.2 ## 6 Guatemala 3 1356 12 8.6 1 3912. 51.9 hgt 68.1 20.6 0.550 3312. 49.8 18.4 73.4 66.6 ## 7 Guatemala 3 1356 18 17.8 1 3912. 51.9 hgt 74.1 20.6 0.550 3312. 49.8 18.4 73.4 72.6 ## 8 Guatemala 3 1356 24 30.5 1 3912. 51.9 hgt 77.1 20.6 0.550 3312. 49.8 18.4 73.4 75.6 ## 9 Guatemala 3 1357 12 1 1 3791. 52.6 hgt 71.5 20.6 0.550 3312. 49.8 18.4 73.4 69.6 ## 10 Guatemala 3 1357 18 12.7 1 3791. 52.6 hgt 77.8 20.6 0.550 3312. 49.8 18.4 73.4 75.9 ## # ... with 1,372 more rows, and 2 more variables: value_B &lt;dbl&gt;, value_C &lt;dbl&gt; options(repr.plot.width = 10, repr.plot.height = 4) list.out$dfsumm ## # A tibble: 2 x 11 ## variable value_var value_mean_var value_A_var value_B_var value_C_var value_var_frac value_mean_var_fr~ value_A_var_frac value_B_var_frac value_C_var_frac ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 21.9 NA 20.3 18.4 8.40 1 NA 0.927 0.841 0.384 ## 2 wgt 2965693. NA 2863501. 2659434. 2346297. 1 NA 0.966 0.897 0.791 5.2.1.4 Example Guatemala IV = vil.id df.use &lt;- df %&gt;% filter(S.country == &#39;Guatemala&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) vars.z &lt;- c(&#39;vil.id&#39;) list.out &lt;- ff_lr_decompose( df=df.use, vars.y, vars.x, vars.c, vars.z, vars.other.keep, list.vars.tomean, list.vars.tomean.name.suffix, graph=TRUE, graph.nrow=1) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped list.out$dfsumm ## # A tibble: 2 x 11 ## variable value_var value_mean_var value_A_var value_B_var value_C_var value_var_frac value_mean_var_fr~ value_A_var_frac value_B_var_frac value_C_var_frac ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 21.9 NA 20.2 16.3 10.0 1 NA 0.926 0.747 0.459 ## 2 wgt 2965693. NA 2876683. 2676220. 2583301. 1 NA 0.97 0.902 0.871 options(repr.plot.width = 10, repr.plot.height = 2) list.out$graph 5.2.1.5 Example Cebu OLS df.use &lt;- df %&gt;% filter(S.country == &#39;Cebu&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) vars.z &lt;- NULL list.out &lt;- ff_lr_decompose( df=df.use, vars.y, vars.x, vars.c, vars.z, vars.other.keep, list.vars.tomean, list.vars.tomean.name.suffix, graph=TRUE, graph.nrow=1) options(repr.matrix.max.rows=10, repr.matrix.max.cols=50) list.out$dfmain ## # A tibble: 7,262 x 19 ## S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean value_A ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cebu 1 1 12 11.3 1 2044. 44.2 hgt 70.8 17.0 0.526 2989. 49.2 17.9 75.0 72.1 ## 2 Cebu 1 2 12 5.9 0 2840. 49.7 hgt 72.2 17.0 0.526 2989. 49.2 17.9 75.0 72.6 ## 3 Cebu 1 2 18 0.5 0 2840. 49.7 hgt 76.5 17.0 0.526 2989. 49.2 17.9 75.0 76.9 ## 4 Cebu 1 2 24 14.1 0 2840. 49.7 hgt 79.2 17.0 0.526 2989. 49.2 17.9 75.0 79.6 ## 5 Cebu 1 3 12 21.4 0 3446. 51.7 hgt 68 17.0 0.526 2989. 49.2 17.9 75.0 67.7 ## 6 Cebu 1 3 18 23.6 0 3446. 51.7 hgt 71.6 17.0 0.526 2989. 49.2 17.9 75.0 71.3 ## 7 Cebu 1 3 24 20.6 0 3446. 51.7 hgt 76.7 17.0 0.526 2989. 49.2 17.9 75.0 76.4 ## 8 Cebu 1 4 12 0.7 0 3091. 50.2 hgt 69.1 17.0 0.526 2989. 49.2 17.9 75.0 69.4 ## 9 Cebu 1 4 18 7.2 0 3091. 50.2 hgt 74.3 17.0 0.526 2989. 49.2 17.9 75.0 74.6 ## 10 Cebu 1 4 24 10.3 0 3091. 50.2 hgt 78.1 17.0 0.526 2989. 49.2 17.9 75.0 78.4 ## # ... with 7,252 more rows, and 2 more variables: value_B &lt;dbl&gt;, value_C &lt;dbl&gt; options(repr.plot.width = 10, repr.plot.height = 4) list.out$dfsumm ## # A tibble: 2 x 11 ## variable value_var value_mean_var value_A_var value_B_var value_C_var value_var_frac value_mean_var_fr~ value_A_var_frac value_B_var_frac value_C_var_frac ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 24.4 NA 22.6 21.3 10.0 1 NA 0.926 0.874 0.41 ## 2 wgt 3337461. NA 3218987. 3039514. 2558514. 1 NA 0.965 0.911 0.767 5.2.1.6 Example Cebu IV df.use &lt;- df %&gt;% filter(S.country == &#39;Cebu&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) vars.z &lt;- c(&#39;wealthIdx&#39;) list.out &lt;- ff_lr_decompose( df=df.use, vars.y, vars.x, vars.c, vars.z, vars.other.keep, list.vars.tomean, list.vars.tomean.name.suffix, graph=TRUE, graph.nrow=1) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped list.out$dfsumm ## # A tibble: 2 x 11 ## variable value_var value_mean_var value_A_var value_B_var value_C_var value_var_frac value_mean_var_fr~ value_A_var_frac value_B_var_frac value_C_var_frac ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 24.4 NA 22.6 22.2 14.4 1 NA 0.928 0.911 0.59 ## 2 wgt 3337461. NA 3237415. 3385815. 3158659. 1 NA 0.97 1.01 0.946 options(repr.plot.width = 10, repr.plot.height = 2) list.out$graph 5.2.1.7 Examples Line by Line The examples are just to test the code with different types of variables. df.use &lt;- df %&gt;% filter(S.country == &#39;Guatemala&#39;) %&gt;% filter(svymthRound %in% c(12, 18, 24)) dim(df.use) ## [1] 2022 16 Setting Up Parameters. # Define Left Hand Side Variables var.y1 &lt;- c(&#39;hgt&#39;) var.y2 &lt;- c(&#39;wgt&#39;) vars.y &lt;- c(var.y1, var.y2) # Define Right Hand Side Variables vars.x &lt;- c(&#39;prot&#39;) vars.c &lt;- c(&#39;male&#39;, &#39;wgt0&#39;, &#39;hgt0&#39;, &#39;svymthRound&#39;) # vars.z &lt;- c(&#39;p.A.prot&#39;) vars.z &lt;- c(&#39;vil.id&#39;) # vars.z &lt;- NULL vars.xc &lt;- c(vars.x, vars.c) # Other variables to keep vars.other.keep &lt;- c(&#39;S.country&#39;, &#39;vil.id&#39;, &#39;indi.id&#39;, &#39;svymthRound&#39;) # Decompose sequence vars.tomean.first &lt;- c(&#39;male&#39;, &#39;hgt0&#39;) var.tomean.first.name.suffix &lt;- &#39;_mh02m&#39; vars.tomean.second &lt;- c(vars.tomean.first, &#39;hgt0&#39;, &#39;wgt0&#39;) var.tomean.second.name.suffix &lt;- &#39;_mh0me2m&#39; vars.tomean.third &lt;- c(vars.tomean.second, &#39;prot&#39;) var.tomean.third.name.suffix &lt;- &#39;_mh0mep2m&#39; vars.tomean.fourth &lt;- c(vars.tomean.third, &#39;svymthRound&#39;) var.tomean.fourth.name.suffix &lt;- &#39;_mh0mepm2m&#39; list.vars.tomean = list( # vars.tomean.first, vars.tomean.second, vars.tomean.third, vars.tomean.fourth ) list.vars.tomean.name.suffix &lt;- list( # var.tomean.first.name.suffix, var.tomean.second.name.suffix, var.tomean.third.name.suffix, var.tomean.fourth.name.suffix ) 5.2.1.7.1 Obtain Regression Coefficients from somewhere # Regressions # regf.iv from C:\\Users\\fan\\R4Econ\\linreg\\ivreg\\ivregdfrow.R df.reg.out &lt;- as_tibble( bind_rows(lapply(vars.y, regf.iv, vars.x=vars.x, vars.c=vars.c, vars.z=vars.z, df=df))) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: attributes are not identical across measure variables; ## they will be dropped # Regressions # reg1 &lt;- regf.iv(var.y = var.y1, vars.x, vars.c, vars.z, df.use) # reg2 &lt;- regf.iv(var.y = var.y2, vars.x, vars.c, vars.z, df.use) # df.reg.out &lt;- as_tibble(bind_rows(reg1, reg2)) options(repr.matrix.max.rows=50, repr.matrix.max.cols=50) df.reg.out ## # A tibble: 2 x 37 ## X.Intercept._Es~ X.Intercept._Pr~ X.Intercept._St~ X.Intercept._zv~ hgt0_Estimate hgt0_Pr...z.. hgt0_Std.Error hgt0_zvalue male_Estimate male_Pr...z.. ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22.2547168993562 8.9088080511633~ 1.21637209166939 18.2959778934199 0.6834853337~ 4.5575874740~ 0.02896994903~ 23.5929077~ 1.2447352843~ 6.9064119002~ ## 2 -1101.090058068~ 0.0051062029326~ 393.210441213089 -2.800256408938~ 75.486789661~ 3.0043362381~ 9.23594523428~ 8.17315258~ 489.85290235~ 3.8106576466~ ## # ... with 27 more variables: male_Std.Error &lt;chr&gt;, male_zvalue &lt;chr&gt;, prot_Estimate &lt;chr&gt;, prot_Pr...z.. &lt;chr&gt;, prot_Std.Error &lt;chr&gt;, prot_zvalue &lt;chr&gt;, ## # Sargan_df1 &lt;chr&gt;, svymthRound_Estimate &lt;chr&gt;, svymthRound_Pr...z.. &lt;chr&gt;, svymthRound_Std.Error &lt;chr&gt;, svymthRound_zvalue &lt;chr&gt;, vars_var.y &lt;chr&gt;, ## # vars_vars.c &lt;chr&gt;, vars_vars.x &lt;chr&gt;, vars_vars.z &lt;chr&gt;, Weakinstruments_df1 &lt;chr&gt;, Weakinstruments_df2 &lt;chr&gt;, Weakinstruments_p.value &lt;chr&gt;, ## # Weakinstruments_statistic &lt;chr&gt;, wgt0_Estimate &lt;chr&gt;, wgt0_Pr...z.. &lt;chr&gt;, wgt0_Std.Error &lt;chr&gt;, wgt0_zvalue &lt;chr&gt;, Wu.Hausman_df1 &lt;chr&gt;, ## # Wu.Hausman_df2 &lt;chr&gt;, Wu.Hausman_p.value &lt;chr&gt;, Wu.Hausman_statistic &lt;chr&gt; # Select Variables str.esti.suffix &lt;- &#39;_Estimate&#39; arr.esti.name &lt;- paste0(vars.xc, str.esti.suffix) str.outcome.name &lt;- &#39;vars_var.y&#39; arr.columns2select &lt;- c(arr.esti.name, str.outcome.name) arr.columns2select ## [1] &quot;prot_Estimate&quot; &quot;male_Estimate&quot; &quot;wgt0_Estimate&quot; &quot;hgt0_Estimate&quot; &quot;svymthRound_Estimate&quot; &quot;vars_var.y&quot; # Generate dataframe for coefficients df.coef &lt;- df.reg.out[,c(arr.columns2select)] %&gt;% mutate_at(vars(arr.esti.name), as.numeric) %&gt;% column_to_rownames(str.outcome.name) df.coef ## prot_Estimate male_Estimate wgt0_Estimate hgt0_Estimate svymthRound_Estimate ## hgt -0.2714772 1.244735 0.0004430418 0.6834853 1.133919 ## wgt -59.0727542 489.852902 0.7696158110 75.4867897 250.778883 str(df.coef) ## &#39;data.frame&#39;: 2 obs. of 5 variables: ## $ prot_Estimate : num -0.271 -59.073 ## $ male_Estimate : num 1.24 489.85 ## $ wgt0_Estimate : num 0.000443 0.769616 ## $ hgt0_Estimate : num 0.683 75.487 ## $ svymthRound_Estimate: num 1.13 250.78 5.2.1.7.2 Decomposition Step 1 # Decomposition Step 1: gather df.decompose_step1 &lt;- df.use %&gt;% filter(svymthRound %in% c(12, 18, 24)) %&gt;% select(one_of(c(vars.other.keep, vars.xc, vars.y))) %&gt;% drop_na() %&gt;% gather(variable, value, -one_of(c(vars.other.keep, vars.xc))) options(repr.matrix.max.rows=20, repr.matrix.max.cols=20) dim(df.decompose_step1) ## [1] 1382 10 df.decompose_step1 ## # A tibble: 1,382 x 10 ## S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Guatemala 3 1352 18 13.3 1 2545. 47.4 hgt 70.2 ## 2 Guatemala 3 1352 24 46.3 1 2545. 47.4 hgt 75.8 ## 3 Guatemala 3 1354 12 1 1 3634. 51.2 hgt 66.3 ## 4 Guatemala 3 1354 18 9.8 1 3634. 51.2 hgt 69.2 ## 5 Guatemala 3 1354 24 15.4 1 3634. 51.2 hgt 75.3 ## 6 Guatemala 3 1356 12 8.6 1 3912. 51.9 hgt 68.1 ## 7 Guatemala 3 1356 18 17.8 1 3912. 51.9 hgt 74.1 ## 8 Guatemala 3 1356 24 30.5 1 3912. 51.9 hgt 77.1 ## 9 Guatemala 3 1357 12 1 1 3791. 52.6 hgt 71.5 ## 10 Guatemala 3 1357 18 12.7 1 3791. 52.6 hgt 77.8 ## # ... with 1,372 more rows 5.2.1.7.3 Decomposition Step 2 # Decomposition Step 2: mutate_at(vars, funs(mean = mean(.))) # the xc averaging could have taken place earlier, no difference in mean across variables df.decompose_step2 &lt;- df.decompose_step1 %&gt;% group_by(variable) %&gt;% mutate_at(vars(c(vars.xc, &#39;value&#39;)), funs(mean = mean(.))) %&gt;% ungroup() options(repr.matrix.max.rows=20, repr.matrix.max.cols=20) dim(df.decompose_step2) ## [1] 1382 16 df.decompose_step2 ## # A tibble: 1,382 x 16 ## S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Guatemala 3 1352 18 13.3 1 2545. 47.4 hgt 70.2 20.6 0.550 3312. 49.8 18.4 73.4 ## 2 Guatemala 3 1352 24 46.3 1 2545. 47.4 hgt 75.8 20.6 0.550 3312. 49.8 18.4 73.4 ## 3 Guatemala 3 1354 12 1 1 3634. 51.2 hgt 66.3 20.6 0.550 3312. 49.8 18.4 73.4 ## 4 Guatemala 3 1354 18 9.8 1 3634. 51.2 hgt 69.2 20.6 0.550 3312. 49.8 18.4 73.4 ## 5 Guatemala 3 1354 24 15.4 1 3634. 51.2 hgt 75.3 20.6 0.550 3312. 49.8 18.4 73.4 ## 6 Guatemala 3 1356 12 8.6 1 3912. 51.9 hgt 68.1 20.6 0.550 3312. 49.8 18.4 73.4 ## 7 Guatemala 3 1356 18 17.8 1 3912. 51.9 hgt 74.1 20.6 0.550 3312. 49.8 18.4 73.4 ## 8 Guatemala 3 1356 24 30.5 1 3912. 51.9 hgt 77.1 20.6 0.550 3312. 49.8 18.4 73.4 ## 9 Guatemala 3 1357 12 1 1 3791. 52.6 hgt 71.5 20.6 0.550 3312. 49.8 18.4 73.4 ## 10 Guatemala 3 1357 18 12.7 1 3791. 52.6 hgt 77.8 20.6 0.550 3312. 49.8 18.4 73.4 ## # ... with 1,372 more rows 5.2.1.7.4 Decomposition Step 3 Non-Loop ff_lr_decompose_valadj &lt;- function(df, df.coef, vars.tomean, str.esti.suffix) { new_value &lt;- (df$value + rowSums((df[paste0(vars.tomean, &#39;_mean&#39;)] - df[vars.tomean]) *df.coef[df$variable, paste0(vars.tomean, str.esti.suffix)])) return(new_value) } # # Decomposition Step 3: mutate_at(vars, funs(mean = mean(.))) # var.decomp.one &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[1]])) # var.decomp.two &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[2]])) # var.decomp.thr &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[3]])) # df.decompose_step3 &lt;- df.decompose_step2 %&gt;% # mutate((!!var.decomp.one) := f_decompose_here(., df.coef, list.vars.tomean[[1]], str.esti.suffix), # (!!var.decomp.two) := f_decompose_here(., df.coef, list.vars.tomean[[2]], str.esti.suffix), # (!!var.decomp.thr) := f_decompose_here(., df.coef, list.vars.tomean[[3]], str.esti.suffix)) # options(repr.matrix.max.rows=10, repr.matrix.max.cols=20) # dim(df.decompose_step3) # df.decompose_step3 5.2.1.7.5 Decomposition Step 3 With Loop df.decompose_step3 &lt;- df.decompose_step2 for (i in 1:length(list.vars.tomean)) { var.decomp.cur &lt;- (paste0(&#39;value&#39;, list.vars.tomean.name.suffix[[i]])) vars.tomean &lt;- list.vars.tomean[[i]] var.decomp.cur df.decompose_step3 &lt;- df.decompose_step3 %&gt;% mutate((!!var.decomp.cur) := ff_lr_decompose_valadj(., df.coef, vars.tomean, str.esti.suffix)) } options(repr.matrix.max.rows=10, repr.matrix.max.cols=20) dim(df.decompose_step3) ## [1] 1382 19 df.decompose_step3 ## # A tibble: 1,382 x 19 ## S.country vil.id indi.id svymthRound prot male wgt0 hgt0 variable value prot_mean male_mean wgt0_mean hgt0_mean svymthRound_mean value_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Guatemala 3 1352 18 13.3 1 2545. 47.4 hgt 70.2 20.6 0.550 3312. 49.8 18.4 73.4 ## 2 Guatemala 3 1352 24 46.3 1 2545. 47.4 hgt 75.8 20.6 0.550 3312. 49.8 18.4 73.4 ## 3 Guatemala 3 1354 12 1 1 3634. 51.2 hgt 66.3 20.6 0.550 3312. 49.8 18.4 73.4 ## 4 Guatemala 3 1354 18 9.8 1 3634. 51.2 hgt 69.2 20.6 0.550 3312. 49.8 18.4 73.4 ## 5 Guatemala 3 1354 24 15.4 1 3634. 51.2 hgt 75.3 20.6 0.550 3312. 49.8 18.4 73.4 ## 6 Guatemala 3 1356 12 8.6 1 3912. 51.9 hgt 68.1 20.6 0.550 3312. 49.8 18.4 73.4 ## 7 Guatemala 3 1356 18 17.8 1 3912. 51.9 hgt 74.1 20.6 0.550 3312. 49.8 18.4 73.4 ## 8 Guatemala 3 1356 24 30.5 1 3912. 51.9 hgt 77.1 20.6 0.550 3312. 49.8 18.4 73.4 ## 9 Guatemala 3 1357 12 1 1 3791. 52.6 hgt 71.5 20.6 0.550 3312. 49.8 18.4 73.4 ## 10 Guatemala 3 1357 18 12.7 1 3791. 52.6 hgt 77.8 20.6 0.550 3312. 49.8 18.4 73.4 ## # ... with 1,372 more rows, and 3 more variables: value_mh0me2m &lt;dbl&gt;, value_mh0mep2m &lt;dbl&gt;, value_mh0mepm2m &lt;dbl&gt; 5.2.1.7.6 Decomposition Step 4 Variance df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;)) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% select(matches(&#39;value&#39;)) %&gt;% select(ends_with(&quot;_var&quot;)) %&gt;% mutate_if(is.numeric, funs( frac = (./value_var))) %&gt;% mutate_if(is.numeric, round, 3) ## # A tibble: 2 x 10 ## value_var value_mean_var value_mh0me2m_v~ value_mh0mep2m_~ value_mh0mepm2m~ value_var_frac value_mean_var_~ value_mh0me2m_v~ value_mh0mep2m_~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.9 NA 25.4 49.0 23.1 1 NA 1.16 2.24 ## 2 2965693. NA 2949188. 4192770. 3147507. 1 NA 0.994 1.41 ## # ... with 1 more variable: value_mh0mepm2m_var_frac &lt;dbl&gt; 5.2.1.7.7 Graphical Results Graphically, difficult to pick up exact differences in variance, a 50 percent reduction in variance visually does not look like 50 percent. Intuitively, we are kind of seeing standard deviation, not variance on the graph if we think abou the x-scale. df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;), -value_mean) ## # A tibble: 1,382 x 5 ## variable value value_mh0me2m value_mh0mep2m value_mh0mepm2m ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 70.2 73.2 71.2 71.7 ## 2 hgt 75.8 78.8 85.8 79.4 ## 3 hgt 66.3 63.6 58.3 65.6 ## 4 hgt 69.2 66.5 63.6 64.1 ## 5 hgt 75.3 72.6 71.2 64.9 ## 6 hgt 68.1 64.3 61.1 68.4 ## 7 hgt 74.1 70.3 69.6 70.0 ## 8 hgt 77.1 73.3 76.0 69.7 ## 9 hgt 71.5 66.8 61.5 68.8 ## 10 hgt 77.8 73.1 71.0 71.5 ## # ... with 1,372 more rows options(repr.plot.width = 10, repr.plot.height = 4) df.decompose_step3 %&gt;% select(variable, contains(&#39;value&#39;), -value_mean) %&gt;% rename(outcome = variable) %&gt;% gather(variable, value, -outcome) %&gt;% ggplot(aes(x=value, color = variable, fill = variable)) + geom_line(stat = &quot;density&quot;) + facet_wrap(~ outcome, scales=&#39;free&#39;, nrow=2) 5.2.1.8 Additional Decomposition Testings head(df.decompose_step2[vars.tomean.first],3) ## # A tibble: 3 x 2 ## male hgt0 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 47.4 ## 2 1 47.4 ## 3 1 51.2 head(df.decompose_step2[paste0(vars.tomean.first, &#39;_mean&#39;)], 3) ## # A tibble: 3 x 2 ## male_mean hgt0_mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.550 49.8 ## 2 0.550 49.8 ## 3 0.550 49.8 head(df.coef[df.decompose_step2$variable, paste0(vars.tomean.first, str.esti.suffix)], 3) ## male_Estimate hgt0_Estimate ## hgt 1.244735 0.6834853 ## hgt.1 1.244735 0.6834853 ## hgt.2 1.244735 0.6834853 df.decompose.tomean.first &lt;- df.decompose_step2 %&gt;% mutate(pred_new = df.decompose_step2$value + rowSums((df.decompose_step2[paste0(vars.tomean.first, &#39;_mean&#39;)] - df.decompose_step2[vars.tomean.first]) *df.coef[df.decompose_step2$variable, paste0(vars.tomean.first, str.esti.suffix)])) %&gt;% select(variable, value, pred_new) head(df.decompose.tomean.first, 10) ## # A tibble: 10 x 3 ## variable value pred_new ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 70.2 71.2 ## 2 hgt 75.8 76.8 ## 3 hgt 66.3 64.7 ## 4 hgt 69.2 67.6 ## 5 hgt 75.3 73.7 ## 6 hgt 68.1 66.1 ## 7 hgt 74.1 72.1 ## 8 hgt 77.1 75.1 ## 9 hgt 71.5 69.0 ## 10 hgt 77.8 75.3 df.decompose.tomean.first %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, sd = sd)) ## # A tibble: 2 x 5 ## variable value_mean pred_new_mean value_sd pred_new_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 73.4 73.4 4.68 4.53 ## 2 wgt 8808. 8808. 1722. 1695. Note the r-square from regression above matches up with the 1 - ratio below. This is the proper decomposition method that is equivalent to r2. df.decompose_step2 %&gt;% mutate(pred_new = df.decompose_step2$value + rowSums((df.decompose_step2[paste0(vars.tomean.second, &#39;_mean&#39;)] - df.decompose_step2[vars.tomean.second]) *df.coef[df.decompose_step2$variable, paste0(vars.tomean.second, str.esti.suffix)])) %&gt;% select(variable, value, pred_new) %&gt;% group_by(variable) %&gt;% summarize_all(funs(mean = mean, var = var)) %&gt;% mutate(ratio = (pred_new_var/value_var)) ## # A tibble: 2 x 6 ## variable value_mean pred_new_mean value_var pred_new_var ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hgt 73.4 73.4 21.9 25.4 1.16 ## 2 wgt 8808. 8808. 2965693. 2949188. 0.994 "],
["nonlinear-regression.html", "Chapter 6 Nonlinear Regression 6.1 Logit Regression", " Chapter 6 Nonlinear Regression 6.1 Logit Regression 6.1.1 Binary Logit Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. Data Preparation df_mtcars &lt;- mtcars # X-variables to use on RHS ls_st_xs &lt;- c(&#39;mpg&#39;, &#39;qsec&#39;) ls_st_xs &lt;- c(&#39;mpg&#39;) ls_st_xs &lt;- c(&#39;qsec&#39;) ls_st_xs &lt;- c(&#39;wt&#39;) ls_st_xs &lt;- c(&#39;mpg&#39;, &#39;wt&#39;, &#39;vs&#39;) svr_binary &lt;- &#39;hpLowHigh&#39; svr_binary_lb0 &lt;- &#39;LowHP&#39; svr_binary_lb1 &lt;- &#39;HighHP&#39; svr_outcome &lt;- &#39;am&#39; sdt_name &lt;- &#39;mtcars&#39; # Discretize hp df_mtcars &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := cut(hp, breaks=c(-Inf, 210, Inf), labels=c(svr_binary_lb0, svr_binary_lb1))) 6.1.1.1 Logit Regresion and Prediction logit regression with glm, and predict using estimation data. Prediction and estimation with one variable. LOGIT REGRESSION R DATA ANALYSIS EXAMPLES Generalized Linear Models # Regress rs_logit &lt;- glm(as.formula(paste(svr_outcome, &quot;~&quot;, paste(ls_st_xs, collapse=&quot;+&quot;))) ,data = df_mtcars, family = &quot;binomial&quot;) summary(rs_logit) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~&quot;, paste(ls_st_xs, ## collapse = &quot;+&quot;))), family = &quot;binomial&quot;, data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.73603 -0.25477 -0.04891 0.13402 1.90321 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 22.69008 13.95112 1.626 0.1039 ## mpg -0.01786 0.33957 -0.053 0.9581 ## wt -6.73804 3.01400 -2.236 0.0254 * ## vs -4.44046 2.84247 -1.562 0.1182 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 13.092 on 28 degrees of freedom ## AIC: 21.092 ## ## Number of Fisher Scoring iterations: 7 # Predcit Using Regression Data df_mtcars$p_mpg &lt;- predict(rs_logit, newdata = df_mtcars, type = &quot;response&quot;) 6.1.1.1.1 Prediction with Observed Binary Input Logit regression with a continuous variable and a binary variable. Predict outcome with observed continuous variable as well as observed binary input variable. # Regress rs_logit_bi &lt;- glm(as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary,&quot;) + &quot;, paste(ls_st_xs, collapse=&quot;+&quot;))) , data = df_mtcars, family = &quot;binomial&quot;) summary(rs_logit_bi) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary, ## &quot;) + &quot;, paste(ls_st_xs, collapse = &quot;+&quot;))), family = &quot;binomial&quot;, ## data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.45771 -0.09563 -0.00875 0.00555 1.87612 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.8285 18.0390 0.212 0.8319 ## factor(hpLowHigh)HighHP 6.9907 5.5176 1.267 0.2052 ## mpg 0.8985 0.8906 1.009 0.3131 ## wt -6.7291 3.3166 -2.029 0.0425 * ## vs -5.9206 4.1908 -1.413 0.1577 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.2297 on 31 degrees of freedom ## Residual deviance: 8.9777 on 27 degrees of freedom ## AIC: 18.978 ## ## Number of Fisher Scoring iterations: 9 # Predcit Using Regresion Data df_mtcars$p_mpg_hp &lt;- predict(rs_logit_bi, newdata = df_mtcars, type = &quot;response&quot;) # Predicted Probabilities am on mgp with or without hp binary scatter &lt;- ggplot(df_mtcars, aes(x=p_mpg_hp, y=p_mpg)) + geom_point(size=1) + # geom_smooth(method=lm) + # Trend line geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Predicted Probabilities &#39;, svr_outcome, &#39; on &#39;, ls_st_xs, &#39; with or without hp binary&#39;), x = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; indicator, 1 is high&#39;), y = paste0(&#39;prediction with only &#39;, ls_st_xs), caption = &#39;mtcars; prediction based on observed data&#39;) + theme_bw() print(scatter) 6.1.1.1.2 Prediction with Binary set to 0 and 1 Now generate two predictions. One set where binary input is equal to 0, and another where the binary inputs are equal to 1. Ignore whether in data binary input is equal to 0 or 1. Use the same regression results as what was just derived. Note that given the example here, the probability changes a lot when we # Previous regression results summary(rs_logit_bi) ## ## Call: ## glm(formula = as.formula(paste(svr_outcome, &quot;~ factor(&quot;, svr_binary, ## &quot;) + &quot;, paste(ls_st_xs, collapse = &quot;+&quot;))), family = &quot;binomial&quot;, ## data = df_mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.45771 -0.09563 -0.00875 0.00555 1.87612 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.8285 18.0390 0.212 0.8319 ## factor(hpLowHigh)HighHP 6.9907 5.5176 1.267 0.2052 ## mpg 0.8985 0.8906 1.009 0.3131 ## wt -6.7291 3.3166 -2.029 0.0425 * ## vs -5.9206 4.1908 -1.413 0.1577 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.2297 on 31 degrees of freedom ## Residual deviance: 8.9777 on 27 degrees of freedom ## AIC: 18.978 ## ## Number of Fisher Scoring iterations: 9 # Two different dataframes, mutate the binary regressor df_mtcars_bi0 &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := svr_binary_lb0) df_mtcars_bi1 &lt;- df_mtcars %&gt;% mutate(!!sym(svr_binary) := svr_binary_lb1) # Predcit Using Regresion Data df_mtcars$p_mpg_hp_bi0 &lt;- predict(rs_logit_bi, newdata = df_mtcars_bi0, type = &quot;response&quot;) df_mtcars$p_mpg_hp_bi1 &lt;- predict(rs_logit_bi, newdata = df_mtcars_bi1, type = &quot;response&quot;) # Predicted Probabilities and Binary Input scatter &lt;- ggplot(df_mtcars, aes(x=p_mpg_hp_bi0)) + geom_point(aes(y=p_mpg_hp), size=4, shape=4, color=&quot;red&quot;) + geom_point(aes(y=p_mpg_hp_bi1), size=2, shape=8) + # geom_smooth(method=lm) + # Trend line geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Predicted Probabilities and Binary Input&#39;, &#39;\\ncross(shape=4)/red is predict actual binary data&#39;, &#39;\\nstar(shape=8)/black is predict set binary = 1 for all&#39;), x = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; = 0 for all&#39;), y = paste0(&#39;prediction with &#39;, ls_st_xs, &#39; and binary &#39;, svr_binary, &#39; = 1&#39;), caption = paste0(sdt_name)) + theme_bw() print(scatter) 6.1.1.1.3 Prediction with Binary set to 0 and 1 Difference What is the difference in probability between binary = 0 vs binary = 1. How does that relate to the probability of outcome of interest when binary = 0 for all. In the binary logit case, the relationship will be hump–shaped by construction between \\(A_i\\) and \\(\\alpha_i\\). In the exponential wage cases, the relationship is convex upwards. # Generate Gap Variable df_mtcars &lt;- df_mtcars %&gt;% mutate(alpha_i = p_mpg_hp_bi1 - p_mpg_hp_bi0) %&gt;% mutate(A_i = p_mpg_hp_bi0) # Binary Marginal Effects and Prediction without Binary scatter &lt;- ggplot(df_mtcars, aes(x=A_i)) + geom_point(aes(y=alpha_i), size=4, shape=4, color=&quot;red&quot;) + geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;Binary Marginal Effects and Prediction without Binary&#39;), x = &#39;P(binary=0) for all&#39;, y = &#39;P(binary=1) - P(binary=0) gap&#39;, caption = paste0(sdt_name)) + theme_bw() print(scatter) 6.1.1.1.4 X variables and A and alpha Given the x-variables included in the logit regression, how do they relate to A_i and alpha_i # Generate Gap Variable df_mtcars &lt;- df_mtcars %&gt;% mutate(alpha_i = p_mpg_hp_bi1 - p_mpg_hp_bi0) %&gt;% mutate(A_i = p_mpg_hp_bi0) # Binary Marginal Effects and Prediction without Binary ggplot.A.alpha.x &lt;- function(svr_x, df, svr_alpha = &#39;alpha_i&#39;, svr_A = &quot;A_i&quot;){ scatter &lt;- ggplot(df, aes(x=!!sym(svr_x))) + geom_point(aes(y=alpha_i), size=4, shape=4, color=&quot;red&quot;) + geom_point(aes(y=A_i), size=2, shape=8, color=&quot;blue&quot;) + geom_abline(intercept = 0, slope = 1) + # 45 degree line labs(title = paste0(&#39;A (blue) and alpha (red) vs x variables=&#39;, svr_x), x = svr_x, y = &#39;Probabilities&#39;, caption = paste0(sdt_name)) + theme_bw() return(scatter) } # Plot over multiple lapply(ls_st_xs, ggplot.A.alpha.x, df = df_mtcars) ## [[1]] ## ## [[2]] ## ## [[3]] "],
["optimization.html", "Chapter 7 Optimization 7.1 Bisection", " Chapter 7 Optimization 7.1 Bisection 7.1.1 Bisection Go back to fan’s REconTools Package, R4Econ Repository (bookdown site), or Intro Stats with R Repository. See the ff_opti_bisect_pmap_multi function from Fan’s REconTools Package, which provides a resuable function based on the algorithm worked out here. The bisection specific code does not need to do much. list variables in file for grouping, each group is an individual for whom we want to calculate optimal choice for using bisection. string variable name of input where functions are evaluated, these are already contained in the dataframe, existing variable names, row specific, rowwise computation over these, each rowwise calculation using different rows. scalar and array values that are applied to every rowwise calculation, all rowwise calculations using the same scalars and arrays. string output variable name This is how I implement the bisection algorithm, when we know the bounding minimum and maximum to be below and above zero already. Evaluate \\(f^0_a = f(a^0)\\) and \\(f^0_b = f(b^0)\\), min and max points. Evaluate at \\(f^0_p = f(p^0)\\), where \\(p_0 = \\frac{a^0+b^0}{2}\\). if \\(f^i_a \\cdot f^i_p &lt; 0\\), then \\(b_{i+1} = p_i\\), else, \\(a_{i+1} = p_i\\) and \\(f^{i+1}_a = p_i\\). iteratre until convergence. Generate New columns of a and b as we iteratre, do not need to store p, p is temporary. Evaluate the function below which we have already tested, but now, in the dataframe before generating all permutations, tb_states_choices, now the fl_N element will be changing with each iteration, it will be row specific. fl_N are first min and max, then each subsequent ps. 7.1.1.1 Initialize Matrix First, initialize the matrix with \\(a_0\\) and \\(b_0\\), the initial min and max points: # common prefix to make reshaping easier st_bisec_prefix &lt;- &#39;bisec_&#39; svr_a_lst &lt;- paste0(st_bisec_prefix, &#39;a_0&#39;) svr_b_lst &lt;- paste0(st_bisec_prefix, &#39;b_0&#39;) svr_fa_lst &lt;- paste0(st_bisec_prefix, &#39;fa_0&#39;) svr_fb_lst &lt;- paste0(st_bisec_prefix, &#39;fb_0&#39;) # Add initial a and b tb_states_choices_bisec &lt;- tb_states_choices %&gt;% mutate(!!sym(svr_a_lst) := fl_N_min, !!sym(svr_b_lst) := fl_N_agg) # Evaluate function f(a_0) and f(b_0) tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% rowwise() %&gt;% mutate(!!sym(svr_fa_lst) := ffi_nonlin_dplyrdo(fl_A, fl_alpha, !!sym(svr_a_lst), ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho), !!sym(svr_fb_lst) := ffi_nonlin_dplyrdo(fl_A, fl_alpha, !!sym(svr_b_lst), ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) # Summarize dim(tb_states_choices_bisec) ## [1] 4 7 summary(tb_states_choices_bisec) ## INDI_ID fl_A fl_alpha bisec_a_0 bisec_b_0 bisec_fa_0 bisec_fb_0 ## Min. :1.00 Min. :-2 Min. :0.1 Min. :0 Min. :100 Min. :100 Min. :-15057.61 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.:0 1st Qu.:100 1st Qu.:100 1st Qu.: -5011.84 ## Median :2.50 Median : 0 Median :0.5 Median :0 Median :100 Median :100 Median : -1033.19 ## Mean :2.50 Mean : 0 Mean :0.5 Mean :0 Mean :100 Mean :100 Mean : -4301.31 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.:0 3rd Qu.:100 3rd Qu.:100 3rd Qu.: -322.66 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :0 Max. :100 Max. :100 Max. : -81.25 7.1.1.2 Iterate and Solve for f(p), update f(a) and f(b) Implement the DPLYR based Concurrent bisection algorithm. # fl_tol = float tolerance criteria # it_tol = number of interations to allow at most fl_tol &lt;- 10^-2 it_tol &lt;- 100 # fl_p_dist2zr = distance to zero to initalize fl_p_dist2zr &lt;- 1000 it_cur &lt;- 0 while (it_cur &lt;= it_tol &amp;&amp; fl_p_dist2zr &gt;= fl_tol ) { it_cur &lt;- it_cur + 1 # New Variables svr_a_cur &lt;- paste0(st_bisec_prefix, &#39;a_&#39;, it_cur) svr_b_cur &lt;- paste0(st_bisec_prefix, &#39;b_&#39;, it_cur) svr_fa_cur &lt;- paste0(st_bisec_prefix, &#39;fa_&#39;, it_cur) svr_fb_cur &lt;- paste0(st_bisec_prefix, &#39;fb_&#39;, it_cur) # Evaluate function f(a_0) and f(b_0) # 1. generate p # 2. generate f_p # 3. generate f_p*f_a tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% rowwise() %&gt;% mutate(p = ((!!sym(svr_a_lst) + !!sym(svr_b_lst))/2)) %&gt;% mutate(f_p = ffi_nonlin_dplyrdo(fl_A, fl_alpha, p, ar_nN_A, ar_nN_alpha, fl_N_agg, fl_rho)) %&gt;% mutate(f_p_t_f_a = f_p*!!sym(svr_fa_lst)) # fl_p_dist2zr = sum(abs(p)) fl_p_dist2zr &lt;- mean(abs(tb_states_choices_bisec %&gt;% pull(f_p))) # Update a and b tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% mutate(!!sym(svr_a_cur) := case_when(f_p_t_f_a &lt; 0 ~ !!sym(svr_a_lst), TRUE ~ p)) %&gt;% mutate(!!sym(svr_b_cur) := case_when(f_p_t_f_a &lt; 0 ~ p, TRUE ~ !!sym(svr_b_lst))) # Update f(a) and f(b) tb_states_choices_bisec &lt;- tb_states_choices_bisec %&gt;% mutate(!!sym(svr_fa_cur) := case_when(f_p_t_f_a &lt; 0 ~ !!sym(svr_fa_lst), TRUE ~ f_p)) %&gt;% mutate(!!sym(svr_fb_cur) := case_when(f_p_t_f_a &lt; 0 ~ f_p, TRUE ~ !!sym(svr_fb_lst))) # Save from last svr_a_lst &lt;- svr_a_cur svr_b_lst &lt;- svr_b_cur svr_fa_lst &lt;- svr_fa_cur svr_fb_lst &lt;- svr_fb_cur # Summar current round print(paste0(&#39;it_cur:&#39;, it_cur, &#39;, fl_p_dist2zr:&#39;, fl_p_dist2zr)) summary(tb_states_choices_bisec %&gt;% select(one_of(svr_a_cur, svr_b_cur, svr_fa_cur, svr_fb_cur))) } ## [1] &quot;it_cur:1, fl_p_dist2zr:1884.20860322127&quot; ## [1] &quot;it_cur:2, fl_p_dist2zr:815.07213515036&quot; ## [1] &quot;it_cur:3, fl_p_dist2zr:346.193951089409&quot; ## [1] &quot;it_cur:4, fl_p_dist2zr:133.268318242343&quot; ## [1] &quot;it_cur:5, fl_p_dist2zr:52.0759336601643&quot; ## [1] &quot;it_cur:6, fl_p_dist2zr:8.2057326579422&quot; ## [1] &quot;it_cur:7, fl_p_dist2zr:12.7240911320081&quot; ## [1] &quot;it_cur:8, fl_p_dist2zr:4.10100732130902&quot; ## [1] &quot;it_cur:9, fl_p_dist2zr:1.19915237247596&quot; ## [1] &quot;it_cur:10, fl_p_dist2zr:1.46089191924225&quot; ## [1] &quot;it_cur:11, fl_p_dist2zr:0.261965457555881&quot; ## [1] &quot;it_cur:12, fl_p_dist2zr:0.462901483859291&quot; ## [1] &quot;it_cur:13, fl_p_dist2zr:0.166336071560483&quot; ## [1] &quot;it_cur:14, fl_p_dist2zr:0.011649263648799&quot; ## [1] &quot;it_cur:15, fl_p_dist2zr:0.0715183716517558&quot; ## [1] &quot;it_cur:16, fl_p_dist2zr:0.0299376539319738&quot; ## [1] &quot;it_cur:17, fl_p_dist2zr:0.0132655999120672&quot; ## [1] &quot;it_cur:18, fl_p_dist2zr:0.00317751042553027&quot; 7.1.1.3 Reshape Wide to long to Wide To view results easily, how iterations improved to help us find the roots, convert table from wide to long. Pivot twice. This allows us to easily graph out how bisection is working out iterationby iteration. Here, we will first show what the raw table looks like, the wide only table, and then show the long version, and finally the version that is medium wide. 7.1.1.3.1 Table One–Very Wide Show what the tb_states_choices_bisec looks like. Variables are formatted like: bisec_xx_yy, where yy is the iteration indicator, and xx is either a, b, fa, or fb. head(tb_states_choices_bisec, 10) ## Source: local data frame [4 x 82] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 82 ## INDI_ID fl_A fl_alpha bisec_a_0 bisec_b_0 bisec_fa_0 bisec_fb_0 p f_p f_p_t_f_a bisec_a_1 bisec_b_1 bisec_fa_1 bisec_fb_1 bisec_a_2 bisec_b_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -2 0.1 0 100 100 -15058. 1.32 1.02e-2 4.45e-4 0 50 100 -6660. 0 25 ## 2 2 -0.667 0.367 0 100 100 -1663. 7.29 -1.26e-3 -5.61e-6 0 50 100 -724. 0 25 ## 3 3 0.667 0.633 0 100 100 -403. 20.9 6.18e-4 1.54e-6 0 50 100 -146. 0 25 ## 4 4 2 0.9 0 100 100 -81.3 54.1 -6.09e-4 -4.46e-8 50 100 7.33 -81.3 50 75 ## # ... with 66 more variables: bisec_fa_2 &lt;dbl&gt;, bisec_fb_2 &lt;dbl&gt;, bisec_a_3 &lt;dbl&gt;, bisec_b_3 &lt;dbl&gt;, bisec_fa_3 &lt;dbl&gt;, bisec_fb_3 &lt;dbl&gt;, bisec_a_4 &lt;dbl&gt;, ## # bisec_b_4 &lt;dbl&gt;, bisec_fa_4 &lt;dbl&gt;, bisec_fb_4 &lt;dbl&gt;, bisec_a_5 &lt;dbl&gt;, bisec_b_5 &lt;dbl&gt;, bisec_fa_5 &lt;dbl&gt;, bisec_fb_5 &lt;dbl&gt;, bisec_a_6 &lt;dbl&gt;, ## # bisec_b_6 &lt;dbl&gt;, bisec_fa_6 &lt;dbl&gt;, bisec_fb_6 &lt;dbl&gt;, bisec_a_7 &lt;dbl&gt;, bisec_b_7 &lt;dbl&gt;, bisec_fa_7 &lt;dbl&gt;, bisec_fb_7 &lt;dbl&gt;, bisec_a_8 &lt;dbl&gt;, ## # bisec_b_8 &lt;dbl&gt;, bisec_fa_8 &lt;dbl&gt;, bisec_fb_8 &lt;dbl&gt;, bisec_a_9 &lt;dbl&gt;, bisec_b_9 &lt;dbl&gt;, bisec_fa_9 &lt;dbl&gt;, bisec_fb_9 &lt;dbl&gt;, bisec_a_10 &lt;dbl&gt;, ## # bisec_b_10 &lt;dbl&gt;, bisec_fa_10 &lt;dbl&gt;, bisec_fb_10 &lt;dbl&gt;, bisec_a_11 &lt;dbl&gt;, bisec_b_11 &lt;dbl&gt;, bisec_fa_11 &lt;dbl&gt;, bisec_fb_11 &lt;dbl&gt;, bisec_a_12 &lt;dbl&gt;, ## # bisec_b_12 &lt;dbl&gt;, bisec_fa_12 &lt;dbl&gt;, bisec_fb_12 &lt;dbl&gt;, bisec_a_13 &lt;dbl&gt;, bisec_b_13 &lt;dbl&gt;, bisec_fa_13 &lt;dbl&gt;, bisec_fb_13 &lt;dbl&gt;, bisec_a_14 &lt;dbl&gt;, ## # bisec_b_14 &lt;dbl&gt;, bisec_fa_14 &lt;dbl&gt;, bisec_fb_14 &lt;dbl&gt;, bisec_a_15 &lt;dbl&gt;, bisec_b_15 &lt;dbl&gt;, bisec_fa_15 &lt;dbl&gt;, bisec_fb_15 &lt;dbl&gt;, bisec_a_16 &lt;dbl&gt;, ## # bisec_b_16 &lt;dbl&gt;, bisec_fa_16 &lt;dbl&gt;, bisec_fb_16 &lt;dbl&gt;, bisec_a_17 &lt;dbl&gt;, bisec_b_17 &lt;dbl&gt;, bisec_fa_17 &lt;dbl&gt;, bisec_fb_17 &lt;dbl&gt;, bisec_a_18 &lt;dbl&gt;, ## # bisec_b_18 &lt;dbl&gt;, bisec_fa_18 &lt;dbl&gt;, bisec_fb_18 &lt;dbl&gt; str(tb_states_choices_bisec) ## Classes &#39;rowwise_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 82 variables: ## $ INDI_ID : int 1 2 3 4 ## $ fl_A : num -2 -0.667 0.667 2 ## $ fl_alpha : num 0.1 0.367 0.633 0.9 ## $ bisec_a_0 : num 0 0 0 0 ## $ bisec_b_0 : num 100 100 100 100 ## $ bisec_fa_0 : num 100 100 100 100 ## $ bisec_fb_0 : num -15057.6 -1663.3 -403.1 -81.3 ## $ p : num 1.32 7.29 20.89 54.1 ## $ f_p : num 0.010225 -0.001259 0.000618 -0.000609 ## $ f_p_t_f_a : num 4.45e-04 -5.61e-06 1.54e-06 -4.46e-08 ## $ bisec_a_1 : num 0 0 0 50 ## $ bisec_b_1 : num 50 50 50 100 ## $ bisec_fa_1 : num 100 100 100 7.33 ## $ bisec_fb_1 : num -6659.8 -723.7 -145.9 -81.3 ## $ bisec_a_2 : num 0 0 0 50 ## $ bisec_b_2 : num 25 25 25 75 ## $ bisec_fa_2 : num 100 100 100 7.33 ## $ bisec_fb_2 : num -2917.6 -285.2 -20.3 -37.2 ## $ bisec_a_3 : num 0 0 12.5 50 ## $ bisec_b_3 : num 12.5 12.5 25 62.5 ## $ bisec_fa_3 : num 100 100 41.08 7.33 ## $ bisec_fb_3 : num -1248.4 -80.3 -20.3 -15 ## $ bisec_a_4 : num 0 6.25 18.75 50 ## $ bisec_b_4 : num 6.25 12.5 25 56.25 ## $ bisec_fa_4 : num 100 15.52 10.54 7.33 ## $ bisec_fb_4 : num -503.16 -80.3 -20.32 -3.85 ## $ bisec_a_5 : num 0 6.25 18.75 53.12 ## $ bisec_b_5 : num 3.12 9.38 21.88 56.25 ## $ bisec_fa_5 : num 100 15.52 10.54 1.74 ## $ bisec_fb_5 : num -170.1 -31.61 -4.86 -3.85 ## $ bisec_a_6 : num 0 6.25 20.31 53.12 ## $ bisec_b_6 : num 1.56 7.81 21.88 54.69 ## $ bisec_fa_6 : num 100 15.52 2.85 1.74 ## $ bisec_fb_6 : num -21.09 -7.82 -4.86 -1.06 ## $ bisec_a_7 : num 0.781 7.031 20.312 53.906 ## $ bisec_b_7 : num 1.56 7.81 21.09 54.69 ## $ bisec_fa_7 : num 45.65 3.909 2.853 0.338 ## $ bisec_fb_7 : num -21.089 -7.822 -0.999 -1.059 ## $ bisec_a_8 : num 1.17 7.03 20.7 53.91 ## $ bisec_b_8 : num 1.56 7.42 21.09 54.3 ## $ bisec_fa_8 : num 13.174 3.909 0.928 0.338 ## $ bisec_fb_8 : num -21.089 -1.942 -0.999 -0.36 ## $ bisec_a_9 : num 1.17 7.23 20.7 53.91 ## $ bisec_b_9 : num 1.37 7.42 20.9 54.1 ## $ bisec_fa_9 : num 13.174 0.988 0.928 0.338 ## $ bisec_fb_9 : num -3.763 -1.9416 -0.0351 -0.0108 ## $ bisec_a_10 : num 1.27 7.23 20.8 54 ## $ bisec_b_10 : num 1.37 7.32 20.9 54.1 ## $ bisec_fa_10: num 4.757 0.988 0.446 0.164 ## $ bisec_fb_10: num -3.763 -0.476 -0.0351 -0.0108 ## $ bisec_a_11 : num 1.32 7.28 20.85 54.05 ## $ bisec_b_11 : num 1.37 7.32 20.9 54.1 ## $ bisec_fa_11: num 0.5096 0.2561 0.2057 0.0765 ## $ bisec_fb_11: num -3.763 -0.476 -0.0351 -0.0108 ## $ bisec_a_12 : num 1.32 7.28 20.87 54.08 ## $ bisec_b_12 : num 1.34 7.3 20.9 54.1 ## $ bisec_fa_12: num 0.5096 0.2561 0.0853 0.0328 ## $ bisec_fb_12: num -1.6236 -0.1099 -0.0351 -0.0108 ## $ bisec_a_13 : num 1.32 7.29 20.89 54.09 ## $ bisec_b_13 : num 1.33 7.3 20.9 54.1 ## $ bisec_fa_13: num 0.5096 0.0731 0.0251 0.011 ## $ bisec_fb_13: num -0.5562 -0.1099 -0.0351 -0.0108 ## $ bisec_a_14 : num 1.32 7.29 20.89 54.1 ## $ bisec_b_14 : num 1.32 7.29 20.89 54.1 ## $ bisec_fa_14: num 5.10e-01 7.31e-02 2.51e-02 7.33e-05 ## $ bisec_fb_14: num -0.02308 -0.01842 -0.00503 -0.01084 ## $ bisec_a_15 : num 1.32 7.29 20.89 54.1 ## $ bisec_b_15 : num 1.32 7.29 20.89 54.1 ## $ bisec_fa_15: num 2.43e-01 2.73e-02 1.00e-02 7.33e-05 ## $ bisec_fb_15: num -0.02308 -0.01842 -0.00503 -0.00538 ## $ bisec_a_16 : num 1.32 7.29 20.89 54.1 ## $ bisec_b_16 : num 1.32 7.29 20.89 54.1 ## $ bisec_fa_16: num 1.10e-01 4.46e-03 2.50e-03 7.33e-05 ## $ bisec_fb_16: num -0.02308 -0.01842 -0.00503 -0.00266 ## $ bisec_a_17 : num 1.32 7.29 20.89 54.1 ## $ bisec_b_17 : num 1.32 7.29 20.89 54.1 ## $ bisec_fa_17: num 4.35e-02 4.46e-03 2.50e-03 7.33e-05 ## $ bisec_fb_17: num -0.02308 -0.00698 -0.00126 -0.00129 ## $ bisec_a_18 : num 1.32 7.29 20.89 54.1 ## $ bisec_b_18 : num 1.32 7.29 20.89 54.1 ## $ bisec_fa_18: num 1.02e-02 4.46e-03 6.18e-04 7.33e-05 ## $ bisec_fb_18: num -0.023082 -0.001259 -0.001264 -0.000609 7.1.1.3.2 Table Two–Very Wide to Very Long We want to treat the iteration count information that is the suffix of variable names as a variable by itself. Additionally, we want to treat the a,b,fa,fb as a variable. Structuring the data very long like this allows for easy graphing and other types of analysis. Rather than dealing with many many variables, we have only 3 core variables that store bisection iteration information. Here we use the very nice pivot_longer function. Note that to achieve this, we put a common prefix in front of the variables we wanted to convert to long. THis is helpful, because we can easily identify which variables need to be reshaped. # New variables svr_bisect_iter &lt;- &#39;biseciter&#39; svr_abfafb_long_name &lt;- &#39;varname&#39; svr_number_col &lt;- &#39;value&#39; svr_id_bisect_iter &lt;- paste0(svr_id_var, &#39;_bisect_ier&#39;) # Pivot wide to very long tb_states_choices_bisec_long &lt;- tb_states_choices_bisec %&gt;% pivot_longer( cols = starts_with(st_bisec_prefix), names_to = c(svr_abfafb_long_name, svr_bisect_iter), names_pattern = paste0(st_bisec_prefix, &quot;(.*)_(.*)&quot;), values_to = svr_number_col ) # Print summary(tb_states_choices_bisec_long) ## INDI_ID fl_A fl_alpha p f_p f_p_t_f_a varname biseciter ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 1.324 Min. :-1.259e-03 Min. :-5.614e-06 Length:304 Length:304 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 5.800 1st Qu.:-7.714e-04 1st Qu.:-1.437e-06 Class :character Class :character ## Median :2.50 Median : 0 Median :0.5 Median :14.092 Median : 4.364e-06 Median : 7.495e-07 Mode :character Mode :character ## Mean :2.50 Mean : 0 Mean :0.5 Mean :20.901 Mean : 2.244e-03 Mean : 1.102e-04 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.:29.192 3rd Qu.: 3.019e-03 3rd Qu.: 1.124e-04 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :54.096 Max. : 1.022e-02 Max. : 4.451e-04 ## value ## Min. :-15057.608 ## 1st Qu.: 0.000 ## Median : 1.367 ## Mean : -82.350 ## 3rd Qu.: 20.892 ## Max. : 100.000 head(tb_states_choices_bisec_long %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 30) ## # A tibble: 30 x 6 ## INDI_ID fl_A fl_alpha varname biseciter value ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 -2 0.1 a 0 0 ## 2 1 -2 0.1 b 0 100 ## 3 1 -2 0.1 fa 0 100 ## 4 1 -2 0.1 fb 0 -15058. ## 5 1 -2 0.1 a 1 0 ## 6 1 -2 0.1 b 1 50 ## 7 1 -2 0.1 fa 1 100 ## 8 1 -2 0.1 fb 1 -6660. ## 9 1 -2 0.1 a 2 0 ## 10 1 -2 0.1 b 2 25 ## # ... with 20 more rows tail(tb_states_choices_bisec_long %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;)), 30) ## # A tibble: 30 x 6 ## INDI_ID fl_A fl_alpha varname biseciter value ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 2 0.9 fa 11 0.0765 ## 2 4 2 0.9 fb 11 -0.0108 ## 3 4 2 0.9 a 12 54.1 ## 4 4 2 0.9 b 12 54.1 ## 5 4 2 0.9 fa 12 0.0328 ## 6 4 2 0.9 fb 12 -0.0108 ## 7 4 2 0.9 a 13 54.1 ## 8 4 2 0.9 b 13 54.1 ## 9 4 2 0.9 fa 13 0.0110 ## 10 4 2 0.9 fb 13 -0.0108 ## # ... with 20 more rows 7.1.1.3.3 Table Two–Very Very Long to Wider Again But the previous results are too long, with the a, b, fa, and fb all in one column as different categories, they are really not different categories, they are in fact different types of variables. So we want to spread those four categories of this variable into four columns, each one representing the a, b, fa, and fb values. The rows would then be uniquly identified by the iteration counter and individual ID. # Pivot wide to very long to a little wide tb_states_choices_bisec_wider &lt;- tb_states_choices_bisec_long %&gt;% pivot_wider( names_from = !!sym(svr_abfafb_long_name), values_from = svr_number_col ) # Print summary(tb_states_choices_bisec_wider) ## INDI_ID fl_A fl_alpha p f_p f_p_t_f_a biseciter a b ## Min. :1.00 Min. :-2 Min. :0.1 Min. : 1.324 Min. :-1.259e-03 Min. :-5.614e-06 Length:76 Min. : 0.000 Min. : 1.324 ## 1st Qu.:1.75 1st Qu.:-1 1st Qu.:0.3 1st Qu.: 5.800 1st Qu.:-7.714e-04 1st Qu.:-1.437e-06 Class :character 1st Qu.: 1.306 1st Qu.: 7.294 ## Median :2.50 Median : 0 Median :0.5 Median :14.092 Median : 4.364e-06 Median : 7.495e-07 Mode :character Median : 7.289 Median : 20.898 ## Mean :2.50 Mean : 0 Mean :0.5 Mean :20.901 Mean : 2.244e-03 Mean : 1.102e-04 Mean :18.356 Mean : 28.883 ## 3rd Qu.:3.25 3rd Qu.: 1 3rd Qu.:0.7 3rd Qu.:29.192 3rd Qu.: 3.019e-03 3rd Qu.: 1.124e-04 3rd Qu.:20.891 3rd Qu.: 54.097 ## Max. :4.00 Max. : 2 Max. :0.9 Max. :54.096 Max. : 1.022e-02 Max. : 4.451e-04 Max. :54.095 Max. :100.000 ## fa fb ## Min. : 0.00007 Min. :-15057.608 ## 1st Qu.: 0.06570 1st Qu.: -21.089 ## Median : 0.92799 Median : -1.029 ## Mean : 22.90627 Mean : -399.547 ## 3rd Qu.: 15.51699 3rd Qu.: -0.018 ## Max. :100.00000 Max. : -0.001 print(tb_states_choices_bisec_wider %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;))) ## # A tibble: 76 x 8 ## INDI_ID fl_A fl_alpha biseciter a b fa fb ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -2 0.1 0 0 100 100 -15058. ## 2 1 -2 0.1 1 0 50 100 -6660. ## 3 1 -2 0.1 2 0 25 100 -2918. ## 4 1 -2 0.1 3 0 12.5 100 -1248. ## 5 1 -2 0.1 4 0 6.25 100 -503. ## 6 1 -2 0.1 5 0 3.12 100 -170. ## 7 1 -2 0.1 6 0 1.56 100 -21.1 ## 8 1 -2 0.1 7 0.781 1.56 45.7 -21.1 ## 9 1 -2 0.1 8 1.17 1.56 13.2 -21.1 ## 10 1 -2 0.1 9 1.17 1.37 13.2 -3.76 ## # ... with 66 more rows print(tb_states_choices_bisec_wider %&gt;% select(-one_of(&#39;p&#39;,&#39;f_p&#39;,&#39;f_p_t_f_a&#39;))) ## # A tibble: 76 x 8 ## INDI_ID fl_A fl_alpha biseciter a b fa fb ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -2 0.1 0 0 100 100 -15058. ## 2 1 -2 0.1 1 0 50 100 -6660. ## 3 1 -2 0.1 2 0 25 100 -2918. ## 4 1 -2 0.1 3 0 12.5 100 -1248. ## 5 1 -2 0.1 4 0 6.25 100 -503. ## 6 1 -2 0.1 5 0 3.12 100 -170. ## 7 1 -2 0.1 6 0 1.56 100 -21.1 ## 8 1 -2 0.1 7 0.781 1.56 45.7 -21.1 ## 9 1 -2 0.1 8 1.17 1.56 13.2 -21.1 ## 10 1 -2 0.1 9 1.17 1.37 13.2 -3.76 ## # ... with 66 more rows 7.1.1.4 Graph Bisection Iteration Results Actually we want to graph based on the long results, not the wider. Wider easier to view in table. # Graph results lineplot &lt;- tb_states_choices_bisec_long %&gt;% mutate(!!sym(svr_bisect_iter) := as.numeric(!!sym(svr_bisect_iter))) %&gt;% filter(!!sym(svr_abfafb_long_name) %in% c(&#39;a&#39;, &#39;b&#39;)) %&gt;% ggplot(aes(x=!!sym(svr_bisect_iter), y=!!sym(svr_number_col), colour=!!sym(svr_abfafb_long_name), linetype=!!sym(svr_abfafb_long_name), shape=!!sym(svr_abfafb_long_name))) + facet_wrap( ~ INDI_ID) + geom_line() + geom_point() + labs(title = &#39;Bisection Iteration over individuals Until Convergence&#39;, x = &#39;Bisection Iteration&#39;, y = &#39;a (left side point) and b (right side point) values&#39;, caption = &#39;DPLYR concurrent bisection nonlinear multple individuals&#39;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) print(lineplot) "]
]
